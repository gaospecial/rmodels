[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R Models",
    "section": "",
    "text": "Preface\nThis is a Quarto book."
  },
  {
    "objectID": "index.html#环境配置",
    "href": "index.html#环境配置",
    "title": "R Models",
    "section": "环境配置",
    "text": "环境配置\n\nVS Code\n\n安装 VS Code\n安装 VS Code 插件\n\nR 扩展 - 提供 R 语言支持\nQuarto 扩展 - 提供 Quarto 功能\nMarkdown 扩展 - 提供 Markdown 格式化快捷方式\n\n\n\n\nQuarto\nMarkdown is an easy to read and write text format:\n\nIt’s plain text so works well with version control\nIt can be rendered into HTML, PDF, and more\nLearn more at: https://quarto.org/docs/authoring/\n\n\n\n配置 Python\n设置 reticulate 运行的 Python 环境\n根据 quarto 扩展的默认设置，Python 代码将通过 reticulate 来运行。如果没有安装的话需要运行 install.packages(\"reticulate\") 安装它。然后使用 use_condaenv() 来指定需要的 Conda 环境。\n\nreticulate::use_condaenv(\"rmodels\")"
  },
  {
    "objectID": "index.html#环境测试",
    "href": "index.html#环境测试",
    "title": "R Models",
    "section": "环境测试",
    "text": "环境测试\n\nCode Cell\nHere is a Python code cell:\n\nimport os\nos.cpu_count()\n\n12\n\n\nHere is a R code chunk:\n\nplot(cars)\n\n\n\n\n\n\nEquation\nUse LaTeX to write equations:\n\n\\chi' = \\sum_{i=1}^n k_i s_i^2"
  },
  {
    "objectID": "index.html#项目简介",
    "href": "index.html#项目简介",
    "title": "R Models",
    "section": "项目简介",
    "text": "项目简介\n使用 R/Python 进行模型构建的学习笔记。\n\ntidymodels 软件包的使用 https://workshops.tidymodels.org\n《数据挖掘实战》读书笔记 (张 2021)\n更多学习材料(TidymodelsHuanYing?; M. K. and J. Silge 2022; E. H. and J. Silge 2023; Robinson 2023; McConville 2022)\n\n\n\n\n\nMcConville, Chester Ismay and Albert Y. Kim Foreword by Kelly S. 2022. Statistical Inference via Data Science.\n\n\nRobinson, Julia Silge and David. 2023. Welcome to Text Mining with R  Text Mining with R.\n\n\nSilge, Emil Hvitfeldt and Julia. 2023. Supervised Machine Learning for Text Analysis in R.\n\n\nSilge, Max Kuhn and Julia. 2022. Tidy Modeling with R.\n\n\n张俊妮. 2021. 数据挖掘：基于R语言的实战. 人民邮电出版社."
  },
  {
    "objectID": "10.tidymodels-intro.html#什么是机器学习",
    "href": "10.tidymodels-intro.html#什么是机器学习",
    "title": "Machine Learning with Tidymodels",
    "section": "什么是机器学习",
    "text": "什么是机器学习\ntidymodels 是一个用于机器学习的软件包。那么什么是机器学习呢（Figure 1）？\n\n\n\n\n\nFigure 1: 什么是机器学习\n\n\n\n\n机器学习（Machine Learning，简称ML）是一种人工智能（Artificial Intelligence，简称AI）的分支领域，致力于研究如何让计算机系统通过经验学习改善性能。机器学习的目标是让计算机能够从数据中学习，自动识别模式、进行预测，并不断地提高自身的性能。\n在传统的编程中，程序员编写规则和算法来指导计算机执行特定的任务。而在机器学习中，我们提供大量的数据和相应的结果（标签），让计算机自己学习从数据中提取模式和规律，而不是显式地编写详尽的规则。\n机器学习任务通常可以分为以下几类：\n\n监督学习（Supervised Learning）：在监督学习中，算法接收带有标签的训练数据，学习输入与输出之间的映射关系。目标是使算法能够对新的、未标记的数据进行准确的预测或分类。\n\n分类（Classification）：预测输入属于哪个类别，例如垃圾邮件检测、手写数字识别等。\n回归（Regression）：预测一个连续值，例如房价预测、股票价格预测等。\n\n无监督学习（Unsupervised Learning）：在无监督学习中，算法接收未标记的训练数据，目标是发现数据中的模式、结构或关系。\n\n聚类（Clustering）：将数据划分为不同的组，使组内的数据相似度较高，组间相似度较低。\n降维（Dimensionality Reduction）：减少数据的维度，保留重要的特征，例如主成分分析（PCA）。\n\n强化学习（Reinforcement Learning）：在强化学习中，算法通过与环境的交互学习，通过尝试最大化累积奖励来决定最佳的行为策略。这种学习方式通常涉及到代理（Agent）和环境之间的交互。\n\n代表性应用：游戏玩家、自动驾驶汽车。\n\n\n机器学习使用多种技术和算法，包括决策树、支持向量机、神经网络、朴素贝叶斯、K近邻等。这些方法在不同的问题和数据情境中表现良好，选择合适的算法取决于具体的任务和数据特征。机器学习在许多领域取得了显著的成就，如自然语言处理、计算机视觉、医学诊断等。"
  },
  {
    "objectID": "10.tidymodels-intro.html#什么是-tidymodels",
    "href": "10.tidymodels-intro.html#什么是-tidymodels",
    "title": "Machine Learning with Tidymodels",
    "section": "什么是 tidymodels？",
    "text": "什么是 tidymodels？\ntidymodels 是一个 R 语言的机器学习工具集合，包含了一系列用于统计建模和机器学习的软件包。\n它是一个统一的建模框架，为 R 用户提供了一种进行预处理，建模，评估和调整的有序方法。其目标是简化数据分析过程。这个框架集成了很多现有且被广泛使用的R包，使得其具有良好的扩展性。\nTidymodels包含以下几个主要的组件：\n\nRecipes: 这是一种用于数据预处理步骤的样板文件/蓝图。例如，缩放或中心化连续变量，编码分类变量等。\nrsample: 用于重复抽样，例如交叉验证或bootstrap。\nparnsip: 用于设置模型规范和引擎。\ntune: 用于模型调优。\nworkflows: 允许将预处理步骤（即配方）和模型规范合并为单一对象，以便在整个工作流程中保持一致性。\nyardstick: 用于计算模型的表现和效果。\n\n以下是 tidymodels 主要组成部分及其功能的简要介绍 (Figure 2)：\n\n\n\nFigure 2: tidymodels 相关包\n\n\n\nlibrary(\"tidymodels\")\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n\n\n✔ broom        1.0.5     ✔ recipes      1.0.9\n✔ dials        1.2.0     ✔ rsample      1.2.0\n✔ dplyr        1.1.4     ✔ tibble       3.2.1\n✔ ggplot2      3.4.4     ✔ tidyr        1.3.0\n✔ infer        1.0.5     ✔ tune         1.1.2\n✔ modeldata    1.2.0     ✔ workflows    1.1.3\n✔ parsnip      1.1.1     ✔ workflowsets 1.0.1\n✔ purrr        1.0.2     ✔ yardstick    1.2.0\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n\n\n\n模型规范（Model Specification）：\n\nparsnip 包提供了一个一致的 API，用于定义、估计和调整各种统计模型。它支持多种模型类型，包括回归、分类、聚类等。\n\nlibrary(parsnip)\n\n# 创建一个线性回归模型规范\nlinear_spec &lt;- linear_reg() %&gt;% set_engine(\"lm\")\n预处理（Preprocessing）：\n\nrsample 和 recipes 包用于创建和执行数据预处理步骤，例如缺失值处理、变量变换、特征工程等。预处理步骤可以与模型规范无缝集成。\n\nlibrary(recipes)\n\n# 创建一个数据预处理配方\npreprocess_recipe &lt;- recipe(target ~ ., data = training_data) %&gt;%\n  step_scale(all_predictors()) %&gt;%\n  step_center(all_predictors())\n模型调参（Model Tuning）：\n\ntune 包用于执行模型参数调优（tuning）。它提供了一个一致的框架，可以对模型进行网格搜索或其他优化方法，以找到最佳的超参数组合。\n\nlibrary(tune)\n\n# 创建一个参数调优网格\ngrid &lt;- expand.grid(neighbors = c(1, 3, 5))\n\n# 进行参数调优\ntune_result &lt;- tune_grid(\n  linear_spec,\n  resamples = training_data,\n  grid = grid\n)\n模型评估（Model Evaluation）：\n\nyardstick 包提供了用于评估模型性能的工具，包括各种指标（如准确率、AUC、RMSE 等）和图形化方法。\n\nlibrary(yardstick)\n\n# 评估线性回归模型的性能\nlinear_metrics &lt;- linear_spec %&gt;%\n  fit(training_data) %&gt;%\n  predict(new_data = testing_data) %&gt;%\n  yardstick::metrics(truth = testing_data$target, estimate = .pred)\n管道（Workflows）：\n\n\nworkflows 包提供了一种组织模型训练、预处理和评估的框架。它允许你定义整个建模过程，并使整个工作流程可重复和可扩展。\n\nlibrary(workflows)\n\n# 创建一个包含预处理和模型的工作流\nwf &lt;- workflow() %&gt;%\n  add_recipe(preprocess_recipe) %&gt;%\n  add_model(linear_spec)\n\n# 训练和评估工作流\nwf_fit &lt;- wf %&gt;%\n  fit(training_data) %&gt;%\n  predict(new_data = testing_data)\n总体而言，tidymodels 提供了一个一致的框架，使数据科学家和分析师能够更轻松地进行模型开发、评估和调优。它与 tidyverse 的其他部分无缝集成，支持整洁的数据处理和可读性强的代码。"
  },
  {
    "objectID": "10.tidymodels-intro.html#模型的依赖包",
    "href": "10.tidymodels-intro.html#模型的依赖包",
    "title": "Machine Learning with Tidymodels",
    "section": "模型的依赖包",
    "text": "模型的依赖包\n下面是为 tidymodels 提供模型的 R 包：\n\nlm（Linear Model）：\n\n描述： 用于拟合线性回归模型，适用于连续型目标变量。\n示例代码：\nmodel &lt;- lm(y ~ x1 + x2, data = my_data)\n\nglm（Generalized Linear Model）：\n\n描述： 用于广义线性模型，适用于具有不同分布的目标变量，如二项分布（逻辑回归）。\n示例代码：\nmodel &lt;- glm(y ~ x1 + x2, family = binomial, data = my_data)\n\nglmnet（Regularized Regression）：\n\n描述： 用于 L1 和 L2 正则化的线性和广义线性模型，适用于处理高维数据集。\n示例代码：\nlibrary(glmnet)\nmodel &lt;- cv.glmnet(x, y, family = \"gaussian\")\n\nkeras（Regression using TensorFlow）：\n\n描述： 提供了使用深度学习框架 TensorFlow 进行回归的功能。\n示例代码：\nlibrary(keras)\nmodel &lt;- keras_model_sequential() %&gt;%\n  layer_dense(units = 1, input_shape = c(n_features))\n\nstan（Bayesian Regression）：\n\n描述： 使用概率编程语言 Stan 进行贝叶斯线性回归。\n示例代码：\nlibrary(rstan)\nmodel &lt;- stan_model(\"linear_regression.stan\")\n\nspark（Large Data Sets）：\n\n描述： 提供了使用 Apache Spark 处理大规模数据集的功能，包括分布式回归。\n示例代码：\nlibrary(sparklyr)\nsc &lt;- spark_connect(master = \"local\")\nmodel &lt;- spark_lm(sc, mpg ~ wt + hp, data = mtcars)\n\n\n这些包涵盖了不同类型回归任务的需求，从传统的线性回归到深度学习和贝叶斯回归。"
  },
  {
    "objectID": "11.tidymodels-basic.html#安装需要的软件包",
    "href": "11.tidymodels-basic.html#安装需要的软件包",
    "title": "1  Basic tidymodels",
    "section": "1.1 安装需要的软件包",
    "text": "1.1 安装需要的软件包\n安装下面的这些软件包，以便完成上面列举的任务。\n\n# Install the packages for the workshop\npkgs &lt;- \n  c(\"bonsai\", \"doParallel\", \"embed\", \"finetune\", \"lightgbm\", \"lme4\",\n    \"plumber\", \"probably\", \"ranger\", \"rpart\", \"rpart.plot\", \"rules\",\n    \"splines2\", \"stacks\", \"text2vec\", \"textrecipes\", \"tidymodels\", \n    \"vetiver\", \"remotes\")\n\npak::pak(pkgs)"
  },
  {
    "objectID": "11.tidymodels-basic.html#数据预处理",
    "href": "11.tidymodels-basic.html#数据预处理",
    "title": "1  Basic tidymodels",
    "section": "1.2 数据预处理",
    "text": "1.2 数据预处理\nmodeldata 包提供了一些示例数据集，用于在 tidymodels 中进行模型建设和演示。其中 “taxi” 数据集是一个简化的示例数据集，描述了芝加哥出租车司机获得小费的情况。\n其详细信息可以使用以下代码查看：\n\nlibrary(modeldata)\ntaxi\n\n# A tibble: 10,000 × 7\n   tip   distance company                      local dow   month  hour\n   &lt;fct&gt;    &lt;dbl&gt; &lt;fct&gt;                        &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt;\n 1 yes      17.2  Chicago Independents         no    Thu   Feb      16\n 2 yes       0.88 City Service                 yes   Thu   Mar       8\n 3 yes      18.1  other                        no    Mon   Feb      18\n 4 yes      20.7  Chicago Independents         no    Mon   Apr       8\n 5 yes      12.2  Chicago Independents         no    Sun   Mar      21\n 6 yes       0.94 Sun Taxi                     yes   Sat   Apr      23\n 7 yes      17.5  Flash Cab                    no    Fri   Mar      12\n 8 yes      17.7  other                        no    Sun   Jan       6\n 9 yes       1.85 Taxicab Insurance Agency Llc no    Fri   Apr      12\n10 yes       1.47 City Service                 no    Tue   Mar      14\n# ℹ 9,990 more rows\n\n\n包含的变量有：\n\ntip：乘客是否留下小费。 “yes” 或 “no”。\ndistance：行程距离，以英里为单位。\ncompany：出租车公司。出现次数较少的公司被分为 “other”。\nlocal：行程是否在同一社区区域开始和结束。\ndow：行程开始的星期几。\nmonth：行程开始的月份。\nhour：行程开始的小时。\n\n这个数据一共有 10000 行。\n\n1.2.1 拆分数据\n在机器学习中，数据集主要分为以下几种类型：\n\n训练集（Training Set）：\n\n定义： 用于训练模型的数据集。\n作用： 模型通过训练集学习特征和模式，调整参数以最小化预测错误。\n\n验证集（Validation Set）：\n\n定义： 用于调整模型超参数、选择模型或防止过拟合的数据集。\n作用： 通过在验证集上评估模型性能，进行超参数调整和模型选择。\n\n测试集（Test Set）：\n\n定义： 用于评估模型在未见过的数据上的性能的数据集。\n作用： 测试集提供了模型在真实场景中的泛化能力的估计。\n\n\n使用 initial_split() 将数据拆分成训练集和测试集。\n\nset.seed(123)\nlibrary(rsample)\n\n# random split\n(taxi_split &lt;- initial_split(taxi))\n\n&lt;Training/Testing/Total&gt;\n&lt;7500/2500/10000&gt;\n\n# access to split data\n(taxi_train &lt;- training(taxi_split))\n\n# A tibble: 7,500 × 7\n   tip   distance company                   local dow   month  hour\n   &lt;fct&gt;    &lt;dbl&gt; &lt;fct&gt;                     &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt;\n 1 yes       0.7  Taxi Affiliation Services yes   Tue   Mar      18\n 2 yes       0.99 Sun Taxi                  yes   Tue   Jan       8\n 3 yes       1.78 other                     no    Sat   Mar      22\n 4 yes       0    Taxi Affiliation Services yes   Wed   Apr      15\n 5 yes       0    Taxi Affiliation Services no    Sun   Jan      21\n 6 yes       2.3  other                     no    Sat   Apr      21\n 7 yes       6.35 Sun Taxi                  no    Wed   Mar      16\n 8 yes       2.79 other                     no    Sun   Feb      14\n 9 yes      16.6  other                     no    Sun   Apr      18\n10 yes       0.02 Chicago Independents      yes   Sun   Apr      15\n# ℹ 7,490 more rows\n\n(taxi_test &lt;- testing(taxi_split))\n\n# A tibble: 2,500 × 7\n   tip   distance company                      local dow   month  hour\n   &lt;fct&gt;    &lt;dbl&gt; &lt;fct&gt;                        &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt;\n 1 yes       0.94 Sun Taxi                     yes   Sat   Apr      23\n 2 yes       1    Taxi Affiliation Services    no    Mon   Feb      18\n 3 yes       1.91 Flash Cab                    no    Wed   Apr      15\n 4 yes       1.1  Chicago Independents         no    Sat   Mar      10\n 5 yes      11.7  other                        no    Wed   Mar      18\n 6 yes      17.8  City Service                 no    Mon   Mar       9\n 7 yes       0.53 Taxicab Insurance Agency Llc yes   Wed   Apr       8\n 8 yes       1.14 City Service                 no    Wed   Mar      14\n 9 yes       1.77 other                        no    Thu   Apr      15\n10 yes      18.6  Flash Cab                    no    Thu   Apr      12\n# ℹ 2,490 more rows\n\n\n使用 initial_validation_split() 将数据拆分成训练集、验证集和测试集。\n\nset.seed(123)\n(taxi_split_2 = initial_validation_split(taxi, prop = c(0.6, 0.2)))\n\n&lt;Training/Validation/Testing/Total&gt;\n&lt;6000/2000/2000/10000&gt;\n\ntraining(taxi_split_2)\n\n# A tibble: 6,000 × 7\n   tip   distance company                      local dow   month  hour\n   &lt;fct&gt;    &lt;dbl&gt; &lt;fct&gt;                        &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt;\n 1 yes      17.2  Chicago Independents         no    Thu   Feb      16\n 2 yes      20.7  Chicago Independents         no    Mon   Apr       8\n 3 yes      12.2  Chicago Independents         no    Sun   Mar      21\n 4 yes       1.85 Taxicab Insurance Agency Llc no    Fri   Apr      12\n 5 yes       0.53 Sun Taxi                     no    Tue   Mar      18\n 6 yes      16.8  Sun Taxi                     no    Wed   Apr      12\n 7 yes       0.86 other                        no    Tue   Feb      13\n 8 no        0.4  other                        yes   Fri   Mar      17\n 9 yes       0.1  Taxi Affiliation Services    no    Sun   Apr      10\n10 yes       1.6  Taxi Affiliation Services    no    Tue   Apr       8\n# ℹ 5,990 more rows\n\ntesting(taxi_split_2)\n\n# A tibble: 2,000 × 7\n   tip   distance company                      local dow   month  hour\n   &lt;fct&gt;    &lt;dbl&gt; &lt;fct&gt;                        &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt;\n 1 yes       0.88 City Service                 yes   Thu   Mar       8\n 2 yes       1.47 City Service                 no    Tue   Mar      14\n 3 yes       1    other                        no    Fri   Mar      17\n 4 yes       1.35 Taxicab Insurance Agency Llc no    Thu   Feb      17\n 5 yes       1.14 City Service                 no    Wed   Mar      14\n 6 yes       1.77 other                        no    Thu   Apr      15\n 7 no        1.07 Sun Taxi                     no    Fri   Feb      15\n 8 no        1.13 other                        no    Sat   Feb      14\n 9 yes      12.1  Chicago Independents         no    Tue   Jan      11\n10 yes       1.91 Sun Taxi                     no    Tue   Jan      17\n# ℹ 1,990 more rows\n\nvalidation(taxi_split_2)\n\n# A tibble: 2,000 × 7\n   tip   distance company                      local dow   month  hour\n   &lt;fct&gt;    &lt;dbl&gt; &lt;fct&gt;                        &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt;\n 1 yes      18.1  other                        no    Mon   Feb      18\n 2 yes       0.94 Sun Taxi                     yes   Sat   Apr      23\n 3 yes      17.5  Flash Cab                    no    Fri   Mar      12\n 4 yes      17.7  other                        no    Sun   Jan       6\n 5 yes       6.65 Taxicab Insurance Agency Llc no    Sun   Apr      11\n 6 yes       1.21 Sun Taxi                     yes   Thu   Apr      19\n 7 yes       1    Taxi Affiliation Services    no    Mon   Feb      18\n 8 yes       1.91 Flash Cab                    no    Wed   Apr      15\n 9 yes       1.1  Chicago Independents         no    Sat   Mar      10\n10 no        0.96 Taxicab Insurance Agency Llc no    Mon   Apr       8\n# ℹ 1,990 more rows\n\n\n使用函数的 strata、prop 参数，以及 initial_time_split()、group_initial_split() 等函数，可以实现更科学的随机分组。"
  },
  {
    "objectID": "11.tidymodels-basic.html#模型的结构",
    "href": "11.tidymodels-basic.html#模型的结构",
    "title": "1  Basic tidymodels",
    "section": "1.3 模型的结构",
    "text": "1.3 模型的结构\n在 R 中使用 tidymodels 进行建模的基本步骤如下：\n\n选择模型：\n\n选择适合任务的模型，例如线性回归、决策树、随机森林等。\n\n指定引擎：\n\n指定模型使用的引擎，如 “lm” 或 “glmnet”。\n\n设置模型模式：\n\n设置模型的模式，是用于分类还是回归。\n\n\n\n\n\n\n\n\nNote\n\n\n\nModels have default engines.\nSome models have a default mode.\n\n\n\n# 使用默认引擎的逻辑回归模型\nlogistic_reg()\n\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\n# 使用 glmnet 引擎的逻辑回归模型\nlogistic_reg() %&gt;%\n  set_engine(\"glmnet\")\n\nLogistic Regression Model Specification (classification)\n\nComputational engine: glmnet \n\n# 使用 stan 引擎的逻辑回归模型\nlogistic_reg() %&gt;%\n  set_engine(\"stan\")\n\nLogistic Regression Model Specification (classification)\n\nComputational engine: stan \n\n# 未指定模式的决策树\ndecision_tree()\n\nDecision Tree Model Specification (unknown mode)\n\nComputational engine: rpart \n\n# 指定分类模式的决策树\ndecision_tree() %&gt;% \n  set_mode(\"classification\")\n\nDecision Tree Model Specification (classification)\n\nComputational engine: rpart \n\n\n\n\n\n\n\n\nTip\n\n\n\nAll available models are listed at https://www.tidymodels.org/find/parsnip/\n\n\n\n1.3.1 模型与引擎的差异\n在 tidymodels 中，模型和计算引擎是分开的。这允许你使用相同的模型规格，但可以选择用于训练模型的不同算法或程序包。\n模型引擎是指用于实现特定类型模型的软件（通常是一个R包）。例如，对于线性回归模型，可能的引擎包括\"lm\"、\"glmnet\"、\"spark\"等，每个都对应不同的实现方法。\n不同的引擎可能会有以下几种差异：\n\n计算效率：一些引擎可能在大数据集上更有效率，而其他引擎在小数据集上可能更快。\n功能：一些引擎可能只支持某些特定的功能。例如，\"glmnet\"引擎支持L1和L2正则化，而\"lm\"引擎则不支持。\n可扩展性：某些引擎（如\"spark\"）可能被设计为可以在分布式计算环境中运行，从而处理大规模数据集。\n结果：由于采用的优化算法和随机初始化等因素的影响，不同引擎可能会得到略微不同的结果。\n\n总的来说，选择哪个引擎并没有固定的答案，取决于具体的需求和环境。你可能需要根据计算资源、数据大小和模型复杂性等因素来选择最适合的引擎。\n例如，下面使用两种不同的ß引擎创建了随机森林模型。\n在 tidymodels 中，创建随机森林模型规格可以使用 rand_forest() 函数。但是，具体的训练过程会由你选择的计算引擎决定。下面是两个例子，分别说明了如何使用 \"ranger\" 引擎和 \"randomForest\" 引擎。\n\n使用 \"ranger\" 引擎：\n\nlibrary(tidymodels)\n\n# 创建模型规格\nrf_spec &lt;- rand_forest(mtry = 10, trees = 1000) %&gt;%\n   set_engine(\"ranger\", importance = 'impurity') %&gt;%\n   set_mode(\"classification\")\n\n# 训练模型\nrf_fit &lt;- rf_spec %&gt;% fit(Class ~ ., data = your_data)\n在这里，我们设置了 mtry = 10（即每个树节点考虑的变量数）和 trees = 1000（生成的树的数量）。然后我们指定了引擎为 \"ranger\"。ranger 包提供了一个参数 importance，用于计算变量重要性（这里我们设为 ‘impurity’，表示计算基于不纯度的变量重要性）。\n\n使用 \"randomForest\" 引擎：\n\n# 创建模型规格\nrf_spec &lt;- rand_forest(mtry = 10, trees = 1000) %&gt;%\n   set_engine(\"randomForest\", importance = TRUE) %&gt;%\n   set_mode(\"classification\")\n\n# 训练模型\nrf_fit &lt;- rf_spec %&gt;% fit(Class ~ ., data = your_data)\n在这个例子中，我们指定了引擎为 \"randomForest\"。randomForest 包提供了一个参数 importance，如果设置为 TRUE，则计算变量重要性。\n两个引擎的主要区别在于：\n\n\"ranger\" 引擎通常比 \"randomForest\" 引擎更快，且能处理更大的数据集。\n\"ranger\" 引擎提供了更多的选项来计算变量重要性。\n\n最终的模型结果可能会有些微小的差异，因为这两个包在实现随机森林时使用了不同的方法和优化技术。"
  },
  {
    "objectID": "11.tidymodels-basic.html#开始建模",
    "href": "11.tidymodels-basic.html#开始建模",
    "title": "1  Basic tidymodels",
    "section": "1.4 开始建模",
    "text": "1.4 开始建模\n使用 2 种模型对 taxi 数据进行建模。\n\nLogistic regression\nDecision trees\n\n\n1.4.1 逻辑回归模型\n逻辑回归模型将事件概率的对数几率（logit）建模为预测变量的线性组合：\n\n\\log\\left(\\frac{p}{1 - p}\\right) = \\beta_0 + \\beta_1 \\cdot A\n\n在这里：\n\np 是事件发生的概率，\n\\beta_0 是截距，\n\\beta_1 是与预测变量 A 相关的系数。\n\n\n\n1.4.2 决策树\n使用决策树建模。\n\n基于预测变量的一系列划分或 if/then 语句：\n首先，树会在满足某些条件之前（如达到最大深度或没有更多数据时）不断地“生长”（grow）。\n然后，为了降低树的复杂性，树会被“修剪”（pruned）。\n\n\ntree_fit &lt;- decision_tree(mode = \"classification\") %&gt;% \n  fit(class ~ A, data = mutate(dat, class = forcats::fct_rev(class)))\n\ntree_preds &lt;- augment(tree_fit, new_data = bin_midpoints)\n\nlibrary(rpart.plot)\n\n载入需要的程辑包：rpart\n\n\n\n载入程辑包：'rpart'\n\n\nThe following object is masked from 'package:dials':\n\n    prune\n\ntree_fit %&gt;%\n  extract_fit_engine() %&gt;%\n  rpart.plot(roundint = FALSE)\n\n\n\n\n建模的效果如下：\n\n\n\n\n\n\nCaution\n\n\n\nAll models are wrong, but some are useful!\n\n\n\n\n\n\n\nFigure 1.2: 逻辑回归模型与决策树的预测结果示意。（A）通过逻辑函数（S形函数），找到一条分隔两个类别的曲线。当 p 大于 0.5 时，预测的类别为 1；否则，为 0。 S 形曲线的形状实现了两个类别之间的平滑过渡。（B）使用决策树建模。\n\n\n\n\n\n\n1.4.3 将模型整合为 workflow\n使用 workflow() 有一些明显的优势（Figure 1.3）。\n\nWorkflow 在处理新数据方面比基本的 R 工具更加灵活，尤其是在涉及新的因子水平时：这在处理分类变量时尤为重要。\n可以使用更多的数据预处理器来提取特征（在高级 tidymodels 中更多关于特征提取的内容！）\n便于同时处理多个模型。\n最重要的是，Workflow 涵盖了整个建模过程：fit() 和 predict() 不仅适用于模型拟合，还适用于预处理步骤。\n\n\n\n\n\n\n\nWorkflow 如何更好地处理因子水平\n\n\n\n\n强制执行不允许在预测时出现新因子水平的规定（可以关闭）\n恢复在拟合时存在但在预测时缺失的因子水平\n\n\n\n\n\n\nFigure 1.3: Workflows bind preprocessors and models\n\n\n经典方法\n\ntree_spec &lt;-\n  decision_tree(cost_complexity = 0.002) %&gt;% \n  set_mode(\"classification\")\n\ntree_spec %&gt;% \n  fit(tip ~ ., data = taxi_train) \n\nparsnip model object\n\nn= 7500 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 7500 579 yes (0.92280000 0.07720000)  \n   2) distance&gt;=11.805 2169  83 yes (0.96173352 0.03826648) *\n   3) distance&lt; 11.805 5331 496 yes (0.90695929 0.09304071)  \n     6) distance&lt; 5.405 5061 419 yes (0.91721004 0.08278996) *\n     7) distance&gt;=5.405 270  77 yes (0.71481481 0.28518519)  \n      14) company=Chicago Independents,City Service,Sun Taxi,Taxicab Insurance Agency Llc,other 181  38 yes (0.79005525 0.20994475) *\n      15) company=Flash Cab,Taxi Affiliation Services 89  39 yes (0.56179775 0.43820225)  \n        30) dow=Sun,Mon,Wed,Thu,Sat 59  17 yes (0.71186441 0.28813559) *\n        31) dow=Tue,Fri 30   8 no (0.26666667 0.73333333) *\n\n\nworkflow方法\n\ntree_spec &lt;-\n  decision_tree(cost_complexity = 0.002) %&gt;% \n  set_mode(\"classification\")\n\n# 建立一个 workflow，同时保存模型参数，表达式和数据\nworkflow() %&gt;%\n  add_formula(tip ~ .) %&gt;%\n  add_model(tree_spec) %&gt;%\n  fit(data = taxi_train) \n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\ntip ~ .\n\n── Model ───────────────────────────────────────────────────────────────────────\nn= 7500 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 7500 579 yes (0.92280000 0.07720000)  \n   2) distance&gt;=11.805 2169  83 yes (0.96173352 0.03826648) *\n   3) distance&lt; 11.805 5331 496 yes (0.90695929 0.09304071)  \n     6) distance&lt; 5.405 5061 419 yes (0.91721004 0.08278996) *\n     7) distance&gt;=5.405 270  77 yes (0.71481481 0.28518519)  \n      14) company=Chicago Independents,City Service,Sun Taxi,Taxicab Insurance Agency Llc,other 181  38 yes (0.79005525 0.20994475) *\n      15) company=Flash Cab,Taxi Affiliation Services 89  39 yes (0.56179775 0.43820225)  \n        30) dow=Sun,Mon,Wed,Thu,Sat 59  17 yes (0.71186441 0.28813559) *\n        31) dow=Tue,Fri 30   8 no (0.26666667 0.73333333) *\n\n# 或者写在一起\nworkflow(tip ~ ., tree_spec) %&gt;% \n  fit(data = taxi_train) \n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\ntip ~ .\n\n── Model ───────────────────────────────────────────────────────────────────────\nn= 7500 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 7500 579 yes (0.92280000 0.07720000)  \n   2) distance&gt;=11.805 2169  83 yes (0.96173352 0.03826648) *\n   3) distance&lt; 11.805 5331 496 yes (0.90695929 0.09304071)  \n     6) distance&lt; 5.405 5061 419 yes (0.91721004 0.08278996) *\n     7) distance&gt;=5.405 270  77 yes (0.71481481 0.28518519)  \n      14) company=Chicago Independents,City Service,Sun Taxi,Taxicab Insurance Agency Llc,other 181  38 yes (0.79005525 0.20994475) *\n      15) company=Flash Cab,Taxi Affiliation Services 89  39 yes (0.56179775 0.43820225)  \n        30) dow=Sun,Mon,Wed,Thu,Sat 59  17 yes (0.71186441 0.28813559) *\n        31) dow=Tue,Fri 30   8 no (0.26666667 0.73333333) *\n\n\nEdit this code to make a workflow with your own model of choice.\nExtension/Challenge: Other than formulas, what kinds of preprocessors are supported?\n\n\n1.4.4 使用模型预测\n推荐使用 augment() 方法进行预测，与传统的 predict() 相比的差异如下。\n\ntree_fit &lt;-\n  workflow(tip ~ ., tree_spec) %&gt;% \n  fit(data = taxi_train) \n\npredict(tree_fit, new_data = taxi_test)\n\n# A tibble: 2,500 × 1\n   .pred_class\n   &lt;fct&gt;      \n 1 yes        \n 2 yes        \n 3 yes        \n 4 yes        \n 5 yes        \n 6 yes        \n 7 yes        \n 8 yes        \n 9 yes        \n10 yes        \n# ℹ 2,490 more rows\n\naugment(tree_fit, new_data = taxi_test)\n\n# A tibble: 2,500 × 10\n   tip   distance company local dow   month  hour .pred_class .pred_yes .pred_no\n   &lt;fct&gt;    &lt;dbl&gt; &lt;fct&gt;   &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt;           &lt;dbl&gt;    &lt;dbl&gt;\n 1 yes       0.94 Sun Ta… yes   Sat   Apr      23 yes             0.917   0.0828\n 2 yes       1    Taxi A… no    Mon   Feb      18 yes             0.917   0.0828\n 3 yes       1.91 Flash … no    Wed   Apr      15 yes             0.917   0.0828\n 4 yes       1.1  Chicag… no    Sat   Mar      10 yes             0.917   0.0828\n 5 yes      11.7  other   no    Wed   Mar      18 yes             0.790   0.210 \n 6 yes      17.8  City S… no    Mon   Mar       9 yes             0.962   0.0383\n 7 yes       0.53 Taxica… yes   Wed   Apr       8 yes             0.917   0.0828\n 8 yes       1.14 City S… no    Wed   Mar      14 yes             0.917   0.0828\n 9 yes       1.77 other   no    Thu   Apr      15 yes             0.917   0.0828\n10 yes      18.6  Flash … no    Thu   Apr      12 yes             0.962   0.0383\n# ℹ 2,490 more rows\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n预测结果在一个 tibble 中；\n列名和数据类型更加清晰和易于理解；\n行数和输出结果的行数是相同的，确保了数据的对应关系。\n\n\n\n\n\n1.4.5 解释模型\n使用 extract_*() 函数提取模型 workflow 对象的组件。如 Figure 1.4 展示了上面决策树模型的分类过程。\n\nlibrary(rpart.plot)\ntree_fit %&gt;%\n  extract_fit_engine() %&gt;%\n  rpart.plot(roundint = FALSE)\n\n\n\n\nFigure 1.4: 决策树的决策过程\n\n\n\n\nYou can use your fitted workflow for model and/or prediction explanations:\n\noverall variable importance, such as with the vip package\nflexible model explainers, such as with the DALEXtra package"
  },
  {
    "objectID": "11.tidymodels-basic.html#模型的评价",
    "href": "11.tidymodels-basic.html#模型的评价",
    "title": "1  Basic tidymodels",
    "section": "1.5 模型的评价",
    "text": "1.5 模型的评价\n先训练一个决策树模型，然后再评价该模型的性能。\n\nlibrary(tidymodels)\n\nset.seed(123)\ntaxi_split &lt;- initial_split(taxi, prop = 0.8, strata = tip)\ntaxi_train &lt;- training(taxi_split)\ntaxi_test &lt;- testing(taxi_split)\n\ntree_spec &lt;- decision_tree(cost_complexity = 0.0001, mode = \"classification\")\ntaxi_wflow &lt;- workflow(tip ~ ., tree_spec)\ntaxi_fit &lt;- fit(taxi_wflow, taxi_train)\n\n\n1.5.1 混淆矩阵\n混淆矩阵将真实值和预测值以热图的形成呈现出来。\n\nlibrary(ggplot2)\nlibrary(forcats)\np1 = augment(taxi_fit, new_data = taxi_train) %&gt;%\n  conf_mat(truth = tip, estimate = .pred_class) %&gt;%\n  autoplot(type = \"heatmap\") +\n  coord_equal()\n\n# 阳性与阴性\ndf = tibble(Truth = as_factor(c(\"yes\",\"yes\",\"no\",\"no\")),\nPrediction = as_factor(c(\"no\",\"yes\",\"no\",\"yes\")),\nlabel = c(\"FN\",\"TP\",\"TN\",\"FP\"))\np2 = ggplot(df, aes(Truth, Prediction, label = label)) +\ngeom_tile(fill = \"grey90\", color = \"grey\", linewidth = 1) +\ngeom_text() +\ncoord_equal() +\ntheme_minimal()\n\naplot::plot_list(p1, p2, labels = c(\"A\",\"B\"))\n\n\n\n\nFigure 1.5: 混淆矩阵\n\n\n\n\n\n\n1.5.2 准确性\n根据 Equation 1.1 可以计算准确性为 0.927625。\n\naccuracy = \\frac{TP + TN}{TP+FP+TN+FN}\n\\tag{1.1}\n\naugment(taxi_fit, new_data = taxi_train) %&gt;%\n  accuracy(truth = tip, estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.928\n\n\n\n\n1.5.3 敏感性\n根据 Equation 1.2 可以计算其数值为 0.9941766。\n\nsensitivity = \\frac{TP}{TP+FP}\n\\tag{1.2}\n\naugment(taxi_fit, new_data = taxi_train) %&gt;%\n  sensitivity(truth = tip, estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 sensitivity binary         0.994\n\n\n\n\n1.5.4 特异性\n根据 Equation 1.3 可以计算其数值为 0.1298701。\n\nsensitivity = \\frac{TP}{TP+FP}\n\\tag{1.3}\n\naugment(taxi_fit, new_data = taxi_train) %&gt;%\n  specificity(truth = tip, estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 specificity binary         0.130\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n敏感性告诉我们，测试有多大程度上能够捕捉到真正的阳性实例，即对于实际为阳性的样本，测试有多大可能性能够正确地识别出它们。\n特异性告诉我们，测试有多大程度上能够正确地排除实际为阴性的样本，即对于实际为阴性的样本，测试有多大可能性能够正确地将它们识别为阴性。\n准确率是一个综合性指标，衡量了分类模型对于所有样本的整体预测准确性。具体而言，它表示模型正确预测的样本在所有样本中的比例。\n\n\n\n使用 metric_set() 可以一次获取多个指标（另见 Figure 1.6）。\n\ntaxi_metrics &lt;- metric_set(accuracy, specificity, sensitivity)\n\naugment(taxi_fit, new_data = taxi_train) %&gt;%\n  taxi_metrics(truth = tip, estimate = .pred_class)\n\n# A tibble: 3 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.928\n2 specificity binary         0.130\n3 sensitivity binary         0.994\n\n\n\n\n\n\n\nFigure 1.6: 敏感性和特异性通常是一对矛盾的指标，即提高敏感性可能会降低特异性，反之亦然。在实际应用中，选择哪个指标更重要取决于具体的问题和应用背景。\n\n\n\n\n\n\n1.5.5 ROC 曲线\nROC（Receiver Operating Characteristic）曲线是一种用于评估二分类模型性能的图形工具。以下是关于ROC曲线的定义和解释：\n\n定义：\n\nROC曲线是一种以假正例率（False Positive Rate，FPR）为横轴、真正例率（True Positive Rate，TPR或敏感性）为纵轴的图形。它显示了在不同阈值下，模型的真正例率和假正例率之间的权衡关系。\n\n绘制方式：\n\n在ROC曲线中，横轴表示FPR，纵轴表示TPR。模型的输出概率或分数被用作不同阈值，从而生成一系列的TPR和FPR值。\n\n解释：\n\nROC曲线能够展示在不同分类阈值下，模型在识别正例（阳性类别）和负例（阴性类别）方面的性能。理想情况下，ROC曲线越靠近左上角，模型性能越好，因为在那里，TPR较高而FPR较低。\n\nAUC值：\n\nROC曲线下的面积（Area Under the Curve，AUC）也是一个常用的性能度量。AUC值越接近1，表示模型性能越好。AUC值为0.5时，表示模型的性能等同于随机猜测。\n\n示例：\n\n一个理想的ROC曲线会沿着左上角的边缘，最终达到（0, 1）点。一般情况下，ROC曲线在图形上是向左上凸起的。\n\n\n在实际应用中，ROC曲线和AUC值是评估分类模型性能的重要工具，尤其在处理不同类别分布和不同阈值的情况下。\ngiven that sensitivity is the true positive rate, and specificity is the true negative rate. Hence 1 - specificity is the false positive rate.\nWe can use the area under the ROC curve as a classification metric:\n\nROC AUC = 1 💯\nROC AUC = 1/2 😢\n\n\n# Assumes _first_ factor level is event; there are options to change that\naugment(taxi_fit, new_data = taxi_train) %&gt;% \n  roc_curve(truth = tip, .pred_yes) %&gt;%\n  slice(1, 20, 50)\n\n# A tibble: 3 × 3\n  .threshold specificity sensitivity\n       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1   -Inf           0         1      \n2      0.783       0.209     0.981  \n3      1           1         0.00135\n\naugment(taxi_fit, new_data = taxi_train) %&gt;% \n  roc_auc(truth = tip, .pred_yes)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.691\n\n\nFigure 1.7 显示了上面这个模型的 ROC 曲线。\n\naugment(taxi_fit, new_data = taxi_train) %&gt;% \n  roc_curve(truth = tip, .pred_yes) %&gt;%\n  autoplot()\n\n\n\n\nFigure 1.7: 这个模型的 ROC AUC 值为 0.691。\n\n\n\n\n\n\n1.5.6 过拟合\n将训练得到的模型分别用于训练数据集和测试数据集，比较二者预测的准确率，可以发现预测训练数据集的结果优于测试数据集。这就是模型的过拟合现象（Figure 1.8）。\n\n\n\nFigure 1.8: 过拟合\n\n\n首先，比较一下模型的准确性指标。\n\n# 模型预测训练数据集\ntaxi_fit %&gt;%\n  augment(taxi_train) %&gt;%\n  accuracy(tip, .pred_class)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.928\n\n# 模型预测测试数据集\ntaxi_fit %&gt;%\n  augment(taxi_test) %&gt;%\n  accuracy(tip, .pred_class)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.908\n\n\n其次，比较一下 Brier 分数。\n\ntaxi_fit %&gt;%\n  augment(taxi_train) %&gt;%\n  brier_class(tip, .pred_yes)\n\n# A tibble: 1 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 brier_class binary        0.0632\n\ntaxi_fit %&gt;%\n  augment(taxi_test) %&gt;%\n  brier_class(tip, .pred_yes)\n\n# A tibble: 1 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 brier_class binary        0.0782\n\n\nBrier分数（Brier Score）是一种用于评估分类模型性能的指标。对于二分类问题，Brier分数的计算公式如下：\n\nBrier\\ Score = \\frac{1}{N} \\sum_{i=1}^{N} (f_i - o_i)^2\n\n其中：\n\nN 是样本数；\nf_i 是模型对事件发生的概率的预测值；\no_i 是实际观测到的二分类结果，取值为0或1（例如，事件未发生为0，事件发生为1）。\n\nBrier分数的取值范围在0到1之间，0表示完美预测，1表示最差的预测。较低的Brier分数表示模型对观测结果的概率预测更准确。所以，仍然可以发现模型过拟合的现象。\n\n\n1.5.7 交叉验证\n在不使用测试数据集的前提下，能不能比较模型的参数？这就要用到交叉验证。vfold_cv() 函数默认将训练数据集中的十分之一（v = 10）取出来，用于计算、比较模型的性能参数。\n\nset.seed(123)\ntaxi_folds &lt;- vfold_cv(taxi_train, v = 10, strata = tip)\ntaxi_folds\n\n#  10-fold cross-validation using stratification \n# A tibble: 10 × 2\n   splits             id    \n   &lt;list&gt;             &lt;chr&gt; \n 1 &lt;split [7200/800]&gt; Fold01\n 2 &lt;split [7200/800]&gt; Fold02\n 3 &lt;split [7200/800]&gt; Fold03\n 4 &lt;split [7200/800]&gt; Fold04\n 5 &lt;split [7200/800]&gt; Fold05\n 6 &lt;split [7200/800]&gt; Fold06\n 7 &lt;split [7200/800]&gt; Fold07\n 8 &lt;split [7200/800]&gt; Fold08\n 9 &lt;split [7200/800]&gt; Fold09\n10 &lt;split [7200/800]&gt; Fold10\n\n\n使用 fit_resamples() 函数来对多次取样的数据进行拟合，使用 collect_mertics() 评价模型的性能。\n\ntaxi_res &lt;- fit_resamples(taxi_wflow, taxi_folds)\ntaxi_res\n\n# Resampling results\n# 10-fold cross-validation using stratification \n# A tibble: 10 × 4\n   splits             id     .metrics         .notes          \n   &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;          \n 1 &lt;split [7200/800]&gt; Fold01 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 2 &lt;split [7200/800]&gt; Fold02 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 3 &lt;split [7200/800]&gt; Fold03 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 4 &lt;split [7200/800]&gt; Fold04 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 5 &lt;split [7200/800]&gt; Fold05 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 6 &lt;split [7200/800]&gt; Fold06 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 7 &lt;split [7200/800]&gt; Fold07 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 8 &lt;split [7200/800]&gt; Fold08 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 9 &lt;split [7200/800]&gt; Fold09 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n10 &lt;split [7200/800]&gt; Fold10 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n\n\n\ntaxi_res %&gt;%\n  collect_metrics()\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary     0.915    10 0.00309 Preprocessor1_Model1\n2 roc_auc  binary     0.624    10 0.0105  Preprocessor1_Model1\n\n\n\n\n\n\n\n\nNote\n\n\n\ncollect_metrics() is one of a suite of collect_*() functions that can be used to work with columns of tuning results. Most columns in a tuning result prefixed with . have a corresponding collect_*() function with options for common summaries.\n\n\n交叉验证通过重采样和性能比较，使得我们可以在仅使用训练集就可以可靠地比较模型的性能。\n\n\n\n\n\n\nWarning\n\n\n\n记住：\n\n训练集会给出过于乐观的指标\n测试集非常宝贵\n\n\n\n\n# Save the assessment set results\nctrl_taxi &lt;- control_resamples(save_pred = TRUE)\ntaxi_res &lt;- fit_resamples(taxi_wflow, taxi_folds, control = ctrl_taxi)\n\ntaxi_res\n\n# Resampling results\n# 10-fold cross-validation using stratification \n# A tibble: 10 × 5\n   splits             id     .metrics         .notes           .predictions\n   &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;           &lt;list&gt;      \n 1 &lt;split [7200/800]&gt; Fold01 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 2 &lt;split [7200/800]&gt; Fold02 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 3 &lt;split [7200/800]&gt; Fold03 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 4 &lt;split [7200/800]&gt; Fold04 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 5 &lt;split [7200/800]&gt; Fold05 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 6 &lt;split [7200/800]&gt; Fold06 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 7 &lt;split [7200/800]&gt; Fold07 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 8 &lt;split [7200/800]&gt; Fold08 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 9 &lt;split [7200/800]&gt; Fold09 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n10 &lt;split [7200/800]&gt; Fold10 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n\n# Save the assessment set results\ntaxi_preds &lt;- collect_predictions(taxi_res)\ntaxi_preds\n\n# A tibble: 8,000 × 7\n   id     .pred_yes .pred_no  .row .pred_class tip   .config             \n   &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;       &lt;fct&gt; &lt;chr&gt;               \n 1 Fold01     0.938   0.0615    14 yes         yes   Preprocessor1_Model1\n 2 Fold01     0.946   0.0544    19 yes         yes   Preprocessor1_Model1\n 3 Fold01     0.973   0.0269    33 yes         yes   Preprocessor1_Model1\n 4 Fold01     0.903   0.0971    43 yes         yes   Preprocessor1_Model1\n 5 Fold01     0.973   0.0269    74 yes         yes   Preprocessor1_Model1\n 6 Fold01     0.903   0.0971   103 yes         yes   Preprocessor1_Model1\n 7 Fold01     0.915   0.0851   104 yes         no    Preprocessor1_Model1\n 8 Fold01     0.903   0.0971   124 yes         yes   Preprocessor1_Model1\n 9 Fold01     0.667   0.333    126 yes         yes   Preprocessor1_Model1\n10 Fold01     0.949   0.0510   128 yes         yes   Preprocessor1_Model1\n# ℹ 7,990 more rows\n\n# Evaluating model performance\ntaxi_preds %&gt;% \n  group_by(id) %&gt;%\n  taxi_metrics(truth = tip, estimate = .pred_class)\n\n# A tibble: 30 × 4\n   id     .metric  .estimator .estimate\n   &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n 1 Fold01 accuracy binary         0.905\n 2 Fold02 accuracy binary         0.925\n 3 Fold03 accuracy binary         0.926\n 4 Fold04 accuracy binary         0.915\n 5 Fold05 accuracy binary         0.902\n 6 Fold06 accuracy binary         0.912\n 7 Fold07 accuracy binary         0.906\n 8 Fold08 accuracy binary         0.91 \n 9 Fold09 accuracy binary         0.918\n10 Fold10 accuracy binary         0.931\n# ℹ 20 more rows\n\ntaxi_res\n\n# Resampling results\n# 10-fold cross-validation using stratification \n# A tibble: 10 × 5\n   splits             id     .metrics         .notes           .predictions\n   &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;           &lt;list&gt;      \n 1 &lt;split [7200/800]&gt; Fold01 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 2 &lt;split [7200/800]&gt; Fold02 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 3 &lt;split [7200/800]&gt; Fold03 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 4 &lt;split [7200/800]&gt; Fold04 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 5 &lt;split [7200/800]&gt; Fold05 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 6 &lt;split [7200/800]&gt; Fold06 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 7 &lt;split [7200/800]&gt; Fold07 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 8 &lt;split [7200/800]&gt; Fold08 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 9 &lt;split [7200/800]&gt; Fold09 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n10 &lt;split [7200/800]&gt; Fold10 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n\n\n交叉验证的蒙特卡洛方法，以及创建验证数据集。\n\nset.seed(322)\n# use the Monte Carlo Cross-Validation\nmc_cv(taxi_train, times = 10)\n\n# Monte Carlo cross-validation (0.75/0.25) with 10 resamples  \n# A tibble: 10 × 2\n   splits              id        \n   &lt;list&gt;              &lt;chr&gt;     \n 1 &lt;split [6000/2000]&gt; Resample01\n 2 &lt;split [6000/2000]&gt; Resample02\n 3 &lt;split [6000/2000]&gt; Resample03\n 4 &lt;split [6000/2000]&gt; Resample04\n 5 &lt;split [6000/2000]&gt; Resample05\n 6 &lt;split [6000/2000]&gt; Resample06\n 7 &lt;split [6000/2000]&gt; Resample07\n 8 &lt;split [6000/2000]&gt; Resample08\n 9 &lt;split [6000/2000]&gt; Resample09\n10 &lt;split [6000/2000]&gt; Resample10\n\n# create validation set\ntaxi_val_split &lt;- initial_validation_split(taxi, strata = tip)\nvalidation_set(taxi_val_split)\n\n# A tibble: 1 × 2\n  splits              id        \n  &lt;list&gt;              &lt;chr&gt;     \n1 &lt;split [6000/2000]&gt; validation"
  },
  {
    "objectID": "11.tidymodels-basic.html#随机森林模型",
    "href": "11.tidymodels-basic.html#随机森林模型",
    "title": "1  Basic tidymodels",
    "section": "1.6 随机森林模型",
    "text": "1.6 随机森林模型\n在了解了模型评价指标之后，我们再建一个随机森林模型，看看这个模型的性能是不是会优于决策树。\n随机森林（Random Forest）是一种集成学习（Ensemble Learning）方法，用于解决分类和回归问题。它建立在决策树的基础上，通过构建多个决策树并结合它们的预测结果来提高模型的性能和鲁棒性。\n以下是随机森林模型的主要特点和工作原理：\n\n决策树基学习器： 随机森林由多个决策树组成。每个决策树都是独立训练的，采用不同的随机子集数据。这有助于防止过拟合，增加模型的泛化能力。\n随机子集（Bootstrap抽样）： 在训练每个决策树时，随机森林使用自助抽样（Bootstrap Sampling）从训练集中随机选择一个子集。这意味着每个决策树的训练数据都是从原始训练集中有放回地随机抽取的。\n随机特征选择： 在每个决策树的节点上，随机森林只考虑特征的随机子集进行划分。这样做有助于确保每个决策树的多样性，防止所有决策树过于相似。\n投票机制： 对于分类问题，随机森林通过多数投票原则（Majority Voting）来确定最终的分类结果。对于回归问题，随机森林取多个决策树的平均预测结果。\n高性能和鲁棒性： 随机森林通常对于各种类型的数据和问题都表现得很好。它们能够处理大量特征和样本，具有较强的鲁棒性，对于处理噪声和复杂关系也有良好的适应性。\n\n随机森林的主要优势在于其简单而强大的集成学习策略，能够有效地降低过拟合风险，并在许多实际应用中表现优异。由于其可解释性、鲁棒性和高性能，随机森林成为了许多数据科学和机器学习问题的首选模型之一。\nBootstrap aggregating，通常简称为 Bagging，是一种集成学习的方法，旨在提高模型的稳定性和准确性。它的核心思想是通过对原始数据集进行自助抽样（Bootstrap Sampling），创建多个数据子集，然后分别训练多个模型，最后将它们的预测结果进行组合。\n下面使用随机森林模型建模，并检查得到模型的性能指标。\n\n# initialize a random forest model\nrf_spec &lt;- rand_forest(trees = 1000, mode = \"classification\")\nrf_spec\n\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  trees = 1000\n\nComputational engine: ranger \n\n# Create a random forest model (workflow)\nrf_wflow &lt;- workflow(tip ~ ., rf_spec)\nrf_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\ntip ~ .\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  trees = 1000\n\nComputational engine: ranger \n\nctrl_taxi &lt;- control_resamples(save_pred = TRUE)\n\n# Random forest uses random numbers so set the seed first\nset.seed(2)\ntaxi_folds = vfold_cv(taxi_train, strata = tip)\nrf_res &lt;- fit_resamples(rf_wflow, taxi_folds, control = ctrl_taxi)\ncollect_metrics(rf_res)\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary     0.923    10 0.00336 Preprocessor1_Model1\n2 roc_auc  binary     0.612    10 0.0178  Preprocessor1_Model1\n\n# taxi_split has train + test info\nfinal_fit &lt;- last_fit(rf_wflow, taxi_split) \nfinal_fit\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits              id               .metrics .notes   .predictions .workflow \n  &lt;list&gt;              &lt;chr&gt;            &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n1 &lt;split [8000/2000]&gt; train/test split &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;\n\n## What is in final_fit\ncollect_metrics(final_fit)\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.914 Preprocessor1_Model1\n2 roc_auc  binary         0.644 Preprocessor1_Model1\n\n# metrics computed with the test set\ncollect_predictions(final_fit)\n\n# A tibble: 2,000 × 7\n   id               .pred_yes .pred_no  .row .pred_class tip   .config          \n   &lt;chr&gt;                &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;       &lt;fct&gt; &lt;chr&gt;            \n 1 train/test split     0.958  0.0415      4 yes         yes   Preprocessor1_Mo…\n 2 train/test split     0.931  0.0689     10 yes         yes   Preprocessor1_Mo…\n 3 train/test split     0.969  0.0313     19 yes         yes   Preprocessor1_Mo…\n 4 train/test split     0.889  0.111      23 yes         yes   Preprocessor1_Mo…\n 5 train/test split     0.945  0.0551     28 yes         yes   Preprocessor1_Mo…\n 6 train/test split     0.981  0.0193     34 yes         yes   Preprocessor1_Mo…\n 7 train/test split     0.952  0.0476     35 yes         yes   Preprocessor1_Mo…\n 8 train/test split     0.936  0.0637     38 yes         yes   Preprocessor1_Mo…\n 9 train/test split     0.991  0.00895    40 yes         yes   Preprocessor1_Mo…\n10 train/test split     0.947  0.0526     42 yes         no    Preprocessor1_Mo…\n# ℹ 1,990 more rows\n\n## What is in final_fit\nextract_workflow(final_fit)\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\ntip ~ .\n\n── Model ───────────────────────────────────────────────────────────────────────\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, num.trees = ~1000,      num.threads = 1, verbose = FALSE, seed = sample.int(10^5,          1), probability = TRUE) \n\nType:                             Probability estimation \nNumber of trees:                  1000 \nSample size:                      8000 \nNumber of independent variables:  6 \nMtry:                             2 \nTarget node size:                 10 \nVariable importance mode:         none \nSplitrule:                        gini \nOOB prediction error (Brier s.):  0.07054061"
  },
  {
    "objectID": "11.tidymodels-basic.html#模型的调优",
    "href": "11.tidymodels-basic.html#模型的调优",
    "title": "1  Basic tidymodels",
    "section": "1.7 模型的调优",
    "text": "1.7 模型的调优\n\n1.7.1 为什么要调优？\n模型调整（tuning）是机器学习中的一个重要步骤，主要出于以下几个原因：\n\n提高模型性能：调整模型参数可以帮助我们找到最优的参数组合，从而使模型在特定任务上实现最佳性能。\n避免过拟合和欠拟合：通过适当的模型调整，我们可以平衡模型的偏差和方差，避免过拟合（模型在训练数据上表现良好，但在测试数据上表现差）和欠拟合（模型在训练数据和测试数据上的表现都不好）。\n适应不同的数据分布：不同的数据集有其特异性，可能需要不同的参数配置才能达到较好的效果。通过模型调度，我们可以针对具体的数据集优化模型。\n增加模型的泛化能力：通过合理的参数设置，可以使模型对未知数据有更好的预测能力。\n\n\n\n1.7.2 适用的情况\n模型调优主要适用于以下几种情况：\n\n模型性能不佳：当模型的预测结果或分类效果不尽如人意时，我们需要对模型进行调优以提升其性能。\n数据集特性变化：当我们处理的数据集有显著的特性变化（例如特征分布改变，噪声增加等）时，我们需要对原有模型进行调整以适应新的数据特性。\n模型过拟合或欠拟合：当模型在训练集上表现良好，但在测试集上表现较差（过拟合），或者模型在训练集和测试集上的表现都不佳（欠拟合）时，我们需要通过调整模型参数来解决这些问题。\n初始参数设置不合理：如果模型的初始参数设置并不合适，可能会导致模型的学习效率较低或者无法到达最优解，此时需要对模型进行调优。\n针对特定任务优化模型：当我们希望模型在某个特定任务上有更好的表现时，我们可以对模型做针对性的调优。\n\n总的来说，只要是希望提升模型性能，适应数据变化，或者解决模型的过拟合和欠拟合问题，我们都可以通过模型调优来实现。\n\n\n1.7.3 常见策略\n最常见的模型调优策略主要有以下几种：\n\n网格搜索（Grid Search）：这是一种穷举搜索的方法，我们为每一个参数设定一组值，然后通过遍历每个参数可能的组合来寻找最优解。\n随机搜索（Random Search）：不同于网格搜索的穷举性质，它在每个参数的可能值中随机选取一部分进行组合，然后从这些组合中寻找最优解。\n手动搜索（Manual Search）：也称为人工搜索，即由研究者根据经验或者对问题的理解，手动设定并调整参数。这种方式需要有一定的专业知识和经验，但在某些情况下可能会找到更好的解。\n自动化搜索（Automated Search）：这类方法，如贝叶斯优化、遗传算法等，使用算法自动寻找最优参数组合。相比前面几种方法，这些方法可以在更大的范围内寻找最优参数，并且节省了人工参与调整的时间。\n\n以上就是目前最常用的模型调优策略，选择哪种策略依赖于具体的需求、问题复杂度以及可用资源。\n\n\n1.7.4 调优模型\ntune_grid() works similar to fit_resamples() but covers multiple parameter values:\n\nrf_spec &lt;- rand_forest(min_n = tune()) %&gt;% \n  set_mode(\"classification\")\n\nrf_wflow &lt;- workflow(tip ~ ., rf_spec)\nrf_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\ntip ~ .\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  min_n = tune()\n\nComputational engine: ranger \n\n## Try out multiple values\nset.seed(22)\nrf_res &lt;- tune_grid(\n  rf_wflow,\n  taxi_folds,\n  grid = 5\n)\n\n## Compare results\n# Inspecting results and selecting the best-performing hyperparameter(s):\nshow_best(rf_res)\n\nWarning: No value of `metric` was given; metric 'roc_auc' will be used.\n\n\n# A tibble: 5 × 7\n  min_n .metric .estimator  mean     n std_err .config             \n  &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1    33 roc_auc binary     0.620    10  0.0157 Preprocessor1_Model1\n2    31 roc_auc binary     0.619    10  0.0163 Preprocessor1_Model3\n3    21 roc_auc binary     0.614    10  0.0169 Preprocessor1_Model4\n4     6 roc_auc binary     0.613    10  0.0180 Preprocessor1_Model2\n5    13 roc_auc binary     0.608    10  0.0186 Preprocessor1_Model5\n\nbest_parameter &lt;- select_best(rf_res)\n\nWarning: No value of `metric` was given; metric 'roc_auc' will be used.\n\nbest_parameter\n\n# A tibble: 1 × 2\n  min_n .config             \n  &lt;int&gt; &lt;chr&gt;               \n1    33 Preprocessor1_Model1\n\n## The final fit\n(rf_wflow &lt;- finalize_workflow(rf_wflow, best_parameter))\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\ntip ~ .\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  min_n = 33\n\nComputational engine: ranger \n\n(final_fit &lt;- last_fit(rf_wflow, taxi_split))\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits              id               .metrics .notes   .predictions .workflow \n  &lt;list&gt;              &lt;chr&gt;            &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n1 &lt;split [8000/2000]&gt; train/test split &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;\n\ncollect_metrics(final_fit)\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.913 Preprocessor1_Model1\n2 roc_auc  binary         0.648 Preprocessor1_Model1\n\n\n这段代码是在使用 tidymodels 包中的函数进行随机森林模型的参数调优和最后的拟合。下面是对各个部分的解释：\n\nrf_spec &lt;- rand_forest(min_n = tune()) %&gt;% set_mode(\"classification\")\n这行代码定义了一个随机森林分类器，并设定 min_n 参数为待调整的参数。\nrf_wflow &lt;- workflow(tip ~ ., rf_spec)\n这行代码创建了一个工作流，将特征和模型规格结合起来。\nrf_res &lt;- tune_grid(rf_wflow, taxi_folds, grid = 5)\n这行代码使用 tune_grid 函数在预定义的交叉验证折叠 taxi_folds 上进行网格搜索，尝试 grid=5 指定的不同的 min_n 参数值。\nshow_best(rf_res)\n这行代码显示了最佳性能的模型参数。\nbest_parameter &lt;- select_best(rf_res)\n这行代码选择出表现最好的模型参数。\nrf_wflow &lt;- finalize_workflow(rf_wflow, best_parameter)\n这行代码将最佳参数设置到工作流中。\nfinal_fit &lt;- last_fit(rf_wflow, taxi_split)\n这行代码在训练集上用确定的最佳参数进行最后的模型拟合。\ncollect_metrics(final_fit)\n这行代码收集并显示最终模型在训练集和测试集上的性能指标。"
  },
  {
    "objectID": "11.tidymodels-basic.html#参考资料",
    "href": "11.tidymodels-basic.html#参考资料",
    "title": "1  Basic tidymodels",
    "section": "1.8 参考资料",
    "text": "1.8 参考资料\n\nhttps://www.tidymodels.org/\nhttps://www.tmwr.org/\nhttp://www.feat.engineering/\nhttps://smltar.com/"
  },
  {
    "objectID": "12.tidymodels-advanced.html#使用-recipes-进行特征工程",
    "href": "12.tidymodels-advanced.html#使用-recipes-进行特征工程",
    "title": "2  Advanced tidymodels",
    "section": "2.1 使用 recipes 进行特征工程",
    "text": "2.1 使用 recipes 进行特征工程\n特征工程（Feature Engineering）是指将原始数据转化为模型可以利用的特征的过程。通过这个过程，我们能够更好地表示潜在问题，从而提高模型的性能。\n特征（Feature）可以理解为模型预测所需要的某种表示或者属性。譬如在房价预测问题中，房屋的面积、房间数量、地段等都可以被视为特征。\n以下是一些常见的特征表示：\n\n交互项（Interactions）：这是通过组合原有的特征创建新的特征。例如，如果我们有两个特征 A 和 B，我们可以创建一个新的特征 A*B 来表示 A 和 B 的交互效应。\n多项式扩展/样条函数（Polynomial expansions/splines）：这是通过对原有特征进行非线性转换创建新的特征。例如，如果我们有一个特征 X，我们可以创建新的特征 X^2, X^3 等来捕捉 X 的非线性效应1。\n\n\n主成分分析（PCA）特征提取：这是一种降维技术，通过将原有的多个特征转化为少数几个主成分（也就是新的特征），从而保留数据中的主要信息。这种方法通常用于处理具有多重共线性的高维数据。\n\n以上就是特征工程的基本概念以及一些常见的特征表示方式。通过合适的特征工程，我们能够提取出更有价值的信息，从而帮助模型更好地理解和预测问题。\n\n2.1.1 特征工程的软件包\nrecipes 包是 R 语言中的一个用于数据预处理和特征工程的工具包。它提供了一种灵活且强大的方式来创建和管理模型需要的预处理步骤。这些步骤可以包括数据清洗、数据转换、特征选择、特征构建等。\n以下是 recipes 包的一些主要功能：\n\n数据预处理：recipes 包提供了许多函数来进行数据预处理，如缩放和中心化（标准化）、处理缺失值、离群值检测和处理等。\n特征选择：recipes 包可以帮助我们选择最重要的特征，以降低模型的复杂性和过拟合的风险。\n特征工程：recipes 包可以用于创建新的特征，如基于现有特征的数学变换（例如平方、对数变换等）、交互项、虚拟变量（独热编码）等。\n简易操作：所有的预处理步骤都可以被封装在一个 “recipe” 对象中，这使得整个预处理过程更加组织化和可复现。\n\n总的来说，recipes 包是一个非常有用的工具，它可以帮助我们更有效地进行数据预处理和特征工程，并应用到模型开发的工作流程中。\n\n\n2.1.2 对时间变量进行特征提取\n对时间变量进行特征提取有很多常见的方法，以下是一些例子：\n\n时间分解：将时间戳分解为年、月、日、小时、分钟和秒。这可以帮助我们理解时间的不同组成部分如何影响目标变量。\n季节性特征：你可以创建表示季节性信息的特征，如季度、月份、一周中的哪一天、一天中的哪个时段等。\n节假日和事件：如果数据中包含特定的节假日或事件，这可能会影响到目标变量。此时，可以创建一个二元特征来表示这些特殊的日期或事件。\n时间间隔：计算两个日期之间的时间差，例如用户上次购买产品到现在的天数。\n趋势：如果你的数据集跨越了很长一段时间，那么可能存在一些长期趋势。在这种情况下，你可以创建一个特征来捕捉这种趋势，例如数据点距离开始日期的天数。\n滑动窗口统计：例如过去7天或30天的平均值、最小值、最大值等。\n\n这些只是一部分常见的处理时间变量的策略，具体采用哪种策略需要结合实际的业务场景和问题来决定。\n\n\n2.1.3 对因子变量进行特征提取\n对因子变量进行特征提取的方法有很多种，下面列出了一些常见的方法：\n\n独热编码（One-Hot Encoding）：也被称为虚拟变量，这是最常见的编码方法。每个类别都被转换为一个二元变量（即0或1）。例如，如果有一个颜色的因子变量包含“红色”，“蓝色”和“绿色”三个级别，那么我们可以创建三个新的二元变量，分别表示这个观察是不是红色，是不是蓝色，是不是绿色。\n标签编码（Label Encoding）：将每个类别映射到一个整数。这种方式适合于类别之间存在自然顺序的情况，比如评级（高、中、低）。\n二进制编码（Binary Encoding）：首先，将所有类别按照一定的顺序编码为连续的整数；然后，将这些整数转换为二进制形式。这种方式特别适合高基数特征，因为它可以大大减少新生成的特征的数量。\n哈希编码（Hashing Encoding）：哈希编码通过哈希函数，将类别映射到更小的固定长度的列。这种方式对于处理具有大量类别的变量非常有效。\n目标编码（Target Encoding）：也被称为均值编码或者响应编码，这种方式是基于类别目标变量的平均值来对类别进行编码。它可以帮助模型捕捉到类别和目标变量之间可能存在的关系，但是如果不慎重处理，容易导致过拟合。\n嵌入式方法（Embedding）：这是一种神经网络处理因子变量的方式，将每个类别映射到一个多维空间中的一个点，可以捕获更复杂的关系。\n\n这些方法各有优缺点，选择哪一种取决于具体的问题和数据。例如，独热编码可能会产生很多列，而哈希编码可能会产生碰撞（不同的类别映射到同一哈希值）。\n下面以房价预测模型为例，展示对数据中的因子变量进行特征提取的方法。\n\n# recipes-startup\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n\n\n✔ broom        1.0.5     ✔ recipes      1.0.9\n✔ dials        1.2.0     ✔ rsample      1.2.0\n✔ dplyr        1.1.4     ✔ tibble       3.2.1\n✔ ggplot2      3.4.4     ✔ tidyr        1.3.0\n✔ infer        1.0.5     ✔ tune         1.1.2\n✔ modeldata    1.2.0     ✔ workflows    1.1.3\n✔ parsnip      1.1.1     ✔ workflowsets 1.0.1\n✔ purrr        1.0.2     ✔ yardstick    1.2.0\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n\nlibrary(modeldatatoo)\n\n# Add another package:\nlibrary(textrecipes)\n\n# Max's usual settings: \ntidymodels_prefer()\ntheme_set(theme_bw())\noptions(\n  pillar.advice = FALSE, \n  pillar.min_title_chars = Inf\n)\n\n\n# data-import\ndata(hotel_rates)\nset.seed(295)\nhotel_rates &lt;- \n  hotel_rates %&gt;% \n  sample_n(5000) %&gt;% \n  arrange(arrival_date) %&gt;% \n  select(-arrival_date) %&gt;% \n  mutate(\n    company = factor(as.character(company)),\n    country = factor(as.character(country)),\n    agent = factor(as.character(agent))\n  )\n\nset.seed(4028)\nhotel_split &lt;- initial_split(hotel_rates, strata = avg_price_per_room)\n\nhotel_train &lt;- training(hotel_split)\nhotel_test &lt;- testing(hotel_split)\n\nhotel_rs &lt;- vfold_cv(hotel_train, strata = avg_price_per_room)\nhotel_rs\n\n#  10-fold cross-validation using stratification \n# A tibble: 10 × 2\n   splits             id    \n   &lt;list&gt;             &lt;chr&gt; \n 1 &lt;split [3372/377]&gt; Fold01\n 2 &lt;split [3373/376]&gt; Fold02\n 3 &lt;split [3373/376]&gt; Fold03\n 4 &lt;split [3373/376]&gt; Fold04\n 5 &lt;split [3373/376]&gt; Fold05\n 6 &lt;split [3374/375]&gt; Fold06\n 7 &lt;split [3375/374]&gt; Fold07\n 8 &lt;split [3376/373]&gt; Fold08\n 9 &lt;split [3376/373]&gt; Fold09\n10 &lt;split [3376/373]&gt; Fold10\n\nhotel_rec &lt;- \n  recipe(avg_price_per_room ~ ., data = hotel_train) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors()) %&gt;% \n  step_corr(all_numeric_predictors(), threshold = 0.9)\nsummary(hotel_rec)\n\n# A tibble: 27 × 4\n   variable                type      role      source  \n   &lt;chr&gt;                   &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 lead_time               &lt;chr [2]&gt; predictor original\n 2 stays_in_weekend_nights &lt;chr [2]&gt; predictor original\n 3 stays_in_week_nights    &lt;chr [2]&gt; predictor original\n 4 adults                  &lt;chr [2]&gt; predictor original\n 5 children                &lt;chr [2]&gt; predictor original\n 6 babies                  &lt;chr [2]&gt; predictor original\n 7 meal                    &lt;chr [3]&gt; predictor original\n 8 country                 &lt;chr [3]&gt; predictor original\n 9 market_segment          &lt;chr [3]&gt; predictor original\n10 distribution_channel    &lt;chr [3]&gt; predictor original\n# ℹ 17 more rows\n\nhotel_rec &lt;- \n  recipe(avg_price_per_room ~ ., data = hotel_train) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors()) %&gt;% \n  embed::step_umap(all_numeric_predictors(), outcome = vars(avg_price_per_room))\nsummary(hotel_rec)\n\n# A tibble: 27 × 4\n   variable                type      role      source  \n   &lt;chr&gt;                   &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 lead_time               &lt;chr [2]&gt; predictor original\n 2 stays_in_weekend_nights &lt;chr [2]&gt; predictor original\n 3 stays_in_week_nights    &lt;chr [2]&gt; predictor original\n 4 adults                  &lt;chr [2]&gt; predictor original\n 5 children                &lt;chr [2]&gt; predictor original\n 6 babies                  &lt;chr [2]&gt; predictor original\n 7 meal                    &lt;chr [3]&gt; predictor original\n 8 country                 &lt;chr [3]&gt; predictor original\n 9 market_segment          &lt;chr [3]&gt; predictor original\n10 distribution_channel    &lt;chr [3]&gt; predictor original\n# ℹ 17 more rows\n\nhotel_rec &lt;- \n  recipe(avg_price_per_room ~ ., data = hotel_train) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors()) %&gt;% \n  embed::step_umap(all_numeric_predictors(), outcome = vars(avg_price_per_room))\nsummary(hotel_rec)\n\n# A tibble: 27 × 4\n   variable                type      role      source  \n   &lt;chr&gt;                   &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 lead_time               &lt;chr [2]&gt; predictor original\n 2 stays_in_weekend_nights &lt;chr [2]&gt; predictor original\n 3 stays_in_week_nights    &lt;chr [2]&gt; predictor original\n 4 adults                  &lt;chr [2]&gt; predictor original\n 5 children                &lt;chr [2]&gt; predictor original\n 6 babies                  &lt;chr [2]&gt; predictor original\n 7 meal                    &lt;chr [3]&gt; predictor original\n 8 country                 &lt;chr [3]&gt; predictor original\n 9 market_segment          &lt;chr [3]&gt; predictor original\n10 distribution_channel    &lt;chr [3]&gt; predictor original\n# ℹ 17 more rows\n\nhotel_rec &lt;- \n  recipe(avg_price_per_room ~ ., data = hotel_train) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors()) %&gt;% \n  step_spline_natural(arrival_date_num, deg_free = 10)\nsummary(hotel_rec)\n\n# A tibble: 27 × 4\n   variable                type      role      source  \n   &lt;chr&gt;                   &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 lead_time               &lt;chr [2]&gt; predictor original\n 2 stays_in_weekend_nights &lt;chr [2]&gt; predictor original\n 3 stays_in_week_nights    &lt;chr [2]&gt; predictor original\n 4 adults                  &lt;chr [2]&gt; predictor original\n 5 children                &lt;chr [2]&gt; predictor original\n 6 babies                  &lt;chr [2]&gt; predictor original\n 7 meal                    &lt;chr [3]&gt; predictor original\n 8 country                 &lt;chr [3]&gt; predictor original\n 9 market_segment          &lt;chr [3]&gt; predictor original\n10 distribution_channel    &lt;chr [3]&gt; predictor original\n# ℹ 17 more rows\n\n\n这段代码主要实现了对数据集 hotel_rates 进行预处理和特征工程的操作。具体来说：\n\n设置环境：加载需要的库并进行一些通用设置。\n数据导入和初步处理：首先从 hotel_rates 数据中随机抽取了5000个样本，然后去除了 arrival_date 列，最后将 company、country 和 agent 三列转换为因子类型。\n创建交叉验证分割：使用 vfold_cv 函数对训练集进行10折交叉验证分割，并按照 avg_price_per_room 这一列进行分层抽样。\n创建数据预处理步骤（recipe）：使用 recipe 函数创建一个数据预处理食谱。在这个过程中，执行了以下步骤：\n\n使用 step_dummy 将所有名义预测变量转换为虚拟（dummy）变量。\n使用 step_zv 删除所有零方差预测变量。\n使用 step_normalize 对所有数值预测变量进行标准化（即变换为均值为0、标准差为1的正态分布）。\n在第一个食谱中，使用 step_corr 删除所有与其他数值预测变量相关性大于0.9的预测变量。\n在第二、三个食谱中，使用 step_umap 对所有数值预测变量进行UMAP降维，并将 avg_price_per_room 作为目标变量。\n在第四个食谱中，使用 step_spline_natural 对 arrival_date_num 这一预测变量进行自然样条变换。\n\n\n每次创建完一个食谱后，都使用了 summary 函数查看了该食谱的内容。这样可以帮助我们理解每一步预处理操作对数据的影响。\n下面这段代码首先创建了一个名为 hotel_indicators 的预处理 “recipe”，然后利用这个 “recipe” 和简单线性回归模型建立了一个工作流（workflow），最后使用10折交叉验证的方式对这个工作流进行了拟合并评估模型性能。\n具体步骤包括：\n\nhotel_indicators &lt;-\n  recipe(avg_price_per_room ~ ., data = hotel_train) %&gt;% \n  step_YeoJohnson(lead_time) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_zv(all_predictors()) %&gt;% \n  step_spline_natural(arrival_date_num, deg_free = 10)\nsummary(hotel_indicators)\n\n# A tibble: 27 × 4\n   variable                type      role      source  \n   &lt;chr&gt;                   &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 lead_time               &lt;chr [2]&gt; predictor original\n 2 stays_in_weekend_nights &lt;chr [2]&gt; predictor original\n 3 stays_in_week_nights    &lt;chr [2]&gt; predictor original\n 4 adults                  &lt;chr [2]&gt; predictor original\n 5 children                &lt;chr [2]&gt; predictor original\n 6 babies                  &lt;chr [2]&gt; predictor original\n 7 meal                    &lt;chr [3]&gt; predictor original\n 8 country                 &lt;chr [3]&gt; predictor original\n 9 market_segment          &lt;chr [3]&gt; predictor original\n10 distribution_channel    &lt;chr [3]&gt; predictor original\n# ℹ 17 more rows\n\n\n这段代码是创建一个预处理数据的 “recipe”。主要步骤如下：\n\nrecipe(avg_price_per_room ~ ., data = hotel_train)：建立一个recipe对象，其目标（响应变量）是avg_price_per_room，预测变量是hotel_train数据集中的所有其他变量。\nstep_YeoJohnson(lead_time)：对lead_time列应用 Yeo-Johnson 变换。Yeo-Johnson 变换是一种用于正态化数据和管理异方差性的方法，可以用于正数、负数和零的数据。\nstep_dummy(all_nominal_predictors())：将所有名义型预测变量转换为虚拟（dummy）变量，也就是进行独热编码。这样做能够让我们把包含多类别的名义变量转换为二元变量，使得模型能够处理。\nstep_zv(all_predictors())：删除所有零方差预测变量。零方差预测变量是指在所有观察中值都相同的变量，这样的变量对模型预测通常没有帮助。\nstep_spline_natural(arrival_date_num, deg_free = 10)：对arrival_date_num这一变量进行自然样条变换，自由度设置为10。样条变换能够帮助处理非线性关系，尤其是当我们预期某个预测变量和响应变量之间存在复杂的非线性关系时。\n\n这个recipe定义了数据预处理和特征工程的步骤，后续可以通过 prep() 和 bake() 函数来实施这个recipe。\n\nreg_metrics &lt;- metric_set(mae, rsq)\n\n定义评估指标：通过 metric_set(mae, rsq) 命令定义了两个模型评估指标，即平均绝对误差（mean absolute error，mae）和决定系数（R-squared，rsq）。\n\n\\begin{align}\nMAE &= \\frac{1}{n}\\sum_{i=1}^n |y_i - \\hat{y}_i| \\notag \\\\\nR^2 &= cor(y_i, \\hat{y}_i)^2\n\\end{align}\n\\tag{2.1}\n\nset.seed(9)\nhotel_lm_wflow &lt;-\n  workflow() %&gt;%\n  add_recipe(hotel_indicators) %&gt;%\n  add_model(linear_reg())\n\n创建工作流：建立了一个包含预处理 “recipe” 和简单线性回归模型的工作流。\n\nctrl &lt;- control_resamples(save_pred = TRUE)\nhotel_lm_res &lt;-\n  hotel_lm_wflow %&gt;%\n  fit_resamples(hotel_rs, control = ctrl, metrics = reg_metrics)\n\n→ A | warning: prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\n\n\nThere were issues with some computations   A: x1\n\n\nThere were issues with some computations   A: x2\n\n\nThere were issues with some computations   A: x9\nThere were issues with some computations   A: x9\n\n\n\n\n\n拟合并评估模型：通过 fit_resamples 函数将工作流应用到10折交叉验证的每一个分割中，并计算了在每一个分割中模型的评估指标。结果保存在 hotel_lm_res 中。\n\ncollect_metrics(hotel_lm_res)\n\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard   16.7      10 0.251   Preprocessor1_Model1\n2 rsq     standard    0.883    10 0.00532 Preprocessor1_Model1\n\n\n收集评估指标：使用 collect_metrics(hotel_lm_res) 命令收集模型的评估指标。\n\n# Since we used `save_pred = TRUE`\nlm_cv_pred &lt;- collect_predictions(hotel_lm_res)\nlm_cv_pred %&gt;% print(n = 7)\n\n# A tibble: 3,749 × 5\n  id     .pred  .row avg_price_per_room .config             \n  &lt;chr&gt;  &lt;dbl&gt; &lt;int&gt;              &lt;dbl&gt; &lt;chr&gt;               \n1 Fold01  26.2    31               54   Preprocessor1_Model1\n2 Fold01  23.0    35               48   Preprocessor1_Model1\n3 Fold01  68.9    45               50   Preprocessor1_Model1\n4 Fold01  60.2    47               55   Preprocessor1_Model1\n5 Fold01  48.6    59               52.8 Preprocessor1_Model1\n6 Fold01  49.0    67               49   Preprocessor1_Model1\n7 Fold01  49.0    72               49   Preprocessor1_Model1\n# ℹ 3,742 more rows\n\n\n收集预测结果：因为在 control_resamples 函数中设置了 save_pred = TRUE，所以我们可以通过 collect_predictions(hotel_lm_res) 命令收集模型在10折交叉验证的每一个分割中的预测结果。\n\n## Calibration Plot\n#| label: fig-lm-cal-plot\n#| fig-width: 5\n#| fig-height: 5\n\nlibrary(probably)\n\ncal_plot_regression(hotel_lm_res, alpha = 1 / 5)\n\n\n\n\n绘制校准图：使用 cal_plot_regression 函数绘制了模型的校准图，这是一种用于评估回归模型预测精度的可视化方法。"
  },
  {
    "objectID": "12.tidymodels-advanced.html#对-agent-进行特征工程",
    "href": "12.tidymodels-advanced.html#对-agent-进行特征工程",
    "title": "2  Advanced tidymodels",
    "section": "2.2 对 agent 进行特征工程",
    "text": "2.2 对 agent 进行特征工程\n\n2.2.1 使用分类汇总\n下面这段代码包括两部分：首先对agent字段进行统计并绘制直方图，然后构建一个新的预处理”recipe”，使用了一个新的步骤：step_other()。\n通过分析发现有一些 agent 出现的频率很低，这种情况下可以将其合并到 other 组中。\n\nagent_stats &lt;- \n  hotel_train %&gt;%\n  group_by(agent) %&gt;%\n  summarize(\n    ADR = mean(avg_price_per_room), \n    num_reservations = n(),\n    .groups = \"drop\"\n    ) %&gt;%\n  mutate(agent = reorder(agent, ADR))\n\nagent_stats %&gt;%   \n  ggplot(aes(x = num_reservations)) +\n  geom_histogram(bins = 30, col = \"blue\", fill = \"blue\", alpha = 1/3) +\n  labs(x = \"Number of reservations per agent\")\n\n\n\nagent_stats %&gt;%   \n  ggplot(aes(x = ADR)) +\n  geom_histogram(bins = 30, col = \"red\", fill = \"red\", alpha = 1/3) +\n  labs(x = \"Average ADR per agent\")\n\n\n\n\n代理统计：对 hotel_train 数据集按 agent 进行分组，并计算每个代理的平均房价（ADR）和预订数量（num_reservations）。然后, 这些统计信息被用来创建两个直方图，分别显示了每个代理的预订数量和平均ADR。\n\nretained_agents &lt;-\n  recipe(avg_price_per_room ~ ., data = hotel_train) %&gt;%\n  step_mutate(original = agent) %&gt;% \n  step_other(agent, threshold = 0.001) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_zv(all_predictors()) %&gt;% \n  step_spline_natural(arrival_date_num, deg_free = 10) %&gt;% \n  prep() %&gt;% \n  tidy(number = 2)\n\n(num_agents &lt;- length(unique(hotel_train$agent)))\n\n[1] 98\n\n(num_other &lt;- num_agents - length(retained_agents$retained))\n\n[1] 34\n\n\n新的预处理”recipe”：新的recipe和之前的非常相似，但增加了一个 step_other(agent, threshold = 0.001) 步骤。这一步把所有预测变量中在数据集中出现频率低于阈值（此处为0.001）的 agent 代理标记为 “Other”。然后，该预处理步骤通过 prep() 函数应用在数据上，结果通过 tidy(number = 2) 展示 out。\n\nhotel_other_rec &lt;-\n  recipe(avg_price_per_room ~ ., data = hotel_train) %&gt;% \n  step_YeoJohnson(lead_time) %&gt;%\n  step_other(agent, threshold = 0.001) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_zv(all_predictors()) %&gt;% \n  step_spline_natural(arrival_date_num, deg_free = 10)\n\nhotel_other_wflow &lt;-\n  hotel_lm_wflow %&gt;%\n  update_recipe(hotel_other_rec)\n\nhotel_other_res &lt;-\n  hotel_other_wflow %&gt;%\n  fit_resamples(hotel_rs, control = ctrl, metrics = reg_metrics)\n\n→ A | warning: prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\n\n\nThere were issues with some computations   A: x5\n\n\nThere were issues with some computations   A: x9\n\n\n\n\ncollect_metrics(hotel_other_res)\n\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard   16.8      10 0.245   Preprocessor1_Model1\n2 rsq     standard    0.883    10 0.00530 Preprocessor1_Model1\n\n\n更新工作流并重新拟合模型：使用 update_recipe(hotel_other_rec) 更新工作流中的 recipe，然后再次拟合样本，并收集评估指标。\n这段代码的目的是处理那些只有少数观察值的类别变量——这里是 agent 变量。这样做可以避免过拟合，并可能提高模型的泛化能力。\n\n\n2.2.2 使用哈希编码\n下面这段代码创建了一个新的预处理 “recipe”，并使用该 “recipe” 更新了工作流，然后再次拟合样本，并收集评估指标。\n主要改变是：在新的 “recipe” 中，对 agent 和 company 使用了哈希虚拟编码（hashing trick）。哈希虚拟编码可以有效地处理高基数的分类特征。这是一种维度约减技术，通过将所有类别映射到更小的固定数量的列来实现。\n具体步骤包括：\n\n创建新的预处理 “recipe”：新的 “recipe” 包含了以下步骤：\n\n对 lead_time 进行Yeo-Johnson变换。\n对 agent 和 company 进行哈希虚拟编码（默认生成32个有符号的指示列）。\n对其他名义预测变量进行普通的虚拟编码。\n删除所有零方差预测变量。\n对 arrival_date_num 进行自然样条变换。\n\n更新工作流：使用 update_recipe(hash_rec) 来更新工作流中的 “recipe”。\n重新拟合模型并收集评估指标：和之前相同，再次拟合样本，并收集评估指标。\n\n哈希虚拟编码是一种处理高基数分类特征的方法，对于有大量唯一值的分类变量（如 IP 地址，用户 ID 等）非常有用。\n\nhash_rec &lt;-\n  recipe(avg_price_per_room ~ ., data = hotel_train) %&gt;%\n  step_YeoJohnson(lead_time) %&gt;%\n  # Defaults to 32 signed indicator columns\n  step_dummy_hash(agent) %&gt;%\n  step_dummy_hash(company) %&gt;%\n  # Regular indicators for the others\n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_spline_natural(arrival_date_num, deg_free = 10)\n\nhotel_hash_wflow &lt;-\n  hotel_lm_wflow %&gt;%\n  update_recipe(hash_rec)\n\nhotel_hash_res &lt;-\n  hotel_hash_wflow %&gt;%\n  fit_resamples(hotel_rs, control = ctrl, metrics = reg_metrics)\n\n→ A | warning: prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\n\n\nThere were issues with some computations   A: x5\n\n\nThere were issues with some computations   A: x9\n\n\n\n\ncollect_metrics(hotel_hash_res)\n\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard   16.7      10 0.254   Preprocessor1_Model1\n2 rsq     standard    0.884    10 0.00572 Preprocessor1_Model1"
  },
  {
    "objectID": "12.tidymodels-advanced.html#debug-recipes",
    "href": "12.tidymodels-advanced.html#debug-recipes",
    "title": "2  Advanced tidymodels",
    "section": "2.3 Debug recipes",
    "text": "2.3 Debug recipes\n\nhash_rec_fit &lt;- prep(hash_rec)\nhash_rec_fit\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 26\n\n\n\n\n\n── Training information \n\n\nTraining data contained 3749 data points and no incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Yeo-Johnson transformation on: lead_time | Trained\n\n\n• Feature hashing with: agent | Trained\n\n\n• Feature hashing with: company | Trained\n\n\n• Dummy variables from: meal, country, market_segment, ... | Trained\n\n\n• Zero variance filter removed: dummyhash_agent_09, ... | Trained\n\n\n• Natural spline expansion: arrival_date_num | Trained\n\n# Get the transformation coefficient\ntidy(hash_rec_fit, number = 1)\n\n# A tibble: 1 × 3\n  terms     value id              \n  &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;           \n1 lead_time 0.173 YeoJohnson_IpstB\n\n# Get the processed data\nbake(hash_rec_fit, hotel_train %&gt;% slice(1:3), contains(\"_agent_\"))\n\n# A tibble: 3 × 30\n  dummyhash_agent_01 dummyhash_agent_02 dummyhash_agent_03 dummyhash_agent_04\n               &lt;int&gt;              &lt;int&gt;              &lt;int&gt;              &lt;int&gt;\n1                  0                  0                  0                  0\n2                  0                 -1                  0                  0\n3                  0                  0                  0                  0\n# ℹ 26 more variables: dummyhash_agent_05 &lt;int&gt;, dummyhash_agent_06 &lt;int&gt;,\n#   dummyhash_agent_07 &lt;int&gt;, dummyhash_agent_08 &lt;int&gt;,\n#   dummyhash_agent_10 &lt;int&gt;, dummyhash_agent_11 &lt;int&gt;,\n#   dummyhash_agent_12 &lt;int&gt;, dummyhash_agent_13 &lt;int&gt;,\n#   dummyhash_agent_14 &lt;int&gt;, dummyhash_agent_15 &lt;int&gt;,\n#   dummyhash_agent_16 &lt;int&gt;, dummyhash_agent_18 &lt;int&gt;,\n#   dummyhash_agent_19 &lt;int&gt;, dummyhash_agent_20 &lt;int&gt;, …"
  },
  {
    "objectID": "12.tidymodels-advanced.html#参考资料",
    "href": "12.tidymodels-advanced.html#参考资料",
    "title": "2  Advanced tidymodels",
    "section": "2.4 参考资料",
    "text": "2.4 参考资料\n\nOnce fit() is called on a workflow, changing the model does not re-fit the recipe.\nA list of all known steps is at https://www.tidymodels.org/find/recipes/.\nSome steps can be skipped when using predict().\nuse feature hashing to create a smaller set of indicator variables\nFeature hashing (for more see FES, SMLTAR, and TMwR):\nHash functions are meant to emulate randomness.\nThe order of the steps matters."
  },
  {
    "objectID": "12.tidymodels-advanced.html#footnotes",
    "href": "12.tidymodels-advanced.html#footnotes",
    "title": "2  Advanced tidymodels",
    "section": "",
    "text": "样条（Spline）是一种数学工具，用于在给定的数据点之间创建平滑曲线。它广泛应用于计算机图形学、数据插值和回归分析等领域。↩︎"
  },
  {
    "objectID": "13.tidymodels-extra.html#部署模型",
    "href": "13.tidymodels-extra.html#部署模型",
    "title": "3  Extra tidymodels",
    "section": "3.1 部署模型",
    "text": "3.1 部署模型\n使用 vetiver 包可以快速实现模型部署。\n这段代码包含了几个主要步骤：\n\n数据集的划分：使用tidymodels包中的initial_split()函数将taxi数据集划分为训练集和测试集，其中80%的数据用于训练，剩余20%的数据用于测试。这个划分是根据tip列（应该是目标变量）进行分层的。\n\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n\n\n✔ broom        1.0.5     ✔ recipes      1.0.9\n✔ dials        1.2.0     ✔ rsample      1.2.0\n✔ dplyr        1.1.4     ✔ tibble       3.2.1\n✔ ggplot2      3.4.4     ✔ tidyr        1.3.0\n✔ infer        1.0.5     ✔ tune         1.1.2\n✔ modeldata    1.2.0     ✔ workflows    1.1.3\n✔ parsnip      1.1.1     ✔ workflowsets 1.0.1\n✔ purrr        1.0.2     ✔ yardstick    1.2.0\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Use tidymodels_prefer() to resolve common conflicts.\n\nset.seed(123)\ntaxi_split &lt;- initial_split(taxi, prop = 0.8, strata = tip)\ntaxi_train &lt;- training(taxi_split)\ntaxi_test &lt;- testing(taxi_split)\n\n\n模型的定义和训练：定义了一个决策树模型规格（使用decision_tree()函数），并设置了成本复杂度参数为0.0001，模式为”分类”。然后，使用workflow()函数创建了一个工作流，指定了目标变量和预测器，并用训练集拟合了这个工作流。\n\n\ntree_spec &lt;- decision_tree(cost_complexity = 0.0001, mode = \"classification\")\ntree_fit &lt;- workflow(tip ~ ., tree_spec) %&gt;% fit(taxi_train)\n\n\n模型部署准备：使用vetiver包的vetiver_model()函数创建了一个vetiver模型对象，这是对已经拟合的模型进行封装，为模型的部署做准备。\n\n\n## Deploying a model\nlibrary(vetiver)\n\n\n载入程辑包：'vetiver'\n\n\nThe following object is masked from 'package:tune':\n\n    load_pkgs\n\nv &lt;- vetiver_model(tree_fit, \"taxi\")\nv\n\n\n── taxi ─ &lt;bundled_workflow&gt; model for deployment \nA rpart classification modeling workflow using 6 features\n\n\n\n建立API：使用plumber包提供的pr()函数创建了一个新的Plumber API，然后利用vetiver_api(v)将vetiver模型对象转化为一个API端点。这样就可以通过这个API来调用我们的机器学习模型。\n\n\n## Deploy your model\nlibrary(plumber)\npr() %&gt;%\n  vetiver_api(v)\n\n# Plumber router with 4 endpoints, 4 filters, and 1 sub-router.\n# Use `pr_run()` on this object to start the API.\n├──[queryString]\n├──[body]\n├──[cookieParser]\n├──[sharedSecret]\n├──/logo\n│  │ # Plumber static router serving from directory: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library/vetiver\n├──/metadata (GET)\n├──/ping (GET)\n├──/predict (POST)\n└──/prototype (GET)\n\n\n以上就是代码的基本解释。如果你想要实际部署这个模型，你需要将这个Plumber API部署到一个可以提供HTTP服务的服务器上，例如 RStudio Connect。"
  },
  {
    "objectID": "13.tidymodels-extra.html#编码的影响",
    "href": "13.tidymodels-extra.html#编码的影响",
    "title": "3  Extra tidymodels",
    "section": "3.2 编码的影响",
    "text": "3.2 编码的影响\n这段代码执行了以下操作：\n数据预处理: 你加载了一个叫做hotel_rates的数据集，并从中随机选择了5000行。然后，你对这个数据集进行了一些预处理，包括去除arrival_date列，以及将company、country和agent列转换为因子类型。\n\ndata(hotel_rates)\nset.seed(295)\nhotel_rates &lt;- \n  hotel_rates %&gt;% \n  sample_n(5000) %&gt;% \n  arrange(arrival_date) %&gt;% \n  select(-arrival_date) %&gt;% \n  mutate(\n    company = factor(as.character(company)),\n    country = factor(as.character(country)),\n    agent = factor(as.character(agent))\n  )\n\n数据划分: 你使用tidymodels包的initial_split()函数将数据集划分为训练集和测试集，其中划分是基于avg_price_per_room列进行的。\n\nset.seed(4028)\nhotel_split &lt;-\n  initial_split(hotel_rates, strata = avg_price_per_room)\n\nhotel_train &lt;- training(hotel_split)\nhotel_test &lt;- testing(hotel_split)\n\n交叉验证: 你设置了一个10折交叉验证（默认设置）的数据集，也是基于avg_price_per_room列进行的。\n\nset.seed(472)\nhotel_rs &lt;- vfold_cv(hotel_train, strata = avg_price_per_room)\n\n特征工程: 你计算了每个代理的平均房价(ADR)和预订数量。然后，你使用了embed包的step_lencode_mixed()函数来对agent列进行混合编码。这个函数根据目标变量的值来计算每个级别的概率，并用这个概率来替换原始的分类变量。\n\nagent_stats &lt;- \n  hotel_train %&gt;%\n  group_by(agent) %&gt;%\n  summarize(\n    ADR = mean(avg_price_per_room), \n    num_reservations = n(),\n    .groups = \"drop\"\n    ) %&gt;%\n  mutate(agent = reorder(agent, ADR))\n\nlibrary(embed)\n\nestimates &lt;- \n  recipe(avg_price_per_room ~ ., data = hotel_train) %&gt;% \n  step_lencode_mixed(agent, outcome = vars(avg_price_per_room), id = \"encoding\") %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors()) %&gt;% \n  prep() %&gt;% \n  tidy(id = \"encoding\") %&gt;% \n  select(agent = level, estimate = value)\n\n为了得到 estimates 的值，依次执行了以下操作：\n\n创建预处理配方：使用recipe()函数创建一个预处理配方，指定要根据所有其他变量来预测的目标avg_price_per_room。\n混合编码：使用step_lencode_mixed()函数对agent进行混合编码。这个函数会计算每个级别的效应大小，并用这个效应大小替换原始分类变量的值。这是一种处理分类变量的方法，可以将分类变量的每个级别与目标变量的某种统计量（如均值）关联起来。\n哑变量编码：使用step_dummy()函数对所有的名义预测变量进行哑变量编码。这会为每个分类变量的每个级别创建一个新的二元变量。\n移除零方差预测变量：使用step_zv()函数移除所有的零方差预测变量。这些变量在所有观测中的值都是相同的，因此不包含任何有用的信息。\n归一化：使用step_normalize()函数对所有的数值预测变量进行归一化。这会将每个变量的值转换为其Z分数，即减去均值然后除以标准差。\n准备配方：使用prep()函数准备（即训练）这个配方。这会使配方学习到训练数据的特性，例如各变量的均值和标准差等。\n提取编码估计值：使用tidy()函数和select()函数提取混合编码的估计值，并将结果保存在estimates中。这个数据框包含两列：agent（原始的级别）和estimate（对应的效应大小）。\n模型训练与评估: 你定义了一个线性回归模型，并在上面应用了你的预处理步骤。然后，你在交叉验证的数据集上拟合了这个模型，并收集了每个折叠的度量结果。\n\n\nhotel_effect_rec &lt;-\n  recipe(avg_price_per_room ~ ., data = hotel_train) %&gt;% \n  step_YeoJohnson(lead_time) %&gt;%\n  step_lencode_mixed(agent, company, outcome = vars(avg_price_per_room)) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_zv(all_predictors())\n\nhotel_effect_wflow &lt;-\n  workflow() %&gt;%\n  add_model(linear_reg()) %&gt;% \n  update_recipe(hotel_effect_rec)\n\nWarning: The workflow has no recipe preprocessor to remove.\n\nreg_metrics &lt;- metric_set(mae, rsq)\n\nhotel_effect_res &lt;-\n  hotel_effect_wflow %&gt;%\n  fit_resamples(hotel_rs, metrics = reg_metrics)\n\n→ A | warning: prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\n\n\nThere were issues with some computations   A: x1\nThere were issues with some computations   A: x1\n\n\n\n\ncollect_metrics(hotel_effect_res)\n\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard   17.8      10 0.189   Preprocessor1_Model1\n2 rsq     standard    0.870    10 0.00357 Preprocessor1_Model1\n\n\n可视化: 在这个过程中，你创建了几个直方图来可视化agent_stats数据，以及一个散点图来比较ADR的样本均值和通过效果编码估计的值。\n\nbefore &lt;- hotel_train %&gt;% \n    select(avg_price_per_room, agent) %&gt;% \n    slice(1:7) %&gt;% \n    add_rowindex()\nbefore\n\n# A tibble: 7 × 3\n  avg_price_per_room agent            .row\n               &lt;dbl&gt; &lt;fct&gt;           &lt;int&gt;\n1               52.7 cynthia_worsley     1\n2               51.8 carlos_bryant       2\n3               53.8 lance_hitchcock     3\n4               51.8 lance_hitchcock     4\n5               46.8 cynthia_worsley     5\n6               54.7 charles_najera      6\n7               46.8 cynthia_worsley     7\n\nafter &lt;- left_join(before, estimates, by = \"agent\") %&gt;% \n  select(avg_price_per_room, agent = estimate, .row)\nafter\n\n# A tibble: 7 × 3\n  avg_price_per_room agent  .row\n               &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1               52.7  88.5     1\n2               51.8  89.5     2\n3               53.8  79.8     3\n4               51.8  79.8     4\n5               46.8  88.5     5\n6               54.7 109.      6\n7               46.8  88.5     7\n\n\n\nagent_stats %&gt;%   \n  ggplot(aes(x = num_reservations)) +\n  geom_histogram(bins = 30, col = \"blue\", fill = \"blue\", alpha = 1/3) +\n  labs(x = \"Number of reservations per agent\")\n\n\n\nagent_stats %&gt;%   \n  ggplot(aes(x = ADR)) +\n  geom_histogram(bins = 30, col = \"red\", fill = \"red\", alpha = 1/3) +\n  labs(x = \"Average ADR per agent\")\n\n\n\nagent_stats %&gt;%   \n  ggplot(aes(x = num_reservations)) +\n  geom_histogram(bins = 30, col = \"blue\", fill = \"blue\", alpha = 1/3) +\n  labs(x = \"Number of reservations per agent\")\n\n\n\nagent_stats %&gt;%   \n  ggplot(aes(x = ADR)) +\n  geom_histogram(bins = 30, col = \"red\", fill = \"red\", alpha = 1/3) +\n  labs(x = \"Average ADR per agent\")\n\n\n\ninner_join(agent_stats, estimates, by = \"agent\") %&gt;% \n  ggplot(aes(x = ADR, y = estimate)) + \n  geom_abline(col = \"green\", lty = 2) +\n  geom_point(aes(size = num_reservations), alpha = 1/3) +\n  coord_obs_pred() +\n  scale_size(range = c(1/3, 5)) +\n  labs(x = \"ADR Sample Mean\", y = \"Estimated via Effects Encoding\")\n\n\n\n\n这段代码是一个完整的机器学习工作流程的实例，包括数据预处理、特征工程、模型训练和评估等步骤。它展示了如何使用R和一些关键的数据科学包（如tidymodels和embed）来进行复杂的数据分析和建模任务。"
  },
  {
    "objectID": "14.tidymodels-list.html#常见机器学习模型",
    "href": "14.tidymodels-list.html#常见机器学习模型",
    "title": "4  常见模型和软件包",
    "section": "4.1 常见机器学习模型",
    "text": "4.1 常见机器学习模型\ntidymodels 中包含了很多不同的模型（https://www.tidymodels.org/find/parsnip/）。\n\n4.1.1 执行回归任务的模型\n仅就执行”回归“任务的模型而言，有下面这些。\n\nBoosted ARIMA Regression Models: 这种方法结合了自回归综合移动平均(ARIMA)和增强算法。主要用于时间序列预测。\nADAM Regression Models: ADAM指的是自适应动态自回归移动平均模型，也主要用于时间序列预测。\nARIMA Regression Models: ARIMA模型是一种常见的时间序列预测模型。\nAutomatic machine learning: 自动机器学习(AutoML)是一种自动化选择最佳算法和参数的过程。\nBagged MARS: Bagged MARS结合了多重自适应回归样条(MARS)和装袋(bagging)技术以提高预测精度。\nBagged neural networks: 这种方法结合了神经网络和装袋算法以提升稳健性。\nBagged trees: 袋装决策树是一种集成学习方法，提高模型准确性和防止过拟合。\nBayesian additive regression trees: BART 是一种贝叶斯方法，可用于处理线性和非线性问题。\nBoosted PROPHET Time Series Models: 结合了 Facebook’s Prophet 时间序列预测方法和 boosting 方法。\nBoosted trees: 提升树模型是一种预测模型，使用梯度提升构建强学习器。\nCubist rule-based regression models: Cubist 是一种基于规则的回归模型，能够适应非线性关系。\nDecision trees: 决策树是一种基本的分类和回归方法。\nExponential Smoothing State Space Models: 也被称为 ETS，主要用于时间序列分析。\nGeneralized additive models: GAM 是一种灵活的线性回归方法，能处理非线性的关系。\nK-nearest neighbors: KNN 是一种基本的分类和回归方法。\nLinear regression via keras/tensorflow/glmnet/glm: 这些都是实现线性回归的工具或框架。\nLinear support vector machines (SVMs): SVM 是一种类别预测工具，线性 SVM 主要用于二元分类问题。\nMultilayer perceptron: MLP 是一种前馈神经网络，多用于分类和回归分析。\nMultiple Seasonality Regression Models: 多季节性回归模型用于处理具有多季节性影响的时间序列数据。\nMultivariate adaptive regression splines: MARS 是一种灵活的回归方法，适用于高维数据集。\nNAIVE Forecast Models: NAIVE 模型是一种简单的时间序列预测方法，通常用作基线预测。\nNNETAR Regression Models: NNETAR 是一种神经网络自回归模型，主要用于时间序列预测。\nParametric survival regression: 此方法用于生存分析中，当数据具有时间至事件特性时。\nPoisson regression: 泊松回归适用于因变量为计数数据的情况。\n\n\n\n4.1.2 执行分类任务的模型\n此外，执行“分类”任务的模型有：\n\nAutomatic machine learning: 自动机器学习（AutoML）是一种利用优化算法挑选出最佳算法和参数的过程。这大大降低了人工选择和调整模型的复杂度。\nBagged MARS: 这是一种集成学习方法，它结合了多重自适应回归样条（MARS）和装袋（bagging）技术，以提高预测精度。\nBagged neural networks: 这是一种集成神经网络的方法。通过创建多个神经网络并取其平均值，可以提升模型的稳定性，并减少过拟合的可能性。\nBagged trees: 袋装决策树是一种集成学习策略，可以提高模型的泛化性能，并降低模型的方差。\nBayesian additive regression trees (BART): BART 是一种非参数贝叶斯方法，可以用于处理非线性和交互效应的问题。\nBoosted trees: 提升树是一种强大的机器学习算法，通过串行构建一系列的小型决策树，每一个新的树学习前面树的错误来进行改进。\nC5.0 rule-based classification models: C5.0 是一个决策树模型，以及一个规则集的生成器。它比它的前身C4.5 更快，需要更少的内存，并且有一个能提高预测精度的boosting选项。\nDecision trees: 决策树是一种监督学习方法，适用于分类和回归问题。\nFlexible discriminant analysis (FDA): FDA 是一种分类技术，它扩展了线性判别分析（LDA），允许在预测分类时使用非线性函数。\nGeneralized additive models (GAM): GAM 是一种灵活的线性回归方法，能处理非线性和非参数关系。\nK-nearest neighbors(KNN): KNN 是一种基于实例的学习或局部近似和所有计算在分类阶段完成的非泛化机器学习方法。\nLinear discriminant analysis(LDA): LDA 是一种分类技术，它寻找最大化类间差异并最小化类内差异的特征投影方向。\nLinear support vector machines (SVMs): 线性支持向量机是一种二元分类模型，其决策边界是数据的超平面。\nLogistic regression: 逻辑回归是一种解决二元分类问题的常见统计方法。\nMultilayer perceptron(MLP): MLP 是一种前馈神经网络，它可以处理复杂的数据集，并在多种任务上表现良好。\nMultinomial regression: 多项式回归是一个用于多类分类的广义线性模型。\nMultivariate adaptive regression splines (MARS): MARS 是一种灵活的非线性回归方法，适用于高维问题。\nNaive Bayes models: 朴素贝叶斯是一种基于应用贝叶斯定理与特征独立假设的简单概率分类器。\nNull model: 空模型是一个不包含任何预测变量的模型，通常用作比较的基准模型。\nPartial least squares(PLS): PLS 是一种处理具有多重共线性数据的回归分析方法。\n多项式支持向量机 (Polynomial SVMs)： 这是一种基于支持向量机 (SVM) 的分类和回归方法。它通过在输入数据中引入高阶项来处理非线性问题。这使得算法可以在高维空间中找到数据的最优分隔超平面。\n二次判别分析 (Quadratic Discriminant Analysis)： 这是一种统计分类技术，它使用贝叶斯理论和二次判别分析来估计给定观察值属于哪个类别的概率。它不假设各类别的协方差矩阵相同，所以能够处理更复杂的情况。\n径向基函数支持向量机 (Radial Basis Function SVMs)： 它是一种使用径向基函数 (RBF) 作为核函数的支持向量机。RBF可以映射输入到无限维的特征空间，使得非线性问题变得线性可解。\n随机森林 (Random forests)： 这是一种基于决策树的集成学习算法。它通过生成和结合许多个决策树，减少过拟合的风险，并提高预测性能。\n正则化判别分析 (Regularized Discriminant Analysis)： 它是一种改进的判别分析方法，通过引入正则化参数来防止过拟合。这使得模型既能保持高度的复杂性，又能防止对训练数据的过度拟合。\nRuleFit 模型: 这是一个对数据生成可解释规则的机器学习模型。它结合了决策树和线性模型的优点，可以生成简单、直观且好理解的规则。"
  },
  {
    "objectID": "14.tidymodels-list.html#用于机器学习任务的-r-软件包",
    "href": "14.tidymodels-list.html#用于机器学习任务的-r-软件包",
    "title": "4  常见模型和软件包",
    "section": "4.2 用于机器学习任务的 R 软件包",
    "text": "4.2 用于机器学习任务的 R 软件包\n这些模型通过调用背后的软件包或算法实现功能。在 CRAN 的 MachineLearning Views，就列举了很多这样的软件包。\n\n4.2.1 R 软件包的列表\n这些软件包氛围核心包、常用包和已被归档的包（停止维护的软件包）。\nCore:\nabess, e1071, gbm, kernlab, mboost, nnet, randomForest, rpart.\nRegular:\nadabag, ahaz, ALEPlot, arules, BART, bartMachine, BayesTree, BDgraph, Boruta, bst, C50, caret, CORElearn, Cubist, DALEX, deepnet, dipm, DoubleML, earth, effects, elasticnet, evclass, evreg, evtree, fastshap, frbs, gamboostLSS, glmertree, glmnet, glmpath, GMMBoost, grf, grplasso, grpreg, h2o, hda, hdi, hdm, iBreakDown, ICEbox, iml, ipred, islasso, joinet, kernelshap, klaR, lars, LiblineaR, lightgbm, lime, maptree, mlpack, mlr3, model4you, mpath, naivebayes, ncvreg, nestedcv, OneR, opusminer, pamr, party, partykit, pdp, penalized, picasso, plotmo, pre, quantregForest, quint, randomForestSRC, ranger, Rborist, rgenoud, RGF, RLT, Rmalschains, rminer, ROCR, RoughSets, RPMM, RSNNS, RWeka, RXshrink, sda, semtree, shapper, shapr, shapviz, SIS, splitTools, ssgraph, stabs, SuperLearner, svmpath, tensorflow, tgp, tidymodels, torch, tree, trtf, varSelRF, wsrf, xgboost.\nArchived:\npenalizedLDA, RcppDL.\n\n\n\n\n\n\nNote\n\n\n\n下面是对这些软件包功能的分类介绍。为了确保一致性，以中英文对照的形式展示。\n\n\n\n\n4.2.2 神经网络和深度学习的软件包\n神经网络和深度学习：单隐层神经网络在 nnet 包中实现（随基础 R 一起提供）。RSNNS 包提供了一个接口，可以连接到斯图加特神经网络模拟器（SNNS）。实现深度学习风格的神经网络的包包括 deepnet（前馈神经网络，受限玻尔兹曼机，深度信念网络，堆叠自编码器），RcppDL（已归档）（去噪自编码器，堆叠去噪自编码器，受限玻尔兹曼机，深度信念网络）和 h2o（前馈神经网络，深度自编码器）。tensorflow 包提供了一个接口，可以连接到 tensorflow。torch 包实现了一个接口，可以连接到 libtorch 库。evreg 包中实现的 ENNreg 证据回归神经网络模型可以量化预测不确定性。\n\nNeural Networks and Deep Learning : Single-hidden-layer neural network are implemented in package nnet (shipped with base R). Package RSNNS offers an interface to the Stuttgart Neural Network Simulator (SNNS). Packages implementing deep learning flavours of neural networks include deepnet (feed-forward neural network, restricted Boltzmann machine, deep belief network, stacked autoencoders), RcppDL (archived) (denoising autoencoder, stacked denoising autoencoder, restricted Boltzmann machine, deep belief network) and h2o (feed-forward neural network, deep autoencoders). An interface to tensorflow is available in tensorflow. The torchpackage implements an interface to the libtorch library. Prediction uncertainty can be quantified by the ENNreg evidential regression neural network model implemented in evreg.\n\n\n\n4.2.3 递归划分的软件包\n\n递归划分：根据CART书中的思想，实现了用于回归、分类和生存分析的树结构模型，这些模型在 rpart（与基础R一起提供）和 tree 中实现。建议使用rpart 包来计算类似CART的树。Weka 提供了丰富的划分算法工具箱，RWeka包为此实现提供了接口，包括C4.5的J4.8变体和M5。Cubist包适合带有线性回归模型的规则基础模型（类似于树），终端叶子、基于实例的修正和boosting。C50包可以拟合 C5.0 分类树、基于规则的模型、以及这些模型的boosted版本。pre 可以拟合更广泛响应变量类型的基于规则的模型。\n在party 和 partykit包中实现了两种具有无偏变量选择和统计停止准则的递归划分算法。函数 ctree() 基于非参数条件推理过程用于测试响应与每个输入变量之间的独立性，而 mob() 可用于对参数模型进行划分。在party 和 partykit包中也提供了可扩展的工具用于可视化二叉树和响应的节点分布。混合效应模型（GLMMs）的划分可以使用 glmertree 包执行；结构方程模型（SEMs）的划分可以使用 semtree 包执行。 用于可视化树的图形工具在 maptree 包中可用。\n混合模型的划分由 RPMM 执行。\n在 partykit 中实现了代表树和统一预测和可视化方法的计算基础设施。这种基础设施被 evtree 包用于实现全局最优树的进化学习。各种包中都有生存树。\n关于异质处理效应的子组识别的树，在partykit，model4you，dipm，quint，pkg(\"SIDES\")，pkg(\"psica\") 和 pkg(\"MrSGUIDE\")包（可能还有更多）中可用。\nRecursive Partitioning : Tree-structured models for regression, classification and survival analysis, following the ideas in the CART book, are implemented in rpart (shipped with base R) and tree. Package rpart is recommended for computing CART-like trees. A rich toolbox of partitioning algorithms is available in Weka, package RWeka provides an interface to this implementation, including the J4.8-variant of C4.5 and M5. The Cubist package fits rule-based models (similar to trees) with linear regression models in the terminal leaves, instance-based corrections and boosting. The C50 package can fit C5.0 classification trees, rule-based models, and boosted versions of these. pre can fit rule-based models for a wider range of response variable types.\nTwo recursive partitioning algorithms with unbiased variable selection and statistical stopping criterion are implemented in package party and partykit. Function ctree() is based on non-parametric conditional inference procedures for testing independence between response and each input variable whereas mob() can be used to partition parametric models. Extensible tools for visualizing binary trees and node distributions of the response are available in package party and partykit as well. Partitioning of mixed-effects models (GLMMs) can be performed with package glmertree; partitioning of structural equation models (SEMs) can be performed with package semtree. Graphical tools for the visualization of trees are available in package maptree.\nPartitioning of mixture models is performed by RPMM.\nComputational infrastructure for representing trees and unified methods for prediction and visualization is implemented in partykit. This infrastructure is used by package evtree to implement evolutionary learning of globally optimal trees. Survival trees are available in various packages.\nTrees for subgroup identification with respect to heterogenuous treatment effects are available in packages partykit, model4you, dipm, quint, pkg(\"SIDES\"), pkg(\"psica\"), and pkg(\"MrSGUIDE\") (and probably many more).\n\n\n\n4.2.4 随机森林的软件包\n\n随机森林：随机森林算法的参考实现方案，包括回归和分类，在randomForest包中都有提供。ipred包提供用于回归、分类和生存分析的bagging方式，以及通过集成学习结合多个模型的bundling方式。此外，基于条件推断树适应任意尺度测量的响应变量的随机森林变体在party包中有实现。randomForestSRC实现了Breiman的随机森林对生存、回归和分类问题的统一处理。Quantile回归森林quantregForest允许通过随机森林方法对探索性变量进行数值响应的分位数回归。针对二元数据，varSelRF和Boruta包关注通过随机森林算法进行的变量选择。此外，ranger和Rborist包提供了R接口快速实现C++随机森林。在RLT包中实现了强化学习树，特点是在将来可能重要的变量中进行划分。wsrf实现了一个替代性的变量权重法，用于替代传统的随机变量抽样进行变量子空间选择。RGF包是一个接口，连接到Python实现的一种叫做正则化贪婪森林的程序。针对参数模型的随机森林，包括用于预测分布估计的森林，可以在trtf（可能在审查和截断下的预测转换森林）和grf（泛化随机森林的实现）包中找到。\nRandom Forests : The reference implementation of the random forest algorithm for regression and classification is available in package randomForest. Package ipred has bagging for regression, classification and survival analysis as well as bundling, a combination of multiple models via ensemble learning. In addition, a random forest variant for response variables measured at arbitrary scales based on conditional inference trees is implemented in package party. randomForestSRC implements a unified treatment of Breiman’s random forests for survival, regression and classification problems. Quantile regression forests quantregForest allow to regress quantiles of a numeric response on exploratory variables via a random forest approach. For binary data, The varSelRF and Boruta packages focus on variable selection by means for random forest algorithms. In addition, packages ranger and Rborist offer R interfaces to fast C++ implementations of random forests. Reinforcement Learning Trees, featuring splits in variables which will be important down the tree, are implemented in package RLT. wsrf implements an alternative variable weighting method for variable subspace selection in place of the traditional random variable sampling. Package RGF is an interface to a Python implementation of a procedure called regularized greedy forests. Random forests for parametric models, including forests for the estimation of predictive distributions, are available in packages trtf (predictive transformation forests, possibly under censoring and trunction) and grf (an implementation of generalised random forests).\n\n\n\n4.2.5 正则化和收缩方法的软件包\n\n正规化和收缩方法：可以使用lars包来拟合带有某些参数估计约束的回归模型。在grplasso包中提供了同时更新参数组（即组lasso）的Lasso；grpreg包实现了诸如组MCP和组SCAD等其他一些组惩罚模型。可以从glmpath包中获得广义线性模型和Cox模型的L1正则化路径，整个lasso或弹性网正则化路径（也在elasticnet中）用于线性回归、逻辑回归和多项式回归模型，可以从glmnet包中获得。penalized包提供了lasso（L1）和ridge（L2）惩罚回归模型（包括GLM和Cox模型）的另一种实现。可以使用RXshrink包生成TRACE显示，当错误为IID Normal时，标识出最大似然或最小MSE风险的收缩程度。ahaz包提供了在lasso惩罚下的半参数加性风险模型。Fisher的LDA投影通过可选的LASSO惩罚产生稀疏解，在penalizedLDA (archived)包中实现。pamr包实现了收缩质心分类器和基因表达分析的实用工具。earth包中提供了多变量自适应回归样条的实现。各种形式的惩罚判别分析在hda和sda包中实现。LiblineaR包提供了对LIBLINEAR库的接口。ncvreg包使用坐标下降算法拟合SCAD和MCP回归惩罚下的线性和逻辑回归模型。同样的惩罚也在picasso包中实现。在非高斯和异方差错误下的Lasso由hdm估计，包含在高维设定中对Lasso回归和估计处理效果的低维组件的推断。SIS包实现了广义线性和Cox模型的独立筛选。相关结果的弹性网在joinet包中提供。通过复合优化共轭算子来拟合使用mpath包的稳健惩罚广义线性模型和稳健支持向量机。islasso包提供了基于诱导平滑理念的lasso实现，这可以为所有模型参数获得可靠的p值。最佳子集选择，用于线性、逻辑、Cox等其他回归模型，基于一个快速的多项式时间算法，可从abess包获取。\nRegularized and Shrinkage Methods : Regression models with some constraint on the parameter estimates can be fitted with the lars package. Lasso with simultaneous updates for groups of parameters (groupwise lasso) is available in package grplasso; the grpreg package implements a number of other group penalization models, such as group MCP and group SCAD. The L1 regularization path for generalized linear models and Cox models can be obtained from functions available in package glmpath, the entire lasso or elastic-net regularization path (also in elasticnet) for linear regression, logistic and multinomial regression models can be obtained from package glmnet. The penalized package provides an alternative implementation of lasso (L1) and ridge (L2) penalized regression models (both GLM and Cox models). Package RXshrink can be used to generate TRACE displays that identify the extent of shrinkage with Maximum Likelihood of Minimum MSE Risk when errors are IID Normal. Semiparametric additive hazards models under lasso penalties are offered by package ahaz. Fisher’s LDA projection with an optional LASSO penalty to produce sparse solutions is implemented in package penalizedLDA (archived). The shrunken centroids classifier and utilities for gene expression analyses are implemented in package pamr. An implementation of multivariate adaptive regression splines is available in package earth. Various forms of penalized discriminant analysis are implemented in packages hda and sda. Package LiblineaR offers an interface to the LIBLINEAR library. The ncvreg package fits linear and logistic regression models under the the SCAD and MCP regression penalties using a coordinate descent algorithm. The same penalties are also implemented in the picasso package. The Lasso under non-Gaussian and heteroscedastic errors is estimated by hdm, inference on low-dimensional components of Lasso regression and of estimated treatment effects in a high-dimensional setting are also contained. Package SIS implements sure independence screening in generalised linear and Cox models. Elastic nets for correlated outcomes are available from package joinet. Robust penalized generalized linear models and robust support vector machines are fitted by package mpath using composite optimization by conjugation operator. The islasso package provides an implementation of lasso based on the induced smoothing idea which allows to obtain reliable p-values for all model parameters. Best-subset selection for linear, logistic, Cox and other regression models, based on a fast polynomial time algorithm, is available from package abess.\n\n\n\n4.2.6 增强和梯度下降的软件包\n\n增强和梯度下降：各种形式的梯度提升在gbm包中实现（基于树的函数梯度下降提升）。lightgbm和xgboost包实现了基于高效树作为基础学习器的树形提升，适用于多个和用户自定义的目标函数。bst包中的提升实现优化了铰链损失。一个可扩展的提升框架，适用于广义线性模型、加法模型和非参数模型，可以在mboost包中找到。混合模型的似然基提升在GMMBoost中实现。GAMLSS模型可以通过gamboostLSS使用提升来拟合。adabag实现了经典的AdaBoost算法，并添加了功能，如变量重要性。\nBoosting and Gradient Descent : Various forms of gradient boosting are implemented in package gbm (tree-based functional gradient descent boosting). Package lightgbm and xgboost implement tree-based boosting using efficient trees as base learners for several and also user-defined objective functions. The Hinge-loss is optimized by the boosting implementation in package bst. An extensible boosting framework for generalized linear, additive and nonparametric models is available in package mboost. Likelihood-based boosting for mixed models is implemented in GMMBoost. GAMLSS models can be fitted using boosting by gamboostLSS. adabag implements the classical AdaBoost algorithm with added functionality, such as variable importances.\n\n\n\n4.2.7 支持向量机和核方法的软件包\n\n支持向量机和核方法：e1071中的svm()函数提供了一个接口到LIBSVM库，kernlab包实现了一个灵活的核学习框架（包括SVMs、RVMs和其他核学习算法）。klaR包提供了一个接口到SVMlight实现（仅用于一对所有的分类）。\nSupport Vector Machines and Kernel Methods : The function svm() from e1071 offers an interface to the LIBSVM library and package kernlab implements a flexible framework for kernel learning (including SVMs, RVMs and other kernel learning algorithms). An interface to the SVMlight implementation (only for one-against-all classification) is provided in package klaR.\n\n\n\n4.2.8 贝叶斯方法的软件包\n\n贝叶斯方法：贝叶斯加法回归树（BART）在BayesTree，BART 和 bartMachine 包中有实现，其最终模型是以许多弱学习器的总和定义的（与集成方法类似）。包tgp提供了贝叶斯非平稳，半参数非线性回归和设计通过高斯过程包含贝叶斯CART和线性模型。包BDgraph实现了针对多变量连续、离散和混合数据的无向图形模型中的贝叶斯结构学习；相应的依赖于尖峰和板状先验的方法可以从ssgraph包中获得。朴素贝叶斯分类器在naivebayes包中提供。\nBayesian Methods : Bayesian Additive Regression Trees (BART), where the final model is defined in terms of the sum over many weak learners (not unlike ensemble methods), are implemented in packages BayesTree, BART, and bartMachine. Bayesian nonstationary, semiparametric nonlinear regression and design by treed Gaussian processes including Bayesian CART and treed linear models are made available by package tgp. Bayesian structure learning in undirected graphical models for multivariate continuous, discrete, and mixed data is implemented in package BDgraph; corresponding methods relying on spike-and-slab priors are available from package ssgraph. Naive Bayes classifiers are available in naivebayes.\n\n\n\n4.2.9 提供遗传算法的软件包\n\n使用遗传算法优化：rgenoud包提供了基于遗传算法的优化程序。Rmalschains包实现了带有本地搜索链的遗传算法，这是一种特殊类型的进化算法，结合了稳定状态的遗传算法和本地搜索用于实值参数优化。\nOptimization using Genetic Algorithms : Package rgenoud offers optimization routines based on genetic algorithms. The package Rmalschains implements memetic algorithms with local search chains, which are a special type of evolutionary algorithms, combining a steady state genetic algorithm with local search for real-valued parameter optimization.\n\n\n\n4.2.10 关联规则分析的软件包\n\n关联规则：arules包为高效处理稀疏二进制数据提供了数据结构，同时为Apriori和Eclat的实现提供了接口，用于挖掘频繁项集、最大频繁项集、封闭频繁项集和关联规则。opusminer包提供了一个接口到OPUS Miner算法（在C++中实现），用于有效地找到交易数据中的关键关联，以自给自足的项集的形式，使用杠杆或提升。\nAssociation Rules : Package arules provides both data structures for efficient handling of sparse binary data as well as interfaces to implementations of Apriori and Eclat for mining frequent itemsets, maximal frequent itemsets, closed frequent itemsets and association rules. Package opusminer provides an interface to the OPUS Miner algorithm (implemented in C++) for finding the key associations in transaction data efficiently, in the form of self-sufficient itemsets, using either leverage or lift.\n\n\n\n4.2.11 模糊规则系统的软件包\n\n模糊规则系统：frbs包实现了从数据中学习模糊规则系统的标准方法，用于回归和分类。RoughSets包在一个包中提供了粗糙集论（RST）和模糊粗糙集论（FRST）的全面实现。\nFuzzy Rule-based Systems : Package frbs implements a host of standard methods for learning fuzzy rule-based systems from data for regression and classification. Package RoughSets provides comprehensive implementations of the rough set theory (RST) and the fuzzy rough set theory (FRST) in a single package.\n\n\n\n4.2.12 模型选择和验证的软件包\n\n模型选择和验证：e1071包有用于超参数调优的tune()函数，可以使用函数errorest()（来自ipred）进行错误率估计。可以利用svmpath包的功能选择支持向量机的成本参数C。splitTools包提供了交叉验证和其他重采样方案的数据分割。nestedcv包为glmnet和caret模型提供了嵌套交叉验证。用于ROC分析和比较候选分类器的其他可视化技术的函数可以从ROCR包获取。hdi 和 stabs包实现了一系列模型的稳定性选择，hdi还提供了高维模型中的其他推理程序。\nModel selection and validation : Package e1071 has function tune() for hyper parameter tuning and function errorest() (ipred) can be used for error rate estimation. The cost parameter C for support vector machines can be chosen utilizing the functionality of package svmpath. Data splitting for crossvalidation and other resampling schemes is available in the splitTools package. Package nestedcv provides nested cross-validation for glmnet and caret models. Functions for ROC analysis and other visualisation techniques for comparing candidate classifiers are available from package ROCR. Packages hdi and stabs implement stability selection for a range of models, hdialso offers other inference procedures in high-dimensional models.\n\n\n\n4.2.13 因果分析的软件包\n\n因果机器学习：DoubleML包是双机器学习框架在各种因果模型中的面向对象实现。基于mlr3生态系统，可以基于广泛的机器学习方法估计因果效应。\nCausal Machine Learning : The package DoubleML is an object-oriented implementation of the double machine learning framework in a variety of causal models. Building upon the mlr3 ecosystem, estimation of causal effects can be based on an extensive collection of machine learning methods.\n\n\n\n4.2.14 其它程序\n\n其他程序：证据分类器在evclass包中使用Dempster-Shafer质量函数量化关于测试模式类别的不确定性。OneR（One Rule）包提供了一个分类算法，增强了对缺失值和数字数据的精细处理，以及广泛的诊断功能。\nOther procedures : Evidential classifiers quantify the uncertainty about the class of a test pattern using a Dempster-Shafer mass function in package evclass. The OneR (One Rule) package offers a classification algorithm with enhancements for sophisticated handling of missing values and numeric data together with extensive diagnostic functions.\n\n\n\n4.2.15 用于机器学习的宏包\n\n宏包：tidymodels包提供了构建预测模型的杂项函数，包括参数调优和变量重要度度量。同样，mlr3包为各种统计和机器学习包提供高级接口。SuperLearner实现了类似的工具箱。h2o包实现了一个通用的机器学习平台，其中包含了许多流行算法的可扩展实现，如随机森林、GBM、GLM（带弹性网正则化）和深度学习（前馈多层网络）等等。可以从mlpack包获取到mlpack C++库的接口。 CORElearn实现了一类相当广泛的机器学习算法，如最近邻，树，随机森林，以及几种特征选择方法。类似地，rminer包接口了在其他包中实现的几种学习算法，并计算了几种性能度量。\nMeta packages : Package tidymodels provides miscellaneous functions for building predictive models, including parameter tuning and variable importance measures. In a similar spirit, package mlr3 offers high-level interfaces to various statistical and machine learning packages. Package SuperLearner implements a similar toolbox. The h2o package implements a general purpose machine learning platform that has scalable implementations of many popular algorithms such as random forest, GBM, GLM (with elastic net regularization), and deep learning (feedforward multilayer networks), among others. An interface to the mlpack C++ library is available from package mlpack. CORElearn implements a rather broad class of machine learning algorithms, such as nearest neighbors, trees, random forests, and several feature selection methods. Similar, package rminer interfaces several learning algorithms implemented in other packages and computes several performance measures.\n\n\n\n4.2.16 可视化的软件包\n\n可视化（最初由Brandon Greenwell贡献）：stats::termplot()函数包可以用来绘制预测方法支持 type=\"terms\"的模型中的项。effects包为带有线性预测器的模型（如，线性和广义线性模型）提供了图形和表格效果显示。弗里德曼的部分依赖图（PDPs），这是预测函数的低维度图形渲染，已经在一些包中实现。gbm，randomForest 和 randomForestSRC 提供了他们自己的函数来显示PDPs，但仅限于使用那些包拟合的模型（来自randomForest 的 partialPlot 函数更受限，因为它一次只允许一个预测器）。pdp，plotmo，和 ICEbox 包更为通用，并允许创建各种机器学习模型的PDPs（例如，随机森林、支持向量机等）；pdp 和 plotmo 都支持多变量显示（plotmo 限于两个预测器，而 pdp 使用trellis图形来显示涉及三个预测器的PDPs）。默认情况下，plotmo 将背景变量固定在他们的中位数（或因素的第一级），这比构造PDPs更快，但包含的信息较少。 ICEbox关注于构建个体条件期望（ICE）曲线，这是对Friedman’s PDPs的改进。 ICE curves以及居中的 ICE curves也可以用pdp 包的 partial() 函数构建。\nVisualisation (initially contributed by Brandon Greenwell) The stats::termplot() function package can be used to plot the terms in a model whose predict method supports type=\"terms\". The effects package provides graphical and tabular effect displays for models with a linear predictor (e.g., linear and generalized linear models). Friedman’s partial dependence plots (PDPs), that are low dimensional graphical renderings of the prediction function, are implemented in a few packages. gbm, randomForest and randomForestSRC provide their own functions for displaying PDPs, but are limited to the models fit with those packages (the function partialPlot from randomForest is more limited since it only allows for one predictor at a time). Packages pdp, plotmo, and ICEbox are more general and allow for the creation of PDPs for a wide variety of machine learning models (e.g., random forests, support vector machines, etc.); both pdp and plotmo support multivariate displays (plotmo is limited to two predictors while pdp uses trellis graphics to display PDPs involving three predictors). By default, plotmo fixes the background variables at their medians (or first level for factors) which is faster than constructing PDPs but incorporates less information. ICEbox focuses on constructing individual conditional expectation (ICE) curves, a refinement over Friedman’s PDPs. ICE curves, as well as centered ICE curves can also be constructed with the partial() function from the pdp package.\n\n\n\n4.2.17 可解释人工智能的软件包\n\nXAI： 可解释人工智能（XAI）领域的大多数软件包和函数都属于最后一节“可视化”。元包 DALEX 和 iml 提供了解释任何模型的不同方法，包括部分依赖性、累积局部效应和排列重要性。 累积局部效应图也可以直接在 ALEPlot中获取。 SHAP（来自 SHapley Additive exPlanations）是解释ML模型最常用的技术之一。 它以公平的方式将预测分解为预测器的加性贡献。 对于基于树的模型，存在非常快的TreeSHAP算法。 它直接与 h2o，xgboost，和 lightgbm 一起提供。 在额外的包中提供了SHAP的模型不可知实现：fastshap 主要使用蒙特卡洛采样来近似SHAP值，而 shapr 和 kernelshap 提供了KernelSHAP的实现。 这些包的SHAP值可以由shapviz包绘制。Python的“shap”包的端口在 shapper中提供。 预测的替代分解在 lime 和 iBreakDown 中实现。\nXAI : Most packages and functions from the last section “Visualization” belong to the field of explainable artificial intelligence (XAI). The meta packages DALEX and iml offer different methods to interpret any model, including partial dependence, accumulated local effects, and permutation importance. Accumulated local effects plots are also directly available in ALEPlot. SHAP (from SHapley Additive exPlanations) is one of the most frequently used techniques to interpret ML models. It decomposes - in a fair way - predictions into additive contributions of the predictors. For tree-based models, the very fast TreeSHAP algorithm exists. It is shipped directly with h2o, xgboost, and lightgbm. Model-agnostic implementations of SHAP are available in additional packages: fastshap mainly uses Monte-Carlo sampling to approximate SHAP values, while shapr and kernelshap provide implementations of KernelSHAP. SHAP values of any of these packages can be plotted by the package shapviz. A port to Python’s “shap” package is provided in shapper. Alternative decompositions of predictions are implemented in lime and iBreakDown."
  },
  {
    "objectID": "14.tidymodels-list.html#一些新的机器学习领域概念",
    "href": "14.tidymodels-list.html#一些新的机器学习领域概念",
    "title": "4  常见模型和软件包",
    "section": "4.3 一些新的机器学习领域概念",
    "text": "4.3 一些新的机器学习领域概念\n\n递归划分：这是一种构建决策树的方法，它反复将数据集划分为两个或更多的子集，这个过程直到满足某种停止条件（比如，达到预设的最大深度）才会停止。每次划分都是基于特定特征的值。\n正则化：正则化是一种用于防止过拟合的技术。通过在损失函数中添加一个惩罚项，可以减小模型复杂度并增强其泛化能力。\nBoosting：Boosting 是一种集成学习技术，它结合了多个“弱”分类器以创建一个“强”分类器。Boosting 的工作方式是，在每个训练阶段，都对预测错误的样本进行加权，使得后续的分类器更关注那些被误分类的样本。\n梯度下降：梯度下降是一种优化算法，用于寻找目标函数（如损失函数或成本函数）的最小值。它通过计算当前点的负梯度来迭代地更新参数值，并向着函数值下降最快的方向移动。\n支持向量机：支持向量机 (SVM) 是一种分类和回归方法。在二分类问题中，SVM 试图找到一个超平面，能够最大化两个类别之间的边界。\n核方法：核方法是一种在更高维的特征空间中进行计算的技巧，而无需显式地计算数据点在这个空间中的表示。它常用于支持向量机等算法。\n贝叶斯方法：贝叶斯方法基于贝叶斯定理，将先验知识与观察到的数据结合起来，得出对未知参数的后验概率分布。\n遗传算法：遗传算法是一种模拟自然选择过程的搜索优化算法。它通过在每一代中重复选择、交叉和变异操作，从而在解空间中搜索最优解。\n模糊规则：模糊规则系统是一种基于模糊逻辑的系统，用于处理模糊或不确定的信息。它由一组如果-那么的模糊规则组成，并且可以处理连续值输入和输出。\n因果分析：因果分析是一种研究因果关系的方法。在机器学习中，它涉及使用强大的统计工具（如 Granger 因果性测试）来识别因果关系，或使用建立在潜在因果关系模型基础上的算法（比如 Pearl’s do-calculus）来进行预测。\nXAI（可解释的人工智能）：这是一个研究领域，旨在让人工智能（AI）决策过程变得可以理解和解释。尽管许多现代AI模型（如深度学习）在各种任务上表现出色，但它们往往被视为“黑箱”，因为我们很难理解它们是如何做出具体决策的。XAI的目标是揭示模型内部的复杂机制，并提供有关模型决策的透明、可理解且可信赖的解释。这不仅有助于提高用户对AI系统的信任，也有助于开发者更好地理解模型的行为，从而进行调试和改进。实现XAI的方法包括特征重要性分析、模型可视化、对抗性例证生成、局部可解释性模型（如LIME）等。"
  },
  {
    "objectID": "15.tidymodels-recipes.html#配置默认环境",
    "href": "15.tidymodels-recipes.html#配置默认环境",
    "title": "5  特征工程",
    "section": "5.1 配置默认环境",
    "text": "5.1 配置默认环境\n\n# 设置 knitr 选项\nknitr::opts_chunk$set(\n    collapse = TRUE,\n    comment = \"#&gt;\",\n    message = FALSE,\n    warning = FALSE\n)\n\n# 显示英文报错信息\nSys.setenv(LANG = \"en\")\n\n# 使用 rmodels 环境\nreticulate::use_condaenv(\"rmodels\",\n    conda = \"/opt/homebrew/anaconda3/bin/conda\"\n)\n\n# 导入 tidyverse 包\nlibrary(\"tidyverse\")\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# 导入 cailab.utils 包\nif (Sys.getenv(\"USER\") == \"gaoch\") {\n    devtools::load_all(\"~/GitHub/cailab.utils\")\n} else {\n    library(\"cailab.utils\")\n}\n\nℹ Loading cailab.utils\nRegistered S3 methods overwritten by 'treeio':\n  method              from    \n  MRCA.phylo          tidytree\n  MRCA.treedata       tidytree\n  Nnode.treedata      tidytree\n  Ntip.treedata       tidytree\n  ancestor.phylo      tidytree\n  ancestor.treedata   tidytree\n  child.phylo         tidytree\n  child.treedata      tidytree\n  full_join.phylo     tidytree\n  full_join.treedata  tidytree\n  groupClade.phylo    tidytree\n  groupClade.treedata tidytree\n  groupOTU.phylo      tidytree\n  groupOTU.treedata   tidytree\n  inner_join.phylo    tidytree\n  inner_join.treedata tidytree\n  is.rooted.treedata  tidytree\n  nodeid.phylo        tidytree\n  nodeid.treedata     tidytree\n  nodelab.phylo       tidytree\n  nodelab.treedata    tidytree\n  offspring.phylo     tidytree\n  offspring.treedata  tidytree\n  parent.phylo        tidytree\n  parent.treedata     tidytree\n  root.treedata       tidytree\n  rootnode.phylo      tidytree\n  sibling.phylo       tidytree\n\n# 导入 tidymodels 包\nlibrary(tidymodels)\n\nRegistered S3 method overwritten by 'parsnip':\n  method          from \n  print.nullmodel vegan\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n✔ broom        1.0.5     ✔ rsample      1.2.0\n✔ dials        1.2.0     ✔ tune         1.1.2\n✔ infer        1.0.5     ✔ workflows    1.1.3\n✔ modeldata    1.2.0     ✔ workflowsets 1.0.1\n✔ parsnip      1.1.1     ✔ yardstick    1.2.0\n✔ recipes      1.0.9     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Dig deeper into tidy modeling with R at https://www.tmwr.org\n\n# 设置 ggplot 默认主题\ntheme_set(theme_bw())"
  },
  {
    "objectID": "15.tidymodels-recipes.html#什么是特征工程",
    "href": "15.tidymodels-recipes.html#什么是特征工程",
    "title": "5  特征工程",
    "section": "5.2 什么是特征工程",
    "text": "5.2 什么是特征工程\n特征工程是一种数据预处理的方法，它可以帮助机器学习模型更好地理解和预测数据。特征工程的主要步骤有：\n\n特征理解：分析数据的来源、类型、分布、缺失值、异常值等，了解数据的特点和含义。\n特征选择：根据数据的相关性、重要性、冗余性等，选择对模型有用的特征，减少特征的维度和噪声。\n特征提取：利用数学或统计方法，从原始数据中提取出新的特征，例如主成分分析（PCA）、线性判别分析（LDA）、奇异值分解（SVD）等。\n特征构造：利用数据领域的知识，创造出新的特征，例如组合、分解、变换、编码等。\n特征转换：将特征转换为适合模型的格式，例如标准化、归一化、离散化、独热编码等。\n\n特征工程是机器学习中的一门艺术，它需要不断地尝试和优化，才能找到最合适的特征组合。特征工程的好坏，往往决定了模型的性能和效果。\n在 tidymodels 中，使用 recipes 软件包进行特征工程，它可以让您用类似dplyr的管道语法来创建和预处理机器学习的特征。recipes软件包的主要优点有：\n\n它可以处理各种数据类型，包括数值、分类、文本、图像等。\n它可以方便地添加、删除、修改和组合特征工程的步骤，以及调整参数和选项。\n它可以自动估计特征工程的统计参数，并将其应用到新的数据集上，保证数据的一致性。\n它可以与其他tidymodels包，如rsample、parsnip、tune等无缝地集成，构建完整的机器学习流程。\n\n如果您想学习如何使用 recipes 软件包，您可以参考以下的资源：\n\nrecipes官方网站：这里有recipes软件包的详细文档、教程和示例，以及常见问题的解答。\nFeature Engineering and Selection：这是一本在线书籍，介绍了特征工程的理论和应用，包括使用recipes软件包的示例。\nR Recipes: A Problem-Solution Approach：这是一本实用的书籍，提供了使用R语言进行数据分析和机器学习的各种问题和解决方案，其中也涉及了recipes软件包的用法。"
  },
  {
    "objectID": "15.tidymodels-recipes.html#recipes-中的概念",
    "href": "15.tidymodels-recipes.html#recipes-中的概念",
    "title": "5  特征工程",
    "section": "5.3 recipes 中的概念",
    "text": "5.3 recipes 中的概念\n来源：https://recipes.tidymodels.org/articles/recipes.html\n首先说明几个定义如下：\n\n变量（Variables）：原始数据集中的列，例如在传统公式 Y ~ A + B + A:B 中，变量包括 A、B 和 Y。\n角色（Roles）：定义变量在模型中如何使用。例如：predictor（自变量）、response（因变量）和 case weight。这意味着角色的设定是开放且可扩展的。\n项（Terms）：设计矩阵中的列，如 A、B 和 A:B。这些也可以是其他派生实体，例如一组主成分或一组定义变量基函数的列。这些与机器学习中的特征是同义词。被赋予 predictor 角色的变量将自动成为主效应项。\n\n总的来说，在 recipes 包中，你可以通过分配“角色”来指定每个变量的用途，并创建“项”以指定模型中的特征。这种方式提供了一个灵活的框架，使得数据预处理和特征工程变得更加简单和直观。"
  },
  {
    "objectID": "15.tidymodels-recipes.html#最小实例",
    "href": "15.tidymodels-recipes.html#最小实例",
    "title": "5  特征工程",
    "section": "5.4 最小实例",
    "text": "5.4 最小实例\n这段代码首先从 “modeldata” 包中加载了一个名为 “ames” 的数据集，然后对 “Sale_Price” 列（房价）进行了对数转换。接着，它定义了一个预处理流程，包括一些特征工程步骤，这个流程将用于训练模型。\n\n\n\n\n\n\nNote\n\n\n\n关于 ames 数据集\nAmes Housing 数据集来源于美国爱荷华州 Ames 市的住宅销售信息，由 Dean De Cock 教授收集而成，用于教学目的，特别是数据清洗和高级回归技术。\n这个数据集包含了 2006 年到 2010 年间 Ames 市近 3000 所房屋的 79 种特征，如房屋类型、建造年份、房间数量、地下室情况、车库大小、建筑材料等，以及每个房屋的最终销售价格。\n在这个数据集中，每一行代表一处房产，每一列代表一个特性，其中 Sale_Price 列是我们通常要预测的目标变量。这个数据集通常被用来进行回归分析或机器学习任务，例如预测未来的房价。\n因为这个数据集有许多特征，并且涉及到各种不同类型的变量（如类别变量、顺序变量和数值变量），所以它是一个非常好的数据集，可以用来练习和展示数据预处理、特征工程和模型调优等技巧。\n\n\n\ndata(ames, package = \"modeldata\")\n\names &lt;- mutate(ames, Sale_Price = log10(Sale_Price))\n\names_rec &lt;-\n  recipe(Sale_Price ~ ., data = ames[-(1:6), ]) %&gt;%\n  step_other(Neighborhood, threshold = 0.05) %&gt;%\n  step_dummy(all_nominal()) %&gt;%\n  step_interact(~ starts_with(\"Central_Air\"):Year_Built) %&gt;%\n  step_ns(Longitude, Latitude, deg_free = 2) %&gt;%\n  step_zv(all_predictors())\n\names_rec = prep(ames_rec)\n\n# return the training set (already embedded in ames_rec)\nbake(ames_rec, new_data = NULL)\n#&gt; # A tibble: 2,924 × 259\n#&gt;    Lot_Frontage Lot_Area Year_Built Year_Remod_Add Mas_Vnr_Area BsmtFin_SF_1\n#&gt;           &lt;dbl&gt;    &lt;int&gt;      &lt;int&gt;          &lt;int&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n#&gt;  1           41     4920       2001           2001            0            3\n#&gt;  2           43     5005       1992           1992            0            1\n#&gt;  3           39     5389       1995           1996            0            3\n#&gt;  4           60     7500       1999           1999            0            7\n#&gt;  5           75    10000       1993           1994            0            7\n#&gt;  6            0     7980       1992           2007            0            1\n#&gt;  7           63     8402       1998           1998            0            7\n#&gt;  8           85    10176       1990           1990            0            3\n#&gt;  9            0     6820       1985           1985            0            3\n#&gt; 10           47    53504       2003           2003          603            1\n#&gt; # ℹ 2,914 more rows\n#&gt; # ℹ 253 more variables: BsmtFin_SF_2 &lt;dbl&gt;, Bsmt_Unf_SF &lt;dbl&gt;,\n#&gt; #   Total_Bsmt_SF &lt;dbl&gt;, First_Flr_SF &lt;int&gt;, Second_Flr_SF &lt;int&gt;,\n#&gt; #   Gr_Liv_Area &lt;int&gt;, Bsmt_Full_Bath &lt;dbl&gt;, Bsmt_Half_Bath &lt;dbl&gt;,\n#&gt; #   Full_Bath &lt;int&gt;, Half_Bath &lt;int&gt;, Bedroom_AbvGr &lt;int&gt;, Kitchen_AbvGr &lt;int&gt;,\n#&gt; #   TotRms_AbvGrd &lt;int&gt;, Fireplaces &lt;int&gt;, Garage_Cars &lt;dbl&gt;,\n#&gt; #   Garage_Area &lt;dbl&gt;, Wood_Deck_SF &lt;int&gt;, Open_Porch_SF &lt;int&gt;, …\n\n# apply processing to other data:\nbake(ames_rec, new_data = head(ames))\n#&gt; # A tibble: 6 × 259\n#&gt;   Lot_Frontage Lot_Area Year_Built Year_Remod_Add Mas_Vnr_Area BsmtFin_SF_1\n#&gt;          &lt;dbl&gt;    &lt;int&gt;      &lt;int&gt;          &lt;int&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n#&gt; 1          141    31770       1960           1960          112            2\n#&gt; 2           80    11622       1961           1961            0            6\n#&gt; 3           81    14267       1958           1958          108            1\n#&gt; 4           93    11160       1968           1968            0            1\n#&gt; 5           74    13830       1997           1998            0            3\n#&gt; 6           78     9978       1998           1998           20            3\n#&gt; # ℹ 253 more variables: BsmtFin_SF_2 &lt;dbl&gt;, Bsmt_Unf_SF &lt;dbl&gt;,\n#&gt; #   Total_Bsmt_SF &lt;dbl&gt;, First_Flr_SF &lt;int&gt;, Second_Flr_SF &lt;int&gt;,\n#&gt; #   Gr_Liv_Area &lt;int&gt;, Bsmt_Full_Bath &lt;dbl&gt;, Bsmt_Half_Bath &lt;dbl&gt;,\n#&gt; #   Full_Bath &lt;int&gt;, Half_Bath &lt;int&gt;, Bedroom_AbvGr &lt;int&gt;, Kitchen_AbvGr &lt;int&gt;,\n#&gt; #   TotRms_AbvGrd &lt;int&gt;, Fireplaces &lt;int&gt;, Garage_Cars &lt;dbl&gt;,\n#&gt; #   Garage_Area &lt;dbl&gt;, Wood_Deck_SF &lt;int&gt;, Open_Porch_SF &lt;int&gt;,\n#&gt; #   Enclosed_Porch &lt;int&gt;, Three_season_porch &lt;int&gt;, Screen_Porch &lt;int&gt;, …\n\n以下是每个步骤的详细解释：\n\nrecipe(Sale_Price ~ ., data = ames[-(1:6), ])：这行创建了一个 “recipe” 对象，指定了因变量 (Sale_Price) 和自变量（数据框的所有其他列）。ames[-(1:6), ] 表示去掉了前 6 行的数据。\nstep_other(Neighborhood, threshold = 0.05)：该步骤将 “Neighborhood” 变量中那些少于 5% 的类别合并为一个新的类别 “other”。\nstep_dummy(all_nominal())：这步创建虚拟（哑）变量，对所有标称变量执行独热编码。\nstep_interact(~ starts_with(\"Central_Air\"):Year_Built)：这步创建交互项，即 “Central_Air” 与 “Year_Built” 的乘积。\nstep_ns(Longitude, Latitude, deg_free = 2)：该步执行自然样条转换，通常用于处理非线性关系。\nstep_zv(all_predictors())：这步会删除所有零方差预测变量，即那些在所有观察值中都具有相同值的列。\n\nprep(ames_rec) 函数对这个 recipe 进行预处理，计算出需要的统计量（如均值、标准差等）。之后，可以使用 bake() 函数来应用这个预处理流程到新的数据上。例如，bake(ames_rec, new_data = NULL) 会将预处理流程应用到训练集（即创建 recipe 时用的数据），而 bake(ames_rec, new_data = head(ames)) 则会将其应用到数据框 “ames” 的前几行。"
  },
  {
    "objectID": "15.tidymodels-recipes.html#特征工程的方法",
    "href": "15.tidymodels-recipes.html#特征工程的方法",
    "title": "5  特征工程",
    "section": "5.5 特征工程的方法",
    "text": "5.5 特征工程的方法\nrecipes 软件包中有两大类函数，一类是用来进行变量选择的函数，与 tidyselect 中的用法大概相同，另一类是用来进行变量转换的函数，通常以 step_* 开头。\nSelectors\n\nuse basic variable names (e.g. x1, x2),\ndplyr functions for selecting variables: contains(), ends_with(), everything(), matches(), num_range(), and starts_with(),\nfunctions that subset on the role of the variables that have been specified so far: all_outcomes(), all_predictors(), has_role(),\nsimilar functions for the type of data: all_nominal(), all_numeric(), and has_type(), or compound selectors such as all_nominal_predictors() or all_numeric_predictors().\n\nStep\nrecipes 库提供了许多 step_ 函数用于数据预处理和特征工程。以下是一些主要的类别：\n\n缩放与中心化：例如 step_center() 和 step_scale()。这两个函数可以将数值型变量重新缩放到均值为 0，标准差为 1。比如：\n\nrecipe &lt;- recipe(~ ., data = your_data) %&gt;%\n  step_center(all_numeric()) %&gt;%\n  step_scale(all_numeric())\n\n离散化：例如 step_discretize()。这个函数可以将连续变量划分为若干个范围（即“桶”），然后转换为因子类型。\n\nrecipe &lt;- recipe(~ ., data = your_data) %&gt;%\n  step_discretize(Age, options = list(cuts = 5))\n\n创建虚拟变量：例如 step_dummy()。这个函数可以对分类变量进行独热编码，每个类别生成一个新的二进制特征。\n\nrecipe &lt;- recipe(~ ., data = your_data) %&gt;%\n  step_dummy(Gender)\n\n交互项和多项式：例如 step_interact() 和 step_poly(). step_interact() 可以创建交互项（即两个或更多变量的乘积），而 step_poly() 可以创建多项式特征。\n\nrecipe &lt;- recipe(~ ., data = your_data) %&gt;%\n  step_interact(~ starts_with(\"x1\"):starts_with(\"x2\")) %&gt;%\n  step_poly(Gender, degree = 2)\n\n缺失值处理：例如 step_impute_knn() 和 step_impute_median(). 这些函数可以用不同的方法（如 KNN 填充或中位数填充）来处理缺失值。\n\nrecipe &lt;- recipe(~ ., data = your_data) %&gt;%\n  step_impute_knn(all_predictors()) %&gt;%\n  step_impute_median(Age)\n此外，recipes 包中的 filter 类函数用于选择或排除特定的观察值或变量。例如：\n\nstep_slice()：这个函数会根据给定的行索引保留或删除观察值。\n\nrecipe &lt;- recipe(~ ., data = your_data) %&gt;%\n  step_slice(row_index(5:10))\n上面的代码将保留第5行至第10行的数据。\n\nstep_rm()：这个函数会从数据集中删除指定的变量。\n\nrecipe &lt;- recipe(~ ., data = your_data) %&gt;%\n  step_rm(Gender)\n上面的代码将从数据集中移除 Gender 这一列。\n\nstep_zv()：该函数会删除所有零方差预测变量，即那些在所有观察值中都具有相同值的列。\n\nrecipe &lt;- recipe(~ ., data = your_data) %&gt;%\n  step_zv(all_predictors())\n上面的代码将移除所有零方差的预测变量。\n\nstep_corr()：对于高度相关的预测变量（即两个变量相互之间的相关性超过给定阈值），此函数将只保留一个。\n\nrecipe &lt;- recipe(~ ., data = your_data) %&gt;%\n  step_corr(all_numeric(), threshold = 0.9)\n以上代码将移除与任何其他数值变量相关性超过 0.9 的变量。\n以上只是一部分例子，recipes 包还提供了更多的 step_ 函数。具体可根据数据集和建模需求选择合适的预处理步骤。"
  },
  {
    "objectID": "15.tidymodels-recipes.html#其它的特征工程工具",
    "href": "15.tidymodels-recipes.html#其它的特征工程工具",
    "title": "5  特征工程",
    "section": "5.6 其它的特征工程工具",
    "text": "5.6 其它的特征工程工具\n除了 recipes 包，还有另外一些包提供了更多的特征工程工具。这些工具可以在 https://www.tidymodels.org/find/recipes/ 找到。\n例如，textrecipes 是 tidymodels 的一个扩展包，专门处理自然语言处理（NLP）任务中常见的文本预处理和特征工程步骤。它遵循与 recipes 包相同的设计原理，并提供了一些针对文本数据的 step_ 函数。\n以下是一些主要的 step_ 函数：\n\nstep_tokenize()： 这个函数将文本分割成单词或标记(token)。\n\nrecipe &lt;- recipe(~ ., data = your_data) %&gt;%\n  step_tokenize(text_column)\n\nstep_stopwords()：这个函数可以删除被认为对模型没有信息价值的常用词（如“the”、“and”等）。\n\nrecipe &lt;- recipe(~ ., data = your_data) %&gt;%\n  step_tokenize(text_column) %&gt;%\n  step_stopwords(text_column)\n\nstep_tfidf()：此函数计算每个词的 TF-IDF （词频-逆文档频率）得分，这是一种常见的计算词重要性的方法。\n\nrecipe &lt;- recipe(~ ., data = your_data) %&gt;%\n  step_tokenize(text_column) %&gt;%\n  step_tfidf(text_column)\n\nstep_sequence_onehot(): 对序列数据进行独热编码。\n\nrecipe &lt;- recipe(~ ., data = your_data) %&gt;%\n  step_sequence_onehot(text_column)\n以上只是 textrecipes 包提供的部分函数，还有更多其他的函数用于处理特定的文本预处理任务，比如词干提取（stemming）、词形还原（lemmatization）等。这个包是对 recipes 包的有效扩展，使其能够更好地处理文本数据。\n除此之外，用户还可以创建自己的 step_* 函数，参见：https://www.tidymodels.org/learn/develop/recipes/。"
  },
  {
    "objectID": "15.tidymodels-recipes.html#创建新变量",
    "href": "15.tidymodels-recipes.html#创建新变量",
    "title": "5  特征工程",
    "section": "5.7 创建新变量",
    "text": "5.7 创建新变量\n这个代码示例主要展示了 recipes 包在数据预处理中的使用，包括创建新的变量以及如何正确地嵌入对象。让我们一步一步来看。\n\nrec &lt;-\n  recipe(~., data = iris) %&gt;%\n  step_mutate(\n    dbl_width = Sepal.Width * 2,\n    half_length = Sepal.Length / 2\n  )\n\nprepped &lt;- prep(rec, training = iris %&gt;% slice(1:75))\n\nlibrary(dplyr)\n\ndplyr_train &lt;-\n  iris %&gt;%\n  as_tibble() %&gt;%\n  slice(1:75) %&gt;%\n  mutate(\n    dbl_width = Sepal.Width * 2,\n    half_length = Sepal.Length / 2\n  )\n\nrec_train &lt;- bake(prepped, new_data = NULL)\nall.equal(dplyr_train, rec_train)\n#&gt; [1] TRUE\n\ndplyr_test &lt;-\n  iris %&gt;%\n  as_tibble() %&gt;%\n  slice(76:150) %&gt;%\n  mutate(\n    dbl_width = Sepal.Width * 2,\n    half_length = Sepal.Length / 2\n  )\nrec_test &lt;- bake(prepped, iris %&gt;% slice(76:150))\nall.equal(dplyr_test, rec_test)\n#&gt; [1] TRUE\n\n# Embedding objects:\nconst &lt;- 1.414\n\nqq_rec &lt;-\n  recipe(~., data = iris) %&gt;%\n  step_mutate(\n    bad_approach = Sepal.Width * const,\n    best_approach = Sepal.Width * !!const\n  ) %&gt;%\n  prep(training = iris)\n\nbake(qq_rec, new_data = NULL, contains(\"appro\")) %&gt;% slice(1:4)\n#&gt; # A tibble: 4 × 2\n#&gt;   bad_approach best_approach\n#&gt;          &lt;dbl&gt;         &lt;dbl&gt;\n#&gt; 1         4.95          4.95\n#&gt; 2         4.24          4.24\n#&gt; 3         4.52          4.52\n#&gt; 4         4.38          4.38\n\n# The difference:\ntidy(qq_rec, number = 1)\n#&gt; # A tibble: 2 × 3\n#&gt;   terms         value               id          \n#&gt;   &lt;chr&gt;         &lt;chr&gt;               &lt;chr&gt;       \n#&gt; 1 bad_approach  Sepal.Width * const mutate_Bv7YY\n#&gt; 2 best_approach Sepal.Width * 1.414 mutate_Bv7YY\n\n首先，定义了一个 recipe，该 recipe 对 iris 数据集的 Sepal.Width 和 Sepal.Length 列进行了变换，创建了两个新的列 dbl_width（等于 Sepal.Width 的两倍）和 half_length（等于 Sepal.Length 的一半）。然后，使用 prep() 函数准备（或训练）这个 recipe，得到了预处理步骤的结果。\n接着，使用 dplyr 手动对训练集和测试集进行了相同的预处理步骤，并使用 all.equal() 检查手动处理后的结果是否与使用 recipes 得到的结果一致。结果表明，两种方式得到的结果是一致的。\n然后，介绍了嵌入常数对象的正确方法。在这部分的 recipe 中，通过对变量 Sepal.Width 乘以一个常数 const 来生成新的变量。这里使用了 !! 符号来强制评估 const。这种做法被称为“非标准评估”（non-standard evaluation, NSE），是 tidyverse 软件包中经常用到的一种技巧。在这种情况下，如果不使用 !!，那么 const 会被当作一个字符串，而不是其对应的值。\n最后，使用 tidy() 函数查看了 recipe 中预处理步骤的详细情况。从输出中可以看到，在 best_approach 列的预处理中，常数 const 已经被替换为其实际值 1.414。\n总的来说，这个示例展示了如何使用 recipes 包进行数据预处理，以及在处理过程中如何正确地处理嵌入的对象。"
  },
  {
    "objectID": "30.tidymodels-example.html#预测光谱数据的实例",
    "href": "30.tidymodels-example.html#预测光谱数据的实例",
    "title": "6  Example of tidymodels",
    "section": "6.1 预测光谱数据的实例",
    "text": "6.1 预测光谱数据的实例\ntidymodels是一组用于建立统计和机器学习模型的R包，它可以处理各种类型的数据，包括光谱数据。以下是一个简单的示例，说明如何使用主成分分析（PCA）和随机森林回归在tidymodels中对光谱数据进行预测。\n\n# 加载必要的包\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n\n\n✔ broom        1.0.5     ✔ recipes      1.0.9\n✔ dials        1.2.0     ✔ rsample      1.2.0\n✔ dplyr        1.1.4     ✔ tibble       3.2.1\n✔ ggplot2      3.4.4     ✔ tidyr        1.3.0\n✔ infer        1.0.5     ✔ tune         1.1.2\n✔ modeldata    1.2.0     ✔ workflows    1.1.3\n✔ parsnip      1.1.1     ✔ workflowsets 1.0.1\n✔ purrr        1.0.2     ✔ yardstick    1.2.0\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.4\n✔ lubridate 1.9.3     ✔ stringr   1.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ readr::col_factor() masks scales::col_factor()\n✖ purrr::discard()    masks scales::discard()\n✖ dplyr::filter()     masks stats::filter()\n✖ stringr::fixed()    masks recipes::fixed()\n✖ dplyr::lag()        masks stats::lag()\n✖ readr::spec()       masks yardstick::spec()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# 假设我们有一个名为spectra的数据框，其中有100列表示光谱特征，最后一列是我们想要预测的响应变量\nset.seed(123)\nspectra &lt;- as_tibble(matrix(rnorm(10000), ncol = 100))\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nℹ Using compatibility `.name_repair`.\n\nspectra$response &lt;- with(spectra, V1 * 2 + V2 ^ 2 + rnorm(nrow(spectra)))\n\n# 划分训练集和测试集\nsplit &lt;- initial_split(spectra, prop = 3/4)\ntrain_data &lt;- training(split)\ntest_data &lt;- testing(split)\n\n# 定义PCA预处理步骤和随机森林规范\npca_recipe &lt;- recipe(response ~ ., data = train_data) %&gt;%\n  step_normalize(all_predictors()) %&gt;%\n  step_pca(all_predictors())\n\nrf_spec &lt;- rand_forest() %&gt;% set_engine(\"randomForest\", importance = TRUE)  |&gt; \n    set_mode(\"regression\")\n\n# 定义工作流\nrf_workflow &lt;- workflow() %&gt;% add_model(rf_spec) %&gt;% add_recipe(pca_recipe)\n\n# 训练模型\nrf_fit &lt;- fit(rf_workflow, data = train_data)\n\n# 进行预测\npredictions &lt;- rf_fit %&gt;% predict(test_data) %&gt;% bind_cols(test_data)\npredictions %&gt;% show()\n\n# A tibble: 25 × 102\n     .pred     V1      V2      V3      V4      V5      V6      V7      V8\n     &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1  2.84   -0.560 -0.710   2.20   -0.715  -0.0736 -0.602   1.07   -0.728 \n 2  1.28    1.72  -0.0450 -0.476   0.331  -1.65   -0.0951 -0.211   0.590 \n 3  0.712  -0.446  0.919  -0.0540  2.04    0.227  -0.0792 -0.849   0.748 \n 4 -0.307   0.401 -1.62    1.23   -1.73    0.653   0.882   1.69   -0.0936\n 5  0.0653  0.111 -0.0556 -0.516  -0.602  -0.123   0.206  -0.0160 -0.0867\n 6  0.978  -1.97  -0.641  -0.723  -1.26    0.430   0.310  -0.675  -0.287 \n 7  1.85    0.701 -0.850  -1.24    1.68    0.535  -1.04   -1.22    0.373 \n 8  1.54    0.838  0.235  -0.705  -0.0608  2.40    1.24    0.171   0.564 \n 9  1.47    0.426  1.44    1.96    1.02   -0.191   0.605  -0.679   1.74  \n10 -0.130  -0.295  0.452  -0.0903 -1.19    0.378  -0.506  -1.86    0.881 \n# ℹ 15 more rows\n# ℹ 93 more variables: V9 &lt;dbl&gt;, V10 &lt;dbl&gt;, V11 &lt;dbl&gt;, V12 &lt;dbl&gt;, V13 &lt;dbl&gt;,\n#   V14 &lt;dbl&gt;, V15 &lt;dbl&gt;, V16 &lt;dbl&gt;, V17 &lt;dbl&gt;, V18 &lt;dbl&gt;, V19 &lt;dbl&gt;,\n#   V20 &lt;dbl&gt;, V21 &lt;dbl&gt;, V22 &lt;dbl&gt;, V23 &lt;dbl&gt;, V24 &lt;dbl&gt;, V25 &lt;dbl&gt;,\n#   V26 &lt;dbl&gt;, V27 &lt;dbl&gt;, V28 &lt;dbl&gt;, V29 &lt;dbl&gt;, V30 &lt;dbl&gt;, V31 &lt;dbl&gt;,\n#   V32 &lt;dbl&gt;, V33 &lt;dbl&gt;, V34 &lt;dbl&gt;, V35 &lt;dbl&gt;, V36 &lt;dbl&gt;, V37 &lt;dbl&gt;,\n#   V38 &lt;dbl&gt;, V39 &lt;dbl&gt;, V40 &lt;dbl&gt;, V41 &lt;dbl&gt;, V42 &lt;dbl&gt;, V43 &lt;dbl&gt;, …\n\n\n在这个例子中，我们首先生成了一个模拟的光谱数据集，然后执行主成分分析以减少数据的维数，最后使用随机森林进行预测。tidymodels提供了许多其他的预处理步骤和模型规范，你可以根据自己的需求进行选择和调整。\n需要注意的是，对于光谱数据，因为特征数量通常很大（可能达到上千或更多），所以一般需要进行降维或者特征选择，而且也需要选择能够处理高维数据的模型。"
  },
  {
    "objectID": "20.data-mining.html#变量类型及处理方法",
    "href": "20.data-mining.html#变量类型及处理方法",
    "title": "Data mining with R",
    "section": "变量类型及处理方法",
    "text": "变量类型及处理方法\n\n变量类型\n在数据挖掘中，变量可以根据测量尺度主要分为四类：\n\n定类尺度（Nominal Scale）：这是最低级的尺度，用于表示差异。例如，性别（男、女），血型（A、B、AB、O）等。\n定序尺度（Ordinal Scale）：定序尺度不仅可以区分项目，还能够确定项目之间的顺序。例如，产品质量评价（优、良、中、差），教育程度（小学、初中、高中、大学）等。\n定距尺度（Interval Scale）：定距尺度除了具有定序尺度的特性外，还可以测量项目之间的距离。例如，温度（摄氏度），年份等。\n定比尺度（Ratio Scale）：定比尺度是最高级的尺度，它具有固定的零点和单位尺度，并且可以进行加、减、乘、除等运算。例如，长度（米）、重量（千克）、收入等。\n\n对变量的理解和处理直接影响到数据挖掘的结果，因此我们需要根据不同的变量类型选择合适的数据挖掘方法。\n\n\n如何处理离散变量\n处理离散（分类）变量一般有以下几种方法：\n\n独热编码（One-hot Encoding）：对于每一个类别生成一个布尔列，这个类别表示为1，其它类别表示为0。例如，如果你的数据集中有一个名为“颜色”的分类变量，其中包含三个值：“红色”，“蓝色”和“绿色”。通过独热编码，可以将这一分类变量转化为3个布尔变量：“是红色”，“是蓝色”和“是绿色”。\n标签编码（Label Encoding）：每一个唯一的分类值被赋予一个整数。例如，“红色”为1，“蓝色”为2，“绿色”为3。这种方式适用于有序的分类变量，如评级（好、中、差等）。但对于无序的分类变量，可能导致模型误解其中存在顺序关系。\n二进制编码（Binary Encoding）：首先，将所有的分类值按照出现的频率从高到低排序，并分配一个唯一的整数值。然后，将这些整数值转换为二进制形式。\n哑变量（Dummy Variable）：哑变量是独热编码的一种特例，常用于统计学中。在哑变量编码中，会为分类变量的每一个类别创建一个新的变量，然后使用0或1来表示类别是否存在。与独热编码不同的是，为了避免共线性问题，哑变量会少创建一个变量。\n效应编码（Effect Encoding）：效应编码是哑变量编码的一种扩展，通常用于线性模型（如线性回归）。对于每个类别，如果该类别对应的观察值是1，则编码为1，如果是0，则编码为-1，否则编码为0。\n哈希编码（Hashing Encoding）：哈希编码通过哈希函数将分类变量映射到比原先分类数目更小的空间。它可以处理大规模分类特征，并且在内存和计算上更高效。但是由于哈希碰撞的存在，可能会导致信息损失。\n\n选择哪种方法取决于你的数据和模型。有些算法（如决策树和随机森林）可以直接处理分类变量，而其他算法（如线性回归和支持向量机）则需要进行以上编码。\n\n\n如何处理时间变量\n时间自变量无法直接进入建模数据集。因为时间是无限增长的，在建模数据集中出现的时间肯定早于预测数据集中出现的时间，所以如果需要再建模过程中考虑时间自变量，就必须对其进行变换。\n处理时间信息的方式主要取决于其在数据分析中的作用和含义。以下是一些常见的处理方式：\n\n日期/时间分解：将日期或时间字段分解为年、月、日、小时、分钟和秒等较小的部分。这样可以帮助我们更好地从中找出潜在的趋势、周期性和模式。\n计算时间间隔：在许多情况下，可能需要知道两个日期或时间之间的间隔，例如计算用户的留存时间、产品的生命周期等。\n日期/时间编码：将日期转换为季度、星期几、工作日或节假日等分类变量，这对于考察特定事件对结果的影响非常有用。\n时间序列分析：如果数据具有时间顺序，那么就可能涉及到时间序列分析。例如通过自相关和偏自相关图来检查数据是否存在季节性和趋势。\n设立时间窗口：在某些场景下，我们可能只关注数据的一部分时间段，比如用户近7天、30天的行为数据等，此时就需要设立合适的时间窗口进行分析。\n离散化/分箱：把连续的时间信息转换成类别，例如将24小时制的时间划分为“早上”、“中午”、“下午”和“晚上”。\n\n以上都是处理时间信息的一些常见策略，具体采取哪种方式需要根据你的数据和需求来定。\n\n\n如何处理数据中的异常值\n处理建模数据集中的极值（即异常大或异常小的值）是很重要的一部分，因为它们可能对模型的学习产生不利影响。以下是一些常见的处理方法：\n\n删除：如果你确定这些极值是由于错误或其他我们不关心的原因导致的，可以选择直接删除。\n截断：将所有超出某个范围的值设定为范围的上限或下限。这种方法可以减少极值的影响，但保留了它们的存在。\n变换：如对数变换、Box-Cox变换等，可以减小极值造成的影响，并使数据更接近正态分布，便于后续建模处理。\n标准化/归一化：通过尺度变换将所有数据转换到同一尺度，可以降低极值对模型的影响。\n使用鲁棒性模型：某些模型（如基于树的模型）对极值具有较强的鲁棒性，不需要额外处理。\n分位数离散化：将连续变量根据其值的大小进行排序，按照分位数（如四分位数）划分为多个等级，较大或较小的极值会被归入最高或最低的分位数中。\n\n对待极值没有通用的最佳做法，具体处理方式需要基于你的数据特性、模型选择以及业务需求来决定。\n\n\nBox-Cox 变换\nBox-Cox变换是一种数值稳定化（稳定方差）和正态化的技术，适用于连续响应变量。它的目标是找到一个合适的指数（λ）来转换数据，使得转换后的数据接近正态分布。\nBox-Cox 变换公式如下：\n\ny^{(\\lambda)} =\n\\begin{cases}\n\\frac{y^{\\lambda}-1}{\\lambda} & \\text{如果}\\ \\lambda \\neq 0, \\\\\nln(y) & \\text{如果}\\ \\lambda = 0,\n\\end{cases}\n\n其中，y 是需要被转换的原始数据，λ 是转换参数。在实际操作中，通常会选择使得数据更接近正态分布的 λ 值。\n这种方法可以有效地处理偏态分布的数据，并且对异常值具有良好的稳健性。然而，Box-Cox 变换要求输入数据必须是正的。对于包含零或负数的数据，可能需要进行平移或其他预处理步骤，以便应用 Box-Cox 变换。\n\n\n如何处理缺失值\n处理缺失值的方法有很多种，具体使用哪种取决于数据丢失的性质以及应用领域。\n如果缺失值实际存在但是没有被观测到，那可以进行填充、插值等。如果缺失值本身就没有，比如新用户没有购买所以就没有购买记录，那么可以使用缺失指示变量。\n以下是一些常见的处理方法：\n\n删除：直接删除包含缺失值的记录。这是处理缺失值最简单的方法，但如果数据丢失是随机的，或者缺失值较多时，可能会导致信息损失。\n填充：将缺失值替换为某个值。常见的填充方式包括使用固定值、平均值、中位数或众数等。对于分类变量，通常使用众数来填充；对于连续变量，可以考虑使用平均值或中位数。\n插值：在时间序列数据中，如果一个观测点的前后数据都是存在的，那么可以根据前后数据对其进行插值。插值方法有线性插值、多项式插值、样条插值等。\n预测模型：利用存在的数据建立模型，预测缺失值。例如使用回归模型、决策树、K-最近邻（KNN）等算法。\n使用缺失值指示变量：创建一个新的变量来指示数据是否丢失。例如，在处理调查问卷时，某个问题的非回答（NA）可能就意味着被调查者对这个问题选择了不回答，而这本身可能就是一个重要的信息。\n多重插补：多重插补（Multiple Imputation）是一种统计技巧，通过自相关关系生成多份替代缺失值的完整数据集，在每个数据集上进行分析，然后将结果合并。它可以有效处理数据丢失不确定性的问题。\n\n在处理缺失值时，首先需要理解数据丢失的机制，例如是完全随机丢失、随机丢失还是非随机丢失，然后再选择适当的方法。对于不同的问题，可能需要尝试不同的方法，以找到最适合的处理方式。\n\n\n类别不平衡问题\n类别不平衡问题（Class Imbalance Problem）是在监督学习中常见的问题，其特点是目标变量的类别分布不均匀。例如，在二分类问题中，大部分样本可能属于一个类别，而另一类别的样本数量相对非常少。\n这种问题在现实世界中很常见，比如信用卡欺诈检测、疾病诊断、电子邮件垃圾过滤等场景。在这些情况下，负面类（如欺诈、疾病或垃圾邮件）通常会远少于正面类。\n类别不平衡问题给机器学习任务带来挑战，因为大多数算法会偏向于多数类，从而忽略了数量较少但可能更重要的少数类，导致模型性能降低。\n处理类别不平衡问题有各种方法，例如：\n\n重新采样：包括过抽样（增加少数类的样本）和欠抽样（减少多数类的样本）。\n产生合成样本：例如SMOTE（Synthetic Minority Over-Sampling Technique）算法，通过插值的方式生成新的少数类样本。\n调整分类阈值：针对不同的类别设定不同的分类阈值。\n代价敏感学习：给予少数类样本更高的权重。\n使用集成方法：如随机森林、Adaboost等，这些算法对不平衡数据具有较好的鲁棒性。\n\n过抽样（Oversampling）和欠抽样（Undersampling）通常用于处理不平衡分类问题。\n\n过抽样：当我们的数据集中少数类的样本数量远少于多数类时，过抽样方法会通过增加少数类的样本数量来达到类别平衡。这可以通过复制少数类样本或生成新的少数类样本（例如使用SMOTE算法）来实现。但是，过度抽样可能会导致模型过拟合，因为它会复制少数类样本，这可能会引入噪声。\n欠抽样：与过抽样相反，当我们的数据集中多数类的样本数量远大于少数类时，欠抽样方法会通过减少多数类样本的数量来达到类别均衡。这可以通过随机删除多数类样本或使用聚类技术将多数类样本进行合并等方式实现。然而，欠抽样可能会丢失多数类的一些重要信息。\n\n以上两种方法都有其优势和缺点，具体使用哪种方法需要基于你的数据特性以及业务需求来决定。另外，也可以考虑使用组合采样（Combination Sampling），同时应用过抽样和欠抽样，或者使用更复杂的方法如代价敏感学习（Cost-Sensitive Learning）等。\n\n\n降维操作\n数据的降维是指通过某种数学方法，将有很多特征（维度）的数据集转化为具有较少特征的数据集，同时还能保留原始数据中的大部分重要信息。\n以下是进行数据降维的主要理由：\n\n减少计算量：大量的特征可能会导致计算量巨大，增加处理数据和训练模型的时间。\n降低模型复杂性：去除不相关或冗余的特征可以降低模型的复杂性，提高模型的可解释性。\n避免维度灾难：在高维度空间中，数据可能会非常稀疏，这使得许多机器学习算法难以有效地工作，这被称为“维度灾难”。\n可视化：对于二维或三维的数据，我们可以利用图形直观地展示其结构或者模式。而更高维度的数据无法直接可视化，需要通过降维技术转换到二维或三维。\n\n最常见的降维技术包括主成分分析（PCA）、线性判别分析（LDA）、t-分布邻域嵌入算法（t-SNE）等。选择哪种技术取决于你的数据特性和需求。"
  },
  {
    "objectID": "20.data-mining.html#数据挖掘方法",
    "href": "20.data-mining.html#数据挖掘方法",
    "title": "Data mining with R",
    "section": "数据挖掘方法",
    "text": "数据挖掘方法\n\n关联规则挖掘\n关联规则挖掘是一种在大型数据集中发现变量间有趣关系的方法，常被用于市场篮分析，它可以帮助我们理解哪些商品经常同时被购买。下面是一个具体的例子：\n假设你是一个零售商，出售各种不同的商品，你希望知道客户的购买行为模式来改善产品布局或优化销售策略。你可能有一个大型的交易数据库，每条记录包含一次购买行为以及该次购买中包含的商品。\n通过关联规则挖掘，你可能会发现像“如果购买了面包和黄油，那么很可能也会购买牛奶”这样的规则。这种规则表明面包、黄油和牛奶之间存在强烈的关联性。利用这个信息，你可能会把牛奶放在面包和黄油附近，从而增加销售量。\n在实际操作中，挖掘关联规则通常使用 Apriori 算法或 FP-Growth 算法等。\nApriori算法是一种用于频繁项集挖掘和关联规则学习的常见方法，主要应用在事务数据上。它最初被提出是为了解决市场篮子分析的问题，即找出哪些商品会被同时购买。\nApriori算法基于一个关键的概念，称为“先验原理”。这个原理指出，如果一个项集是频繁的，那么它的所有子集也一定是频繁的。反过来，如果一个项集是稀疏的（非频繁的），那么它的所有超集也一定是稀疏的。\nApriori算法的工作流程如下：\n\n第一步：首先扫描数据库，计算每个项目的支持度（也就是在所有交易中的出现频率）。然后根据给定的最小支持度阈值，删除那些支持度低于阈值的项目，得到一组频繁的1-项集。\n第二步：然后，算法使用频繁的k-项集生成候选的(k+1)-项集，并计算它们的支持度。删除支持度低于阈值的项集。这个过程不断重复，直到不能生成更大的项集。\n第三步：当我们有了频繁项集后，就可以用它们来生成关联规则。对于每个频繁项集，我们生成所有可能的规则，并计算它们的置信度（也就是规则的条件概率）。删除那些置信度低于最小置信度阈值的规则。剩下的规则就是我们挖掘出来的关联规则。\n\nApriori算法的优点是原理简单、易于实现。但是，当数据库很大或者项集数量很多时，Apriori算法的计算和存储需求可能会非常大。因此，在实际应用中，可能需要使用更高效的算法，如FP-Growth等。\n关联规则分析的一种典型应用是购物篮分析，我们来举一个假设的购物篮数据分析例子。\n首先，我们需要一个交易数据集。每个交易代表一个购物篮，包含若干商品。假设我们有以下数据：\n\n# 安装并加载arules包\n# install.packages(\"arules\")\nlibrary(arules)\n\n载入需要的程辑包：Matrix\n\n\n\n载入程辑包：'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\n\n载入程辑包：'arules'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following objects are masked from 'package:base':\n\n    abbreviate, write\n\n# 创建交易数据\ntransactions &lt;- list(\n  c(\"面包\", \"黄油\"),\n  c(\"牛奶\", \"面包\", \"黄油\"),\n  c(\"茶\", \"糖\"),\n  c(\"面包\", \"黄油\", \"果酱\"),\n  c(\"牛奶\", \"面包\", \"黄油\", \"茶\"),\n  c(\"面包\"),\n  c(\"茶\"),\n  c(\"茶\", \"糖\"),\n  c(\"面包\", \"牛奶\"),\n  c(\"面包\", \"黄油\", \"茶\")\n)\ntransactions &lt;- as(transactions, \"transactions\")\n\n然后，使用Apriori算法来找出频繁项集和关联规则：\n\n# 查找频繁项集\nfrequent_itemsets &lt;- apriori(transactions, parameter = list(support = 0.2, target = \"frequent itemsets\"))\n\nApriori\n\nParameter specification:\n confidence minval smax arem  aval originalSupport maxtime support minlen\n         NA    0.1    1 none FALSE            TRUE       5     0.2      1\n maxlen            target  ext\n     10 frequent itemsets TRUE\n\nAlgorithmic control:\n filter tree heap memopt load sort verbose\n    0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n\nAbsolute minimum support count: 2 \n\nset item appearances ...[0 item(s)] done [0.00s].\nset transactions ...[6 item(s), 10 transaction(s)] done [0.00s].\nsorting and recoding items ... [5 item(s)] done [0.00s].\ncreating transaction tree ... done [0.00s].\nchecking subsets of size 1 2 3 done [0.00s].\nsorting transactions ... done [0.00s].\nwriting ... [13 set(s)] done [0.00s].\ncreating S4 object  ... done [0.00s].\n\ninspect(head(frequent_itemsets))\n\n    items    support count\n[1] {糖}     0.2     2    \n[2] {牛奶}   0.3     3    \n[3] {茶}     0.5     5    \n[4] {黄油}   0.5     5    \n[5] {面包}   0.7     7    \n[6] {茶, 糖} 0.2     2    \n\n# 查找关联规则\nrules &lt;- apriori(transactions, parameter = list(support = 0.2, confidence = 0.6))\n\nApriori\n\nParameter specification:\n confidence minval smax arem  aval originalSupport maxtime support minlen\n        0.6    0.1    1 none FALSE            TRUE       5     0.2      1\n maxlen target  ext\n     10  rules TRUE\n\nAlgorithmic control:\n filter tree heap memopt load sort verbose\n    0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n\nAbsolute minimum support count: 2 \n\nset item appearances ...[0 item(s)] done [0.00s].\nset transactions ...[6 item(s), 10 transaction(s)] done [0.00s].\nsorting and recoding items ... [5 item(s)] done [0.00s].\ncreating transaction tree ... done [0.00s].\nchecking subsets of size 1 2 3 done [0.00s].\nwriting ... [10 rule(s)] done [0.00s].\ncreating S4 object  ... done [0.00s].\n\ninspect(head(rules))\n\n    lhs       rhs    support confidence coverage lift     count\n[1] {}     =&gt; {面包} 0.7     0.7000000  1.0      1.000000 7    \n[2] {糖}   =&gt; {茶}   0.2     1.0000000  0.2      2.000000 2    \n[3] {牛奶} =&gt; {黄油} 0.2     0.6666667  0.3      1.333333 2    \n[4] {牛奶} =&gt; {面包} 0.3     1.0000000  0.3      1.428571 3    \n[5] {黄油} =&gt; {面包} 0.5     1.0000000  0.5      1.428571 5    \n[6] {面包} =&gt; {黄油} 0.5     0.7142857  0.7      1.428571 5    \n\n\n上述代码会返回支持度大于0.2的频繁项集，以及支持度大于0.2且置信度大于0.6的关联规则。\n然后可以根据业务需求来解读这些结果，例如把经常一起购买的商品放在一起，或者针对某个商品的购买者进行推荐等。\n请注意，这只是一个基础示例。在实际应用中，可能需要更复杂的数据预处理步骤，以及调整Apriori算法的参数以找到最有价值的项集和规则。\n\n\n聚类分析\n以下是一个使用R语言进行k-means聚类分析的例子。我们将使用内置的iris数据集。\n首先，加载所需的库并读取数据：\n\n# 读取数据\ndata(iris)\n\n然后进行k均值聚类，这里我们设定cluster的数量为3（实际情况下可能会需要用到一些方法比如elbow method来确定k的值）：\n\nset.seed(123) # 设置随机数种子以保证结果可复制\n\n# 执行k-means聚类\nclusters &lt;- kmeans(iris[, 1:4], centers = 3)\n\n# 查看聚类结果\nprint(clusters)\n\nK-means clustering with 3 clusters of sizes 50, 62, 38\n\nCluster means:\n  Sepal.Length Sepal.Width Petal.Length Petal.Width\n1     5.006000    3.428000     1.462000    0.246000\n2     5.901613    2.748387     4.393548    1.433871\n3     6.850000    3.073684     5.742105    2.071053\n\nClustering vector:\n  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n [75] 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 3 3 3 3 2 3 3 3 3\n[112] 3 3 2 2 3 3 3 3 2 3 2 3 2 3 3 2 2 3 3 3 3 3 2 3 3 3 3 2 3 3 3 2 3 3 3 2 3\n[149] 3 2\n\nWithin cluster sum of squares by cluster:\n[1] 15.15100 39.82097 23.87947\n (between_SS / total_SS =  88.4 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\n现在，clusters对象包含了聚类的结果。我们可以将其添加回原始数据，并对结果进行可视化：\n\n# 将聚类结果添加到原始数据\niris$Cluster &lt;- as.factor(clusters$cluster)\n\n# 可视化聚类结果\nggplot(iris, aes(Petal.Length, Petal.Width, color = Cluster)) + \n  geom_point() +\n  theme_minimal()\n\n\n\n\n以上就是一个基本的k-means聚类分析的例子。注意，k-means聚类假设簇是凸形和圆形的，如果你的数据不满足这些假设，可能需要选择其他的聚类算法。\n\n\n广义线性模型\n广义线性模型（Generalized Linear Model，简称GLM）是一类灵活的统计模型。它扩展了传统的线性回归模型，允许因变量（响应变量）服从广义的概率分布，而不仅仅是正态分布。\nGLM主要包含以下三个部分：\n\n随机分布成分：这指定了因变量或响应变量服从的概率分布。在GLM中，响应变量可以服从包括二项分布、泊松分布、负二项分布、伽马分布等在内的指数族分布。\n系统部分：这和传统的线性回归模型类似，假设预测值是自变量（解释变量）的线性组合。\n连结函数：连结函数定义了响应变量的期望值和线性预测子之间的关系。最常见的例子包括恒等链接函数（用于正态分布）、对数链接函数（用于泊松分布或伽马分布）和logit链接函数（用于二项分布）。\n\nGLM的一个重要优点是它能够处理各种类型的响应变量，包括连续的、二元的、计数的等等。因此，GLM被广泛应用于各种领域，包括社会科学、生物科学、医学和工程等。\n在R语言中，可以使用glm函数来拟合GLM模型，例如：\n# 使用泊松回归（对数链接函数）预测计数数据\nmodel &lt;- glm(y ~ x1 + x2, data = mydata, family = poisson(link = \"log\"))\n\n连接函数\n在广义线性模型（GLM）中，连接函数用于描述因变量（响应变量）的期望值和线性预测子之间的关系。根据响应变量的类型和分布，可能会选择不同的连接函数。\n以下是一些常见的连接函数及其适用场景：\n\n恒等链接函数（Identity link）：这种连接函数指的是线性预测子直接等于响应变量的期望值。也就是说，g(μ) = μ。适用于因变量服从正态分布的线性回归。\n\nmodel &lt;- glm(y ~ x1 + x2, data = mydata, family = gaussian(link = \"identity\"))\n\n对数链接函数（Log link）：其中 g(μ) = log(μ)。这种连接函数通常用于因变量是计数数据且服从泊松分布的泊松回归或者伽马回归。\n\nmodel &lt;- glm(y ~ x1 + x2, data = mydata, family = poisson(link = \"log\"))\n\nLogit链接函数：其中 g(μ) = log[μ / (1 - μ)]。这种连接函数用于二元响应变量，比如逻辑回归中的二项分布。\n\nmodel &lt;- glm(y ~ x1 + x2, data = mydata, family = binomial(link = \"logit\"))\n\nProbit链接函数：其中 g(μ) = Φ^(-1)(μ)，Φ为标准正态分布的累积分布函数。也用于二元响应变量，它提供了一种与logit链接稍有不同的对于概率的建模方式。\n\nmodel &lt;- glm(y ~ x1 + x2, data = mydata, family = binomial(link = \"probit\"))\n以上只是部分常见链接函数的示例，实际上还有许多其他类型的连接函数，可以根据具体的数据和建模需求进行选择。\n\n\n泰坦尼克存活数据\n在R语言的titanic包中就包含了泰坦尼克号数据集，使用该数据分析乘客的存活情况与其他变量之间的关系。\n以下是一个简单的示例：\n\n# 安装和加载所需的R包\n# pak::pak(\"titanic\")\nlibrary(titanic)\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n\n\n✔ broom        1.0.5     ✔ rsample      1.2.0\n✔ dials        1.2.0     ✔ tune         1.1.2\n✔ infer        1.0.5     ✔ workflows    1.1.3\n✔ modeldata    1.2.0     ✔ workflowsets 1.0.1\n✔ parsnip      1.1.1     ✔ yardstick    1.2.0\n✔ recipes      1.0.9     \n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard()     masks purrr::discard()\n✖ recipes::discretize() masks arules::discretize()\n✖ Matrix::expand()      masks tidyr::expand()\n✖ dplyr::filter()       masks stats::filter()\n✖ recipes::fixed()      masks stringr::fixed()\n✖ dplyr::lag()          masks stats::lag()\n✖ Matrix::pack()        masks tidyr::pack()\n✖ arules::recode()      masks dplyr::recode()\n✖ yardstick::spec()     masks readr::spec()\n✖ recipes::step()       masks stats::step()\n✖ Matrix::unpack()      masks tidyr::unpack()\n✖ recipes::update()     masks Matrix::update(), stats::update()\n• Dig deeper into tidy modeling with R at https://www.tmwr.org\n\n# 加载数据\ndata(\"titanic_train\")\n\n# 使用 tidymodels 处理数据，并进行模型训练\ndata &lt;- titanic_train %&gt;% \n select(Survived, Pclass, Sex, Age, Fare) %&gt;% \n mutate(Survived = factor(Survived))  |&gt; \n drop_na()\n\n# 划分训练集和测试集\nset.seed(123)\ndata_split &lt;- initial_split(data, prop = 0.75, strata = Survived)\ndata_train &lt;- training(data_split)\ndata_test &lt;- testing(data_split)\n\n# 定义预处理步骤\nrec &lt;- recipe(Survived ~ ., data = data_train) %&gt;%\n step_dummy(all_nominal(), -all_outcomes()) %&gt;%\n step_zv(all_predictors()) %&gt;%\n prep()\n\n# 应用预处理步骤到训练集和测试集\ndata_train_preprocessed &lt;- bake(rec, new_data = data_train)\ndata_test_preprocessed &lt;- bake(rec, new_data = data_test)\n\n# 定义模型\nmodel &lt;- logistic_reg() %&gt;%\n set_engine(\"glm\") %&gt;%\n fit(Survived ~ ., data = data_train_preprocessed)\n\n# 预测测试集的结果\nresults &lt;- predict(model, data_test_preprocessed) %&gt;%\n bind_cols(data_test)\n\nmodel\n\nparsnip model object\n\n\nCall:  stats::glm(formula = Survived ~ ., family = stats::binomial, \n    data = data)\n\nCoefficients:\n(Intercept)       Pclass          Age         Fare     Sex_male  \n   4.603776    -1.245534    -0.029056     0.001864    -2.377828  \n\nDegrees of Freedom: 534 Total (i.e. Null);  530 Residual\nNull Deviance:      722.5 \nResidual Deviance: 491.8    AIC: 501.8\n\n\n在泰坦尼克号生存预测的模型中，我们使用了逻辑回归模型。逻辑回归模型的结果可以通过其系数进行解释。\n逻辑回归模型的基本形式是（Equation 1）：\n\nlogit(p) = β0 + β1*X1 + β2*X2 + ... + βn*Xn\n\\tag{1}\n其中，p 是正例（这里是”Survived”）的概率，βi是第i个特征的系数，Xi是第i个特征的值。logit(p)是p的对数几率。\n每个系数βi表示当其他所有特征保持不变时，X_i增加一单位时其对应的对数几率的改变量。如果βi &gt; 0，那么该特征与正例的概率正相关；如果βi &lt; 0，那么该特征与正例的概率负相关。\n例如，如果得到的模型系数为：\n(Intercept)      4.60\nSexmale         -2.37\nAge             -0.03\nFare             0.002\nPclass          -1.24\n那么，可以解释为性别为男性、乘客等级为2和3等级相比于1等级、年龄增加，都会降低生存的对数几率，而票价增加则会增加生存的对数几率。\n上面采用了 tidymodels 的流程，如果采用基础 R 方法，则有如下结果。\n\nmodel &lt;- glm(Survived ~ ., data = data_train, family = binomial(link = \"logit\"))\nmodel\n\n\nCall:  glm(formula = Survived ~ ., family = binomial(link = \"logit\"), \n    data = data_train)\n\nCoefficients:\n(Intercept)       Pclass      Sexmale          Age         Fare  \n   4.603776    -1.245534    -2.377828    -0.029056     0.001864  \n\nDegrees of Freedom: 534 Total (i.e. Null);  530 Residual\nNull Deviance:      722.5 \nResidual Deviance: 491.8    AIC: 501.8\n\n\n两个结果是一致的。\n\n\n\n神经网络模型\n我们使用Keras库和tensorflow后端在R环境中进行神经网络的建立和训练。这里我们使用UCI机器学习存储库中的红酒质量数据集。\n\n# 加载所需库\nlibrary(keras)\nreticulate::use_condaenv(\"tf\")\n\n# 读取并预处理数据\n# 安装和加载rattle包\nif (!require(rattle)) {\n  pak::pak(\"rattle\")\n}\n# 加载wine数据集\ndata(wine, package = \"rattle\")\n\n# 将数据分为训练集和测试集\nset.seed(123)  \nindices &lt;- sample(1:nrow(wine), nrow(wine)*0.7)\ntrain_data &lt;- wine[indices, ] |&gt; as.numeric()\ntest_data &lt;- wine[-indices, ]  |&gt; as.numeric()\n\n# 分离特征和标签\ntrain_X &lt;- as.matrix(train_data[, -1]) \ntrain_Y &lt;- train_data$type\n\ntest_X &lt;- as.matrix(test_data[, -1])\ntest_Y &lt;- test_data$type\n\n# 数据标准化\nmean &lt;- apply(train_X, 2, mean, na.rm = TRUE)\nstd &lt;- apply(train_X, 2, sd, na.rm = TRUE)\ntrain_X &lt;- scale(train_X, center = mean, scale = std)\ntest_X &lt;- scale(test_X, center = mean, scale = std)\n\n# 构建神经网络模型\nmodel &lt;- keras_model_sequential() \nmodel %&gt;% \n  layer_dense(units = 64, activation = 'relu', input_shape = ncol(train_X)) %&gt;% \n  layer_dense(units = 32, activation = 'relu') %&gt;%\n  layer_dense(units = 1)\n\n# 编译模型\nmodel %&gt;% compile(\n  loss = 'mse',\n  optimizer = optimizer_rmsprop(),\n  metrics = c('mae')\n)\n\n# 训练模型\nhistory &lt;- model %&gt;% fit(\n  train_X, train_Y,\n  epochs = 300, batch_size = 16, \n  validation_split = 0.2\n)\n\n以上就是使用keras在R中构建神经网络模型进行红酒质量预测的示例。注意，实际应用中可能需要更多的步骤来优化模型性能和解释模型结果。"
  },
  {
    "objectID": "21.house-price-pred-lm.html#对数据集的说明",
    "href": "21.house-price-pred-lm.html#对数据集的说明",
    "title": "7  Linear Model For House Price Predication",
    "section": "7.1 对数据集的说明",
    "text": "7.1 对数据集的说明\n这个数据集包含了美国华盛顿州金县（其中包括西雅图）的房屋销售价格以及相关房屋特性。\n以下是一些列的描述：\n\nprice: 房屋销售价格，这通常是我们要预测的目标变量。\nbedrooms: 卧室数量。\nbathrooms: 浴室数量。\nsqft_living: 居住面积（平方英尺）。\nsqft_lot: 地块大小（平方英尺）。\nfloors: 楼层数。\ncondition: 房屋状况，一般是按照某种等级划分的。\ngrade: 根据 King County 分级系统评出的房屋等级。\n\n这个数据集经常被用来进行回归分析或机器学习任务，例如预测房价。"
  },
  {
    "objectID": "21.house-price-pred-lm.html#传统方法",
    "href": "21.house-price-pred-lm.html#传统方法",
    "title": "7  Linear Model For House Price Predication",
    "section": "7.2 传统方法",
    "text": "7.2 传统方法\n下面是由 “Jia-Qi He” 在 “2022/9/11” 创作的传统方法。\n\n7.2.1 Load Library\n\n## 加载程序包,使用里面的管道函数\nlibrary(dplyr)\n\n\n载入程辑包：'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n## 设置随机数种子\nset.seed(202209)\n\n\n\n7.2.2 Data Processing\n读入数据，生成R数据框。将 price、sqft_living、sqft_lot、sqft_above 这四个变量取对数；并计算到 2015 年时房屋的年龄。\n\nhouse &lt;- house_raw %&gt;%\n  mutate(log_price = log(price)) %&gt;%\n  mutate(log_sqft_living = log(sqft_living)) %&gt;%\n  mutate(log_sqft_lot = log(sqft_lot)) %&gt;%\n  mutate(log_sqft_above = log(sqft_above)) %&gt;%\n  mutate(age = 2015-yr_built)\n\n使用 sample() 函数将数据集随机划分为学习数据集和测试数据集。先抽取学习数据集的观测序号，学习数据集是抽取的观测序号对应的观测。测试数据集是未被抽取到学习数据集的观测。\n\nid_learning &lt;- sample(1:nrow(house), round(0.7*nrow(house)))\nhouse_learning &lt;- house[id_learning,]\nhouse_testing &lt;- house[-id_learning,]\n\n\n\n7.2.3 Fitting\n对学习数据集拟合线性模型。因变量是 log_price，log_sqft_living 等变量均为自变量。\n\nfit.lm &lt;- lm(log_price ~ log_sqft_living + log_sqft_lot + log_sqft_above + age + bedrooms + bathrooms + floors + condition + grade, data = house_learning)\n\n查看建模结果。\n\nsummary(fit.lm)\n\n\nCall:\nlm(formula = log_price ~ log_sqft_living + log_sqft_lot + log_sqft_above + \n    age + bedrooms + bathrooms + floors + condition + grade, \n    data = house_learning)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.29041 -0.21109  0.01026  0.20526  1.67461 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      7.7428942  0.0735920 105.214  &lt; 2e-16 ***\nlog_sqft_living  0.5174014  0.0161666  32.004  &lt; 2e-16 ***\nlog_sqft_lot    -0.0336378  0.0035713  -9.419  &lt; 2e-16 ***\nlog_sqft_above  -0.0871745  0.0152195  -5.728 1.04e-08 ***\nage              0.0060742  0.0001142  53.181  &lt; 2e-16 ***\nbedrooms        -0.0438852  0.0035787 -12.263  &lt; 2e-16 ***\nbathrooms        0.0773968  0.0059612  12.983  &lt; 2e-16 ***\nfloors           0.0680483  0.0072774   9.351  &lt; 2e-16 ***\ncondition        0.0343253  0.0043309   7.926 2.43e-15 ***\ngrade            0.2409012  0.0036670  65.694  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3165 on 15119 degrees of freedom\nMultiple R-squared:  0.638, Adjusted R-squared:  0.6377 \nF-statistic:  2960 on 9 and 15119 DF,  p-value: &lt; 2.2e-16\n\n\n模型中各个自变量的系数均显著不为 0；模型的 R 方为 0.6406。\n提取模型的系数估计值\n\ncoefficients(fit.lm)\n\n    (Intercept) log_sqft_living    log_sqft_lot  log_sqft_above             age \n    7.742894171     0.517401364    -0.033637793    -0.087174490     0.006074193 \n       bedrooms       bathrooms          floors       condition           grade \n   -0.043885175     0.077396849     0.068048324     0.034325337     0.240901189 \n\n\n提取模型的因变量拟合值。\n\nyhat &lt;- fitted(fit.lm)\nstr(yhat)\n\n Named num [1:15129] 13.1 13.2 13.4 13 13.4 ...\n - attr(*, \"names\")= chr [1:15129] \"1\" \"2\" \"3\" \"4\" ...\n\n\n提取模型的残差。\n\nresid &lt;- residuals(fit.lm)\nstr(resid)\n\n Named num [1:15129] -0.0141 -0.308 0.1671 -0.2173 0.443 ...\n - attr(*, \"names\")= chr [1:15129] \"1\" \"2\" \"3\" \"4\" ...\n\n\n\n\n7.2.4 模型诊断\n将绘图窗口分为 2*2 的矩阵。指定绘图区域离下边界、左边界、上边界和右边界的距离（单位为文本行数），方便画下所有诊断图。\n画模型诊断图。\n\npar(mfrow=c(2, 2))\npar(mar=c(2.5, 2.5, 1.5, 1.5))\n\nlibrary(ggplot2)\nplot(fit.lm, which=c(1:4))\n\n\n\n\n\n\n7.2.5 Model Optimization\n从 Cook 距离图中可以看出，序号为”15871”的观测是异常点。\n去除序号为”15871”的观测，重新拟合线性模型\n\nfit2.lm &lt;- lm(log_price ~ log_sqft_living + log_sqft_lot + log_sqft_above + age + bedrooms + bathrooms + floors + condition + grade,data = house_learning[rownames(house_learning)!=\"15871\",])\n\npar(mfrow=c(2, 2))\npar(mar=c(2.5, 2.5, 1.5, 1.5))\nplot(fit2.lm, which=c(1:4))\n\n\n\n\n使用所得的线性模型对测试数据集进行预测。\n\nprediction.lm &lt;- predict(fit2.lm, house_testing)\n\npredition.lm 中含有预测的对数价格，exp(pred.lm) 将对数价格转换为预测的价格。将预测价格与真实价格取差值，平方之后平均，再开根号。计算出测试数据集的房屋价格预测的均方根误差。\n\nrmse.lm &lt;- sqrt(mean((exp(prediction.lm) - house_testing$price)^2))\n\nstr(rmse.lm)\n\n num 209627"
  },
  {
    "objectID": "21.house-price-pred-lm.html#tidymodels-方法",
    "href": "21.house-price-pred-lm.html#tidymodels-方法",
    "title": "7  Linear Model For House Price Predication",
    "section": "7.3 tidymodels 方法",
    "text": "7.3 tidymodels 方法\ntidymodels 方法重现了上面的建模过程，只是逻辑性和扩展性更好。\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n\n\n✔ broom        1.0.5     ✔ rsample      1.2.0\n✔ dials        1.2.0     ✔ tibble       3.2.1\n✔ infer        1.0.5     ✔ tidyr        1.3.0\n✔ modeldata    1.2.0     ✔ tune         1.1.2\n✔ parsnip      1.1.1     ✔ workflows    1.1.3\n✔ purrr        1.0.2     ✔ workflowsets 1.0.1\n✔ recipes      1.0.9     ✔ yardstick    1.2.0\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Learn how to get started at https://www.tidymodels.org/start/\n\n# 划分数据集\nset.seed(20240206)\n(house_split = initial_split(house_raw, strata = price))\n\n&lt;Training/Testing/Total&gt;\n&lt;16209/5404/21613&gt;\n\n# 训练集和测试集\n(house_train = training(house_split))\n\n# A tibble: 16,209 × 10\n    price bedrooms bathrooms sqft_living sqft_lot floors condition grade\n    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 180000        2       1           770    10000    1           3     6\n 2 291850        3       1.5        1060     9711    1           3     7\n 3 229500        3       1          1780     7470    1           3     7\n 4 310000        3       1          1430    19901    1.5         4     7\n 5 230000        3       1          1250     9774    1           4     7\n 6 233000        3       2          1710     4697    1.5         5     6\n 7 280000        2       1.5        1190     1265    3           3     7\n 8 240000        4       1          1220     8075    1           2     7\n 9 309000        3       1          1280     9656    1           4     6\n10 210490        3       1           990     8528    1           3     6\n# ℹ 16,199 more rows\n# ℹ 2 more variables: sqft_above &lt;dbl&gt;, yr_built &lt;dbl&gt;\n\n(house_test = testing(house_split))\n\n# A tibble: 5,404 × 10\n    price bedrooms bathrooms sqft_living sqft_lot floors condition grade\n    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 221900        3      1           1180     5650      1         3     7\n 2 510000        3      2           1680     8080      1         3     8\n 3 257500        3      2.25        1715     6819      2         3     7\n 4 468000        2      1           1160     6000      1         4     7\n 5 400000        3      1.75        1370     9680      1         4     7\n 6 189000        2      1           1200     9850      1         4     7\n 7 285000        5      2.5         2270     6300      2         3     8\n 8 252700        2      1.5         1070     9643      1         3     7\n 9 329000        3      2.25        2450     6500      2         4     8\n10 719000        4      2.5         2570     7173      2         3     8\n# ℹ 5,394 more rows\n# ℹ 2 more variables: sqft_above &lt;dbl&gt;, yr_built &lt;dbl&gt;\n\n# 创建 recipe\nhouse_rec = recipe(price ~ ., data = house_train) |&gt; \n  step_log(price, starts_with(\"sqft_\")) |&gt; \n  step_mutate(age = 2015 - yr_built) |&gt; \n  step_rm(yr_built)\nsummary(house_rec)\n\n# A tibble: 10 × 4\n   variable    type      role      source  \n   &lt;chr&gt;       &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 bedrooms    &lt;chr [2]&gt; predictor original\n 2 bathrooms   &lt;chr [2]&gt; predictor original\n 3 sqft_living &lt;chr [2]&gt; predictor original\n 4 sqft_lot    &lt;chr [2]&gt; predictor original\n 5 floors      &lt;chr [2]&gt; predictor original\n 6 condition   &lt;chr [2]&gt; predictor original\n 7 grade       &lt;chr [2]&gt; predictor original\n 8 sqft_above  &lt;chr [2]&gt; predictor original\n 9 yr_built    &lt;chr [2]&gt; predictor original\n10 price       &lt;chr [2]&gt; outcome   original\n\n# 定义训练参数\nreg_metrics = metric_set(mae, rsq)\n\n# 初始化 workflow\nhouse_wflow = workflow() |&gt; \n  add_recipe(house_rec) |&gt; \n  add_model(linear_reg())\n\n# 交叉验证集\n(house_rs = vfold_cv(house_train, strata = price))\n\n#  10-fold cross-validation using stratification \n# A tibble: 10 × 2\n   splits               id    \n   &lt;list&gt;               &lt;chr&gt; \n 1 &lt;split [14586/1623]&gt; Fold01\n 2 &lt;split [14586/1623]&gt; Fold02\n 3 &lt;split [14587/1622]&gt; Fold03\n 4 &lt;split [14588/1621]&gt; Fold04\n 5 &lt;split [14588/1621]&gt; Fold05\n 6 &lt;split [14589/1620]&gt; Fold06\n 7 &lt;split [14589/1620]&gt; Fold07\n 8 &lt;split [14589/1620]&gt; Fold08\n 9 &lt;split [14589/1620]&gt; Fold09\n10 &lt;split [14590/1619]&gt; Fold10\n\n# 拟合并评估模型\nctrl &lt;- control_resamples(save_pred = TRUE)\nhouse_res &lt;-\n  house_wflow %&gt;%\n  fit_resamples(house_rs, control = ctrl, metrics = reg_metrics)\n\n# 在测试集上预测并收集结果\nhouse_test_preds &lt;- house_wflow %&gt;%\n  last_fit(house_split) %&gt;%\n  collect_predictions(new_data = house_test)\n\n# 输出模型指标\nhouse_test_results &lt;- house_test_preds %&gt;%\n  metrics(truth = price, estimate = .pred)\n\nhouse_test_results\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.316\n2 rsq     standard       0.631\n3 mae     standard       0.250\n\n\n上述代码主要完成了以下几个步骤：\n\n使用 initial_split() 函数将数据集划分为训练集和测试集。strata = price 参数表示在划分数据时，会根据 price 列的值进行分层抽样，以确保训练集和测试集中的 price 分布相似。\n创建了一个预处理 recipe，其中包括对 price 和所有以 “sqft_” 开头的列进行对数转换，以及计算房龄（2015年减去建造年份）。\n定义了回归任务的评价指标：平均绝对误差（MAE）和决定系数（R²）。\n初始化了一个工作流，其中包含上述的预处理 recipe 以及线性回归模型。\n对训练数据进行分层交叉验证，创建了一系列的训练/验证集。\n最后一步是利用交叉验证的结果来拟合工作流，并评估模型性能。\nlast_fit() 将工作流拟合到完整的训练数据上，然后用拟合好的模型在测试集上做预测。最后，计算了测试集上的 MAE 和 R² 指标并打印出来。\n\n最后，数据可视化是数据科学工作中的重要一环。我们可以通过可视化来更好地理解模型的性能以及数据的特点。以下是两个常见的可视化任务：\n\n观察预测值与真实值的关系：我们可以绘制一个散点图，横坐标为预测值，纵坐标为真实值。\n\n\nggplot(house_test_preds, aes(x = .pred, y = price)) +\n  geom_point(alpha = 0.4) +\n  geom_abline(color = \"blue\") +\n  xlab(\"Predicted Price\") +\n  ylab(\"True Price\")\n\n\n\n\n在这个图中，蓝色的线表示预测值和真实值完全相等的情况。如果模型的预测效果良好，那么点应该紧密地围绕在这条线周围。\n\n查看每次交叉验证的结果：我们可以绘制一个箱型图，展示每次交叉验证结果的分布。\n\n\nhouse_res |&gt; \n  select(id, .metrics)  |&gt; \n  unnest(.metrics) %&gt;%\n  ggplot(aes(x = id, y = .estimate)) +\n  geom_col() +\n  facet_wrap(~.metric, ncol = 1)\n\n\n\n\n在这个图中，每个箱型图代表一次交叉验证的结果（即模型在不同训练/验证集上的表现）。通过查看箱型图，我们可以了解模型性能的稳定性和可靠性。\n以上只是一些基本的可视化示例，具体可视化的内容和方式会依据你对数据和任务的理解进行调整。\n\n7.3.1 最佳拟合结果\n在 tidymodels 中，如果你使用了调参（tune）功能寻找最佳的模型超参数，那么可以使用以下方法来获取最佳的拟合结果：\n\n# 获取最佳参数组合\nbest_params &lt;- house_res %&gt;%\n  select_best(metric = \"mae\")\n\n# 使用最佳参数重新拟合模型\nbest_fit &lt;- house_wflow %&gt;%\n  finalize_workflow(best_params) %&gt;%\n  last_fit(house_split)\n\n# 提取拟合结果\nfit_result &lt;- best_fit %&gt;% \n  extract_fit_parsnip()\n\n# 拟合得到的参数\nfit_result %&gt;% tidy()\n\n# A tibble: 10 × 5\n   term        estimate std.error statistic   p.value\n   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)  7.78     0.0717      108.   0        \n 2 bedrooms    -0.0510   0.00370     -13.8  6.19e- 43\n 3 bathrooms    0.0839   0.00576      14.6  1.01e- 47\n 4 sqft_living  0.495    0.0157       31.5  1.25e-211\n 5 sqft_lot    -0.0332   0.00341      -9.74 2.27e- 22\n 6 floors       0.0633   0.00703       9.00 2.51e- 19\n 7 condition    0.0364   0.00419       8.70 3.53e- 18\n 8 grade        0.243    0.00353      68.9  0        \n 9 sqft_above  -0.0699   0.0147       -4.77 1.90e-  6\n10 age          0.00609  0.000111     55.0  0        \n\n# 自变量的重要性\nfit_result |&gt; vip::vip()\n\n\n\n\n在上面的代码中： - house_res 是你在调参过程中得到的结果，包含了所有尝试过的参数组合以及对应的评价指标。 - select_best() 函数用于选择使指定指标达到最优的参数组合。在这个例子中，我们选择使 RMSE 最小的参数组合。 - finalize_workflow() 函数将最佳参数设置到工作流中。 - last_fit() 函数则使用这个参数再次拟合模型。\n然后，我们像之前一样使用 extract_fit_parsnip() 和 tidy() 函数来提取模型拟合结果。此时，fit_result 就是一个包含了最佳模型参数的数据框。"
  },
  {
    "objectID": "90.conclusion.html",
    "href": "90.conclusion.html",
    "title": "Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "95.references.html",
    "href": "95.references.html",
    "title": "References",
    "section": "",
    "text": "McConville, Chester Ismay and Albert Y. Kim Foreword by Kelly S. 2022.\nStatistical Inference via Data\nScience.\n\n\nRobinson, Julia Silge and David. 2023. Welcome to Text\nMining with R  Text Mining\nwith R.\n\n\nSilge, Emil Hvitfeldt and Julia. 2023. Supervised Machine\nLearning for Text Analysis in R.\n\n\nSilge, Max Kuhn and Julia. 2022. Tidy Modeling with\nR.\n\n\n张俊妮. 2021. 数据挖掘：基于R语言的实战.\n人民邮电出版社."
  }
]